{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "source": [
    "# Read all the needed datasets\n",
    "dataset들 읽고 합치기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Character                                           Sentence\n",
       "0     Dumbledore  I should've known that you would be here, Prof...\n",
       "1     McGonagall                Good evening, Professor Dumbledore.\n",
       "2     McGonagall                        Are the rumors true, Albus?\n",
       "3     Dumbledore                          I'm afraid so, professor.\n",
       "4     Dumbledore                              The good and the bad.\n",
       "...          ...                                                ...\n",
       "4920    HERMIONE                             How fast is it, Harry?\n",
       "4921       HARRY                                             Lumos.\n",
       "4922       HARRY          I solemnly swear that I am up to no good.\n",
       "4923       HARRY                                  Mischief managed.\n",
       "4924       HARRY                                               Nox.\n",
       "\n",
       "[4925 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Character</th>\n      <th>Sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dumbledore</td>\n      <td>I should've known that you would be here, Prof...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>McGonagall</td>\n      <td>Good evening, Professor Dumbledore.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>McGonagall</td>\n      <td>Are the rumors true, Albus?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dumbledore</td>\n      <td>I'm afraid so, professor.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dumbledore</td>\n      <td>The good and the bad.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4920</th>\n      <td>HERMIONE</td>\n      <td>How fast is it, Harry?</td>\n    </tr>\n    <tr>\n      <th>4921</th>\n      <td>HARRY</td>\n      <td>Lumos.</td>\n    </tr>\n    <tr>\n      <th>4922</th>\n      <td>HARRY</td>\n      <td>I solemnly swear that I am up to no good.</td>\n    </tr>\n    <tr>\n      <th>4923</th>\n      <td>HARRY</td>\n      <td>Mischief managed.</td>\n    </tr>\n    <tr>\n      <th>4924</th>\n      <td>HARRY</td>\n      <td>Nox.</td>\n    </tr>\n  </tbody>\n</table>\n<p>4925 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "input_file = r'.\\data' # csv파일들이 있는 디렉토리 위치\n",
    "output_file = r'.\\data\\Harry Potter.csv' # 병합하고 저장하려는 파일명\n",
    "\n",
    "allFile_list = glob.glob(os.path.join(input_file, 'Harry Potter *')) # glob함수로 sales_로 시작하는 파일들을 모은다\n",
    "allData = [] # 읽어 들인 csv파일 내용을 저장할 빈 리스트를 하나 만든다\n",
    "for file in allFile_list:\n",
    "    df = pd.read_csv(file,sep=';', encoding='iso-8859-1') # for구문으로 csv파일들을 읽어 들인다\n",
    "    df.columns = ['Character','Sentence'] # index를 Character, Sentence로 맞춘다.\n",
    "    allData.append(df) # 빈 리스트에 읽어 들인 내용을 추가한다\n",
    "dataCombine = pd.concat(allData, axis=0, ignore_index=True) # 리스트의 내용을 수직으로 병합. ignore_index=True는 인데스 값이 기존 순서를 무시하고 순서대로 정렬되도록 한다.\n",
    "dataCombine.to_csv(output_file, index=False) # to_csv함수로 저장한다. 인데스를 빼려면 False로 설정\n",
    "\n",
    "harry=pd.read_csv('./data/Harry Potter.csv')\n",
    "harry"
   ]
  },
  {
   "source": [
    "# 등장인물 별 대사 모으기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등장인물의 이름이 unifying하지 않아서 전부 대문자로 바꿔주기\n",
    "harry['Character'] = harry['Character'].str.upper()\n",
    "# Character 별로 대사 개수 순으로 나열해서 가장 많은 5명 뽑기: groupby(묶어서), count(개수 세고), sort(나열하고), [-5:](뒤에서부터 5개)\n",
    "selected = harry.groupby('Character').count().sort_values(by='Sentence')[-5:]   # Groupby의 label이었던 Character가 index가 된다.\n",
    "topfive = [character for character in selected.index]   # 대사가 가장 많은 5명의 리스트 (역순)\n",
    "\n",
    "for character in topfive:\n",
    "    condition = harry['Character'] == character # condition: harry['Character']가 topfive의 원소인 character와 같을 bool 조건\n",
    "    ownsentence = harry[condition]  # ownsentence: condition에 맞는 row만 filtering한 dataframe\n",
    "    output_file = '.\\data\\\\test\\\\HP_'+character+'.csv' # raw string이 아니라 '\\'를 쓰려면 \\\\라고 해야 함\n",
    "    ownsentence.to_csv(output_file, index=False)    # data\\test\\HP_(등장인물이름).csv 파일로 저장"
   ]
  },
  {
   "source": [
    "# Stemming\n",
    "morph analysis 기법."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "['.\\\\data\\\\\\\\test\\\\HP_DUMBLEDORE.csv', '.\\\\data\\\\\\\\test\\\\HP_HAGRID.csv', '.\\\\data\\\\\\\\test\\\\HP_HARRY.csv', '.\\\\data\\\\\\\\test\\\\HP_HERMIONE.csv', '.\\\\data\\\\\\\\test\\\\HP_RON.csv']\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'stemmer' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0a27fdeba5d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenData\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mtempData\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mtempData\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mstemData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtempData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mbag_of_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Character'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstemData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stemmer' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import glob \n",
    "\n",
    "stemmer=PorterStemmer()  \n",
    "\n",
    "bag_of_words = {} #{등장인물 : 해당인물의 stemData, ~}\n",
    "stemData = []\n",
    "input_file = r'.\\data\\\\test\\\\'\n",
    "\n",
    "allHPfile_list = glob.glob(os.path.join(input_file, 'HP_*'))\n",
    "print(allHPfile_list)\n",
    "\n",
    "for file in allHPfile_list:\n",
    "    df = pd.read_csv(file, sep=',', encoding='iso-8859-1')\n",
    "    #print(df)\n",
    "    c_df = df['Sentence'].str.replace('[^a-zA-Z ]' ,'')\n",
    "    tokenData = nltk.word_tokenize(str(c_df))\n",
    "\n",
    "    for w in tokenData:\n",
    "        tempData=[]\n",
    "        tempData=stemmer.stem(w)\n",
    "        stemData.append(tempData)\n",
    "    bag_of_words[df['Character'][0]] = stemData\n",
    "    stemData=[]\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
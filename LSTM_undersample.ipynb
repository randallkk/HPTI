{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# (MBTI 별 Data 모으기: 생략) MBTI file 불러오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('.\\data\\\\training\\\\mbti.csv')"
   ]
  },
  {
   "source": [
    "# Tokenize and Stemming\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\n",
      "normalize 완료\n"
     ]
    }
   ],
   "source": [
    "X_data = mbti['posts'].ravel()\n",
    "y_data = mbti['type'].ravel()\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data: #for sentence in X_data.ravel()\n",
    "    rm_urls = re.sub(r'http[s]?://\\S+', '', sentence)\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '', rm_urls.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "X_data = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n"
     ]
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in X_data:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        if word not in stop_words:\n",
    "            tempData.append(ps.stem(word))\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "stemData = np.array(stemData)\n",
    "\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['oi', 'went', 'break', 'month', 'ago', 'togeth', 'year', 'plan', 'life', 'around', 'relationship', 'wasnt', 'one', 'break', 'relationship', 'might', 'imagin', 'ourenfj', 'pun', 'mani', 'punswel', 'person', 'dont', 'go', 'much', 'attract', 'gener', 'see', 'chang', 'that', 'good', 'alreadi', 'may', 'ask', 'want', 'mere', 'sexualsorri', 'infp', 'im', 'realli', 'postrock', 'post', 'go', 'leav', 'sneak', 'thread', 'dthat', 'doesnt', 'sound', 'enfj', 'think', 'x', 'id', 'never', 'act', 'cold', 'toward', 'romant', 'interest', 'get', 'warm', 'side', 'think', 'like', 'someon', 'way', 'want', 'straight', 'themnot', 'realli', 'im', 'mostli', 'guitar', 'player', 'bass', 'obsess', 'im', 'person', 'ask', 'bass', 'louder', 'least', 'relev', 'like', 'tri', 'new', 'instrument', 'sometim', 'add', 'aguitar', 'mostli', 'add', 'bass', 'glockenspiel', 'harmonica', 'drum', 'ocarinaahahah', 'im', 'sorri', 'im', 'laugh', 'hard', 'realli', 'plagu', 'never', 'thought', 'possiblemostli', 'sound', 'lyric', 'ill', 'obviusli', 'consid', 'listen', 'mostli', 'post', 'math', 'rock', 'dont', 'care', 'much', 'wordshii', 'welcomei', 'two', 'infj', 'close', 'friend', 'never', 'though', 'naiv', 'actual', 'take', 'serious', 'though', 'notic', 'nonnf', 'friend', 'dont', 'seem', 'way', 'usual', 'dont', 'realli', 'know', 'isthi', 'actual', 'shouldnt', 'cooler', 'pollut', 'big', 'deal', 'peopl', 'avoid', 'better', 'isnt', 'like', 'peopl', 'dont', 'car', 'least', 'countri', 'livewhat', 'ignor', 'side', 'effect', 'slightli', 'mess', 'peopl', 'around', 'let', 'tri', 'get', 'pratic', 'exampl', 'x', 'constatli', 'tri', 'get', 'nerv', 'provok', 'orwelcomehi', 'fellow', 'enfj', 'wonder', 'deal', 'direct', 'conflict', 'within', 'group', 'your', 'feel', 'like', 'someon', 'doesnt', 'like', 'much', 'start', 'bother', 'lot', 'justwelcom', 'isnt', 'way', 'see', 'also', 'part', 'person', 'close', 'two', 'infj', 'one', 'low', 'self', 'esteem', 'amazingli', 'pessimist', 'pretti', 'much', 'everyth', 'otherenfj', 'blue', 'violetsorri', 'infj', 'enfj', 'kinda', 'relat', 'im', 'get', 'point', 'right', 'think', 'low', 'self', 'esteem', 'transvers', 'type', 'quit', 'low', 'self', 'esteem', 'im', 'peopleth', 'friend', 'say', 'welcom', 'hiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii', 'ddddi', 'dont', 'think', 'infj', 'thing', 'social', 'behavior', 'mayb', 'someth', 'good', 'make', 'peopl', 'jealou', 'mayb', 'your', 'pretti', 'smart', 'get', 'along', 'peopl', 'want', 'dwelcom', 'dwelcom', 'dwelcom', 'dddthi', 'truei', 'dont', 'know', 'consid', 'compliment', 'thing', 'make', 'day', 'well', 'done', 'pleas', 'dont', 'ever', 'stop', 'your', 'wish', 'abl', 'love', 'somethingi', 'think', 'notic', 'differ', 'behavior', 'drink', 'littl', 'much', 'get', 'fairli', 'happi', 'less', 'alcohol', 'friend', 'bit', 'time', 'never', 'get', 'realli', 'realli', 'drunk', 'probabl', 'iiv', 'walk', 'away', 'estp', 'theyr', 'realli', 'hard', 'get', 'along', 'also', 'istp', 'kinda', 'feel', 'like', 'energi', 'suck', 'longer', 'togeth', 'understand', 'anddamn', 'friendli', 'fun', 'read', 'nice', 'forum', 'envoirnmentinfj', 'probabl', 'favorit', 'type', 'notic', 'behavior', 'differ', 'group', 'peopl', 'two', 'us', 'privat', 'environ', 'usual', 'take', 'lead', 'talk', 'likei', 'agre', 'cursedsoul', 'actual', 'reciev', 'thing', 'object', 'whatev', 'doesnt', 'make', 'happi', 'realli', 'enjoy', 'peopl', 'stuff', 'mean', 'theyr', 'actual', 'give', 'time', 'andthank', 'welcom', 'ye', 'sorri', 'confus', 'thank', 'di', 'think', 'might', 'romant', 'interst', 'mayb', 'said', 'wouldnt', 'flirt', 'tri', 'get', 'feedback', 'im', 'say', 'lot', 'peopl', 'imthank', 'hi', 'wave', 'ive', 'lurk', 'around', 'forum', 'time', 'think', 'time', 'proper', 'introduct', 'im', 'ri', 'enfj', 'im', 'realli', 'interest', 'pretti', 'much']\n8675\n"
     ]
    }
   ],
   "source": [
    "print(stemData[0])\n",
    "vocab_size = len(np.unique(stemData))\n",
    "print(vocab_size)"
   ]
  },
  {
   "source": [
    "# Garbage Collection 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "88324"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "'''del [[mbti]]\n",
    "\n",
    "del sentence\n",
    "\n",
    "del [[dataCombine]]\n",
    "\n",
    "del [condition]\n",
    "\n",
    "for ownsentence in allData:\n",
    "    del ownsentence\n",
    "del allData\n",
    "\n",
    "for tokenData in tokenizedData:   # delete 'stemData'\n",
    "    for data in tokenData:\n",
    "        del data\n",
    "    del tokenData\n",
    "del tokenizedData'''\n",
    "\n",
    "for clean_sentence in normalized_text:\n",
    "    del clean_sentence\n",
    "del normalized_text\n",
    "\n",
    "del X_data\n",
    "\n",
    "del tokenData\n",
    "del tempData\n",
    "\n",
    "del sentence\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "source": [
    "# Vectorization (tfidf)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.        0.        0.        ... 0.        0.        0.       ]\n [0.0518922 0.        0.        ... 0.        0.        0.       ]\n [0.        0.        0.        ... 0.        0.        0.       ]\n ...\n [0.        0.        0.        ... 0.        0.        0.       ]\n [0.        0.        0.        ... 0.        0.        0.       ]\n [0.        0.        0.        ... 0.        0.        0.       ]]\n(8675, 271354)\n['ENFJ' 'ENFJ' 'ENFJ' ... 'ISTP' 'ISTP' 'ISTP']\n(8675,)\n"
     ]
    }
   ],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "X = tfidf.transform(stemData).toarray()\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "y = np.array(y_data)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1 0 0 0]\n [1 0 0 0]\n [1 0 0 0]\n ...\n [0 1 1 1]\n [0 1 1 1]\n [0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "b_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    # transform mbti to binary vector\n",
    "    \n",
    "    return [b_Pers[l] for l in personality]\n",
    "\n",
    "def translate_back(personality):\n",
    "    # transform binary vector to mbti personality\n",
    "    \n",
    "    s = \"\"\n",
    "    for i, l in enumerate(personality):\n",
    "        s += b_Pers_list[i][l]\n",
    "    return s\n",
    "y = np.array([translate_personality(p) for p in y])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'print(X.shape[0])\\nprint(len(X[0].toarray()[0]))'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "'''print(X.shape[0])\n",
    "print(len(X[0].toarray()[0]))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"print(tfidf_matrix.shape)\\n\\nmax_words = max(len(l) for l in X)   # post의 최대 길이(단어 수)\\nprint(max_words)\\nprint('post의 평균 길이 :{}'.format(sum(map(len, stemData))/len(stemData)))\\nprint('post의 전체 길이 :{}'.format(sum(map(len, stemData))))\\npost_num = len(stemData)    #post의 갯수\\nprint(post_num)\""
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "'''print(tfidf_matrix.shape)\n",
    "\n",
    "max_words = max(len(l) for l in X)   # post의 최대 길이(단어 수)\n",
    "print(max_words)\n",
    "print('post의 평균 길이 :{}'.format(sum(map(len, stemData))/len(stemData)))\n",
    "print('post의 전체 길이 :{}'.format(sum(map(len, stemData))))\n",
    "post_num = len(stemData)    #post의 갯수\n",
    "print(post_num)'''"
   ]
  },
  {
   "source": [
    "# Garbage Collection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import psutil\n",
    "# import os\n",
    "# import gc"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "for data in stemData:   # delete 'stemData'\n",
    "    for component in data:\n",
    "        del component\n",
    "    del data\n",
    "del stemData\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(6072, 271354)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(6072, 4)\n",
      "[[1 0 1 0]\n",
      " [0 1 1 0]\n",
      " [1 0 1 1]\n",
      " ...\n",
      " [0 0 1 1]\n",
      " [0 0 1 1]\n",
      " [1 0 1 1]]\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state=1234)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train)\n",
    "print(y_train.shape)\n",
    "print(y_train)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(X.shape[0], 128))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('.\\\\models\\\\LSTM_undersample.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "plt.plot(epochs, history.history['acc'])\n",
    "plt.plot(epochs, history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "plt.plot(epochs, history.history['loss'])\n",
    "plt.plot(epochs, history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=32)"
   ]
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# (MBTI 별 Data 모으기: 생략) MBTI file 불러오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('.\\data\\\\training\\\\mbti.csv')"
   ]
  },
  {
   "source": [
    "# Tokenize and Stemming\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\n",
      "normalize 완료\n"
     ]
    }
   ],
   "source": [
    "X_data = mbti['posts'].ravel()\n",
    "y_data = mbti['type'].ravel()\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data: #for sentence in X_data.ravel()\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '', sentence.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "X_data = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n"
     ]
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stopwords = ['http*']\n",
    "#print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in X_data:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        tempData.append(ps.stem(word))\n",
    "        #tempData = [word for word in tempData if not word in stopwords] # 불용어 제거\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "#print(stemData[0])\n",
    "\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]"
   ]
  },
  {
   "source": [
    "# Garbage Collection 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "'''del [[mbti]]\n",
    "\n",
    "del sentence\n",
    "\n",
    "del [[dataCombine]]\n",
    "\n",
    "del [condition]\n",
    "\n",
    "for ownsentence in allData:\n",
    "    del ownsentence\n",
    "del allData\n",
    "\n",
    "for tokenData in tokenizedData:   # delete 'stemData'\n",
    "    for data in tokenData:\n",
    "        del data\n",
    "    del tokenData\n",
    "del tokenizedData'''\n",
    "\n",
    "for clean_sentence in normalized_text:\n",
    "    del clean_sentence\n",
    "del normalized_text\n",
    "\n",
    "del X_data\n",
    "\n",
    "del tokenData\n",
    "del tempData\n",
    "\n",
    "del sentence\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "source": [
    "# Vectorization (tfidf)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 294178)\t0.018313994351578357\n  (0, 293671)\t0.05667299274712749\n  (0, 292648)\t0.14736037481992406\n  (0, 291475)\t0.012844119309787095\n  (0, 291298)\t0.015917293926600354\n  (0, 289803)\t0.06245131892979903\n  (0, 288699)\t0.018554642544113317\n  (0, 287300)\t0.08606730960753338\n  (0, 286757)\t0.017381440260737353\n  (0, 285917)\t0.028056984359135558\n  (0, 285593)\t0.1016963400988987\n  (0, 285472)\t0.01968822480951418\n  (0, 284394)\t0.011231264358716912\n  (0, 283395)\t0.010640763011372371\n  (0, 282902)\t0.024391435899897337\n  (0, 282448)\t0.03860584354908418\n  (0, 281916)\t0.022327030698545473\n  (0, 281840)\t0.009444409717358306\n  (0, 281370)\t0.03597244101296453\n  (0, 281297)\t0.02120252125544941\n  (0, 280685)\t0.011532506841579923\n  (0, 280567)\t0.05811753463491229\n  (0, 280476)\t0.04925191972988302\n  (0, 279481)\t0.010691768530648934\n  (0, 278952)\t0.03210151554628096\n  :\t:\n  (8674, 15476)\t0.01889596232388363\n  (8674, 14739)\t0.0730334088398339\n  (8674, 13090)\t0.009365984799265245\n  (8674, 11910)\t0.02278294631254482\n  (8674, 11203)\t0.00773853163381637\n  (8674, 9377)\t0.2163148390828034\n  (8674, 9126)\t0.029267791915343636\n  (8674, 9062)\t0.043191318862179244\n  (8674, 8825)\t0.017765014269406406\n  (8674, 8131)\t0.022077201113953306\n  (8674, 7940)\t0.0076586895864992404\n  (8674, 7530)\t0.007501234128848571\n  (8674, 7374)\t0.012452803482451766\n  (8674, 7212)\t0.013562329199055395\n  (8674, 7067)\t0.013248923361356255\n  (8674, 6959)\t0.05628108314601411\n  (8674, 6326)\t0.03748944953526852\n  (8674, 3229)\t0.05628108314601411\n  (8674, 2756)\t0.016265600556329644\n  (8674, 2144)\t0.01711878488273953\n  (8674, 2045)\t0.017595625955746087\n  (8674, 1387)\t0.015332559505403647\n  (8674, 522)\t0.030897221895662224\n  (8674, 403)\t0.0254019814089718\n  (8674, 0)\t0.13217716597324525\n['ENFJ' 'ENFJ' 'ENFJ' ... 'ISTP' 'ISTP' 'ISTP']\n['ENFJ' 'ENFJ' 'ENFJ' ... 'ISTP' 'ISTP' 'ISTP']\n"
     ]
    }
   ],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "X = tfidf.transform(stemData)\n",
    "print(X)\n",
    "\n",
    "print(y_data)\n",
    "y = np.array(y_data)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-3c22e5f7321d>, line 5)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-3c22e5f7321d>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    '''print('post의 평균 길이 :{}'.format(sum(map(len, stemData))/len(stemData)))\u001b[0m\n\u001b[1;37m                                                                             \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''print(tfidf_matrix.shape)\n",
    "\n",
    "max_words = max(len(l) for l in X)   # post의 최대 길이(단어 수)\n",
    "print(max_words)\n",
    "'''print('post의 평균 길이 :{}'.format(sum(map(len, stemData))/len(stemData)))\n",
    "print('post의 전체 길이 :{}'.format(sum(map(len, stemData))))'''\n",
    "post_num = len(stemData)    #post의 갯수\n",
    "print(post_num)'''"
   ]
  },
  {
   "source": [
    "# Garbage Collection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import psutil\n",
    "# import os\n",
    "# import gc"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "for data in stemData:   # delete 'stemData'\n",
    "    for component in data:\n",
    "        del component\n",
    "    del data\n",
    "del stemData\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'scipy.sparse.csr.csr_matrix'>, <class 'NoneType'>\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Please provide as model inputs either a single array or a list of arrays. You passed: inputs=  (0, 293671)\t0.02955070789316349\n  (0, 292648)\t0.09796763951958413\n  (0, 292429)\t0.039119493552237324\n  (0, 292224)\t0.012193632747822246\n  (0, 291475)\t0.01607338378879723\n  (0, 291298)\t0.01991921501121431\n  (0, 291185)\t0.03318935944756016\n  (0, 291156)\t0.03679040708606734\n  (0, 289363)\t0.033682024142516705\n  (0, 288699)\t0.011609822498621039\n  (0, 288529)\t0.024932895368988028\n  (0, 287950)\t0.009637792632344915\n  (0, 287427)\t0.00753071391516984\n  (0, 285593)\t0.07520192861562311\n  (0, 284394)\t0.014055025347945704\n  (0, 283536)\t0.01221605924451546\n  (0, 283395)\t0.013316060335651062\n  (0, 283052)\t0.009222820738824754\n  (0, 282903)\t0.024077541609769052\n  (0, 282902)\t0.02289294234133414\n  (0, 282725)\t0.05385316289988784\n  (0, 282720)\t0.025848888602759232\n  (0, 282448)\t0.0724681690766283\n  (0, 281840)\t0.02954730067208133\n  (0, 281370)\t0.007502770155945566\n  :\t:\n  (6071, 13090)\t0.015222912089259797\n  (6071, 12815)\t0.014985359687241386\n  (6071, 12682)\t0.019239513503011202\n  (6071, 11203)\t0.012577746952011299\n  (6071, 11015)\t0.014554172908724008\n  (6071, 10980)\t0.02392249534882054\n  (6071, 10801)\t0.03150323865857783\n  (6071, 9738)\t0.045737954917848385\n  (6071, 9377)\t0.1855588634510059\n  (6071, 9062)\t0.07521491685259875\n  (6071, 7530)\t0.006096028882802334\n  (6071, 7374)\t0.0202400427441504\n  (6071, 6919)\t0.18943053504637572\n  (6071, 6326)\t0.025388796390232282\n  (6071, 4913)\t0.0249230960206615\n  (6071, 4777)\t0.01016468109739225\n  (6071, 4189)\t0.009764477901430644\n  (6071, 2144)\t0.013911925063413362\n  (6071, 2067)\t0.010939208941227073\n  (6071, 1823)\t0.017650119963690612\n  (6071, 1657)\t0.02923170087144471\n  (6071, 1530)\t0.019593673663862585\n  (6071, 1086)\t0.012027843186240126\n  (6071, 522)\t0.08034959745088072\n  (6071, 0)\t0.11229899927935708",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-fbe3a226c9a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2417\u001b[0m     \u001b[1;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2419\u001b[1;33m       \u001b[0mall_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2420\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m   2580\u001b[0m     \u001b[1;31m# or lists of arrays, and extract a flat list of inputs from the passed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2581\u001b[0m     \u001b[1;31m# structure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2582\u001b[1;33m     \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_input_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2584\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mvalidate_input_types\u001b[1;34m(inp, orig_inp, allow_dict, field_name)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     raise ValueError(\n\u001b[0;32m   1150\u001b[0m         \u001b[1;34m'Please provide as model inputs either a single array or a list of '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         'arrays. You passed: {}={}'.format(field_name, orig_inp))\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Please provide as model inputs either a single array or a list of arrays. You passed: inputs=  (0, 293671)\t0.02955070789316349\n  (0, 292648)\t0.09796763951958413\n  (0, 292429)\t0.039119493552237324\n  (0, 292224)\t0.012193632747822246\n  (0, 291475)\t0.01607338378879723\n  (0, 291298)\t0.01991921501121431\n  (0, 291185)\t0.03318935944756016\n  (0, 291156)\t0.03679040708606734\n  (0, 289363)\t0.033682024142516705\n  (0, 288699)\t0.011609822498621039\n  (0, 288529)\t0.024932895368988028\n  (0, 287950)\t0.009637792632344915\n  (0, 287427)\t0.00753071391516984\n  (0, 285593)\t0.07520192861562311\n  (0, 284394)\t0.014055025347945704\n  (0, 283536)\t0.01221605924451546\n  (0, 283395)\t0.013316060335651062\n  (0, 283052)\t0.009222820738824754\n  (0, 282903)\t0.024077541609769052\n  (0, 282902)\t0.02289294234133414\n  (0, 282725)\t0.05385316289988784\n  (0, 282720)\t0.025848888602759232\n  (0, 282448)\t0.0724681690766283\n  (0, 281840)\t0.02954730067208133\n  (0, 281370)\t0.007502770155945566\n  :\t:\n  (6071, 13090)\t0.015222912089259797\n  (6071, 12815)\t0.014985359687241386\n  (6071, 12682)\t0.019239513503011202\n  (6071, 11203)\t0.012577746952011299\n  (6071, 11015)\t0.014554172908724008\n  (6071, 10980)\t0.02392249534882054\n  (6071, 10801)\t0.03150323865857783\n  (6071, 9738)\t0.045737954917848385\n  (6071, 9377)\t0.1855588634510059\n  (6071, 9062)\t0.07521491685259875\n  (6071, 7530)\t0.006096028882802334\n  (6071, 7374)\t0.0202400427441504\n  (6071, 6919)\t0.18943053504637572\n  (6071, 6326)\t0.025388796390232282\n  (6071, 4913)\t0.0249230960206615\n  (6071, 4777)\t0.01016468109739225\n  (6071, 4189)\t0.009764477901430644\n  (6071, 2144)\t0.013911925063413362\n  (6071, 2067)\t0.010939208941227073\n  (6071, 1823)\t0.017650119963690612\n  (6071, 1657)\t0.02923170087144471\n  (6071, 1530)\t0.019593673663862585\n  (6071, 1086)\t0.012027843186240126\n  (6071, 522)\t0.08034959745088072\n  (6071, 0)\t0.11229899927935708"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state=1234)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, post_num))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.25869917 0.         0.         ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6072, 295647)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('.\\\\models\\\\LSTM_undersample.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "plt.plot(epochs, history.history['acc'])\n",
    "plt.plot(epochs, history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "plt.plot(epochs, history.history['loss'])\n",
    "plt.plot(epochs, history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MBTI별 포스팅 모으기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('./data/mbti_1.csv')\n",
    "\n",
    "types = mbti.groupby('type').count()\n",
    "\n",
    "output_file = '.\\data\\\\training\\\\mbti.csv' # raw string이 아니라 '\\'를 쓰려면 \\\\라고 해야 함\n",
    "allData = []\n",
    "\n",
    "for type in types.index:\n",
    "    condition = mbti['type'] == type # condition: mbti['type']가 topfive의 원소인 type 같을 bool 조건\n",
    "    ownsentence = mbti[condition]  # ownsentence: condition에 맞는 row만 filtering한 dataframe\n",
    "    allData.append(ownsentence)\n",
    "dataCombine = pd.concat(allData, axis=0, ignore_index=True)\n",
    "dataCombine.to_csv(output_file, index=False)\n",
    "\n",
    "#input_file = r'.\\data\\training'\n",
    "#allFile_list = glob.glob(os.path.join(input_file, 'mbti_*'))\n",
    "#for file in allFile_list:\n",
    "#    csv = pd.read_csv(file,sep=';', encoding='iso-8859-1') # for구문으로 csv파일들을 읽어 들인다\n",
    "#    cleanMbti = csv['posts'].str.replace('[^A-Za-z\\s]+', '')"
   ]
  },
  {
   "source": [
    "# OverSampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      posts\ntype       \nENFJ    190\nENFP    675\nENTJ    231\nENTP    685\nESFJ     42\nESFP     48\nESTJ     39\nESTP     89\nINFJ   1470\nINFP   1832\nINTJ   1091\nINTP   1304\nISFJ    166\nISFP    271\nISTJ    205\nISTP    337\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'INFJ': 1832,\n",
       "         'ENTP': 1832,\n",
       "         'INTP': 1832,\n",
       "         'INTJ': 1832,\n",
       "         'ENTJ': 1832,\n",
       "         'ENFJ': 1832,\n",
       "         'INFP': 1832,\n",
       "         'ENFP': 1832,\n",
       "         'ISFP': 1832,\n",
       "         'ISTP': 1832,\n",
       "         'ISFJ': 1832,\n",
       "         'ISTJ': 1832,\n",
       "         'ESTP': 1832,\n",
       "         'ESFP': 1832,\n",
       "         'ESTJ': 1832,\n",
       "         'ESFJ': 1832})"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "mbti=pd.read_csv('./data/mbti_1.csv')\n",
    "\n",
    "types = mbti.groupby('type').count()\n",
    "\n",
    "output_file = '.\\data\\\\training\\\\mbti.csv' # raw string이 아니라 '\\'를 쓰려면 \\\\라고 해야 함\n",
    "allData = []\n",
    "\n",
    "for type in types.index:\n",
    "    condition = mbti['type'] == type # condition: mbti['type']가 topfive의 원소인 type 같을 bool 조건\n",
    "    ownsentence = mbti[condition]  # ownsentence: condition에 맞는 row만 filtering한 dataframe\n",
    "    allData.append(ownsentence)\n",
    "dataCombine = pd.concat(allData, axis=0, ignore_index=True)\n",
    "dataCombine.to_csv(output_file, index=False)\n",
    "\n",
    "print(types)\n",
    "\n",
    "X_data = mbti['posts']\n",
    "X = X_data.values.reshape(-1, 1)\n",
    "\n",
    "y_data = mbti['type']\n",
    "y = y_data.values.reshape(-1, 1)\n",
    "\n",
    "X_resampled, y_resampled = RandomOverSampler(random_state=0).fit_sample(X, y)\n",
    "#format(Counter(y_resampled))\n",
    "X_resampled #ndarray\n",
    "Counter(y_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# tokenize and stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\n",
      "normalize 완료\n"
     ]
    }
   ],
   "source": [
    "mbti= dataCombine\n",
    "X_data = X_resampled.ravel()\n",
    "y_data = y_resampled\n",
    "\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data: #for sentence in X_data.ravel()\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '', sentence.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "X_data = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n",
      "[[0.16126    0.         0.         ... 0.         0.         0.        ]\n",
      " [0.15308429 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.0701916  0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.14387306 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.25156428 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.20342093 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stopwords = ['http*']\n",
    "#print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in X_data:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        tempData.append(ps.stem(word))\n",
    "        #tempData = [word for word in tempData if not word in stopwords] # 불용어 제거\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "#print(stemData[0])\n",
    "\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]\n",
    "\n",
    "#vectorization (tfidf)\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "print(tfidf.transform(stemData).toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
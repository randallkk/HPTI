{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MBTI별 포스팅 모으기 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('.\\data\\\\training\\\\mbti.csv')"
   ]
  },
  {
   "source": [
    "# tokenize and stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\n",
      "normalize 완료\n"
     ]
    }
   ],
   "source": [
    "X_data = mbti['posts'].ravel()\n",
    "y_data = mbti['type']\n",
    "\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data: #for sentence in X_data.ravel()\n",
    "    rm_urls = re.sub(r'http[s]?://\\S+', '', sentence)\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '', rm_urls.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "X_data = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gc\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n",
      "['i', 'do', 'stuff', 'like', 'disthey', 'no', 'longer', 'becom', 'outlandish', 'onc', 'they', 'start', 'becom', 'realiti', 'i', 'think', 'you', 'also', 'have', 'to', 'alway', 'rememb', 'whi', 'you', 'start', 'to', 'make', 'music', 'in', 'the', 'first', 'place', 'the', 'way', 'it', 'feel', 'or', 'how', 'you', 'make', 'othersim', 'a', 'musician', 'myself', 'but', 'i', 'start', 'record', 'when', 'i', 'wa', 'a', 'junior', 'in', 'high', 'school', 'i', 'start', 'write', 'music', 'that', 'would', 'person', 'appeal', 'to', 'me', 'and', 'honestli', 'it', 'actual', 'work', 'out', 'pretti', 'well', 'sothen', 'again', 'i', 'hate', 'to', 'gener', 'but', 'that', 'wa', 'my', 'experienceboth', 'loldat', 'an', 'esfj', 'wa', 'my', 'last', 'girl', 'but', 'she', 'wa', 'crazi', 'bruh', 'veri', 'on', 'and', 'off', 'i', 'call', 'her', 'out', 'on', 'it', 'becaus', 'it', 'seem', 'like', 'she', 'wa', 'never', 'consist', 'on', 'how', 'she', 'actual', 'felt', 'but', 'it', 'cool', 'doe', 'nikka', 'youextrem', 'attract', 'to', 'esfj', 'but', 'would', 'love', 'to', 'date', 'an', 'infp', 'at', 'one', 'point', 'dyeah', 'happen', 'quit', 'often', 'i', 'guess', 'that', 'whi', 'i', 'usual', 'stay', 'quiet', 'most', 'of', 'the', 'time', 'i', 'listen', 'to', 'thi', 'as', 'i', 'watch', 'the', 'sunsetthat', 'wa', 'creepi', 'af', 'bro', 'what', 'the', 'helli', 'saw', 'thi', 'thi', 'turkey', 'onc', 'an', 'i', 'wa', 'straight', 'chewin', 'that', 'breast', 'to', 'piec', 'bruh', 'left', 'no', 'skin', 'on', 'it', 'ill', 'grab', 'you', 'a', 'turkey', 'with', 'those', 'nice', 'thigh', 'your', 'go', 'to', 'love', 'stickin', 'your', 'face', 'in', 'em', 'manseek', 'is', 'an', 'independ', 'woman', 'that', 'dont', 'need', 'no', 'man', 'but', 'still', 'tryna', 'look', 'good', 'for', 'everyon', 'els', 'can', 'i', 'get', 'a', 'heeellll', 'yeaeveryon', 'eggsi', 'like', 'fuck', 'zebra', 'cake', 'what', 'a', 'kinksterintp', 'are', 'fun', 'to', 'talk', 'to', 'intellectu', 'dont', 'you', 'mean', 'yum', 'circl', 'lolchicken', 'pot', 'pie', 'oh', 'mer', 'gerdd', 'yessssim', 'pretti', 'sarcast', 'but', 'i', 'usual', 'just', 'use', 'extrem', 'bad', 'pun', 'to', 'get', 'laugh', 'lolhigh', 'expect', 'come', 'at', 'a', 'high', 'price', 'we', 'just', 'expect', 'too', 'much', 'well', 'have', 'to', 'learn', 'how', 'to', 'expect', 'less', 'from', 'our', 'potenti', 'signific', 'othersi', 'realli', 'want', 'a', 'shred', 'beef', 'taco', 'or', 'multipl', 'of', 'those', 'right', 'now', 'also', 'funni', 'name', 'for', 'thing', 'carrot', 'dirt', 'rocket', 'pizza', 'yum', 'circl', 'and', 'the', 'one', 'i', 'actual', 'made', 'up', 'hot', 'sauc', 'danger', 'liquidthey', 'remind', 'me', 'of', 'michael', 'bubl', 'with', 'a', 'hint', 'of', 'snoop', 'dogg', 'ya', 'feeli', 'have', 'noooo', 'idea', 'how', 'you', 'feel', 'right', 'now', 'but', 'thank', 'you', 'everyon', 'all', 'aboard', 'the', 'feel', 'traini', 'use', 'to', 'want', 'to', 'bring', 'peac', 'to', 'the', 'world', 'but', 'that', 'all', 'chang', 'when', 'the', 'fire', 'nation', 'attack', 'lol', 'on', 'the', 'real', 'doe', 'i', 'use', 'to', 'think', 'the', 'same', 'way', 'but', 'if', 'you', 'can', 'notic', 'these', 'thing', 'about', 'previouscaus', 'it', 'dope', 'lol', 'not', 'so', 'much', 'a', 'good', 'lyric', 'song', 'but', 'it', 'bump', 'like', 'a', 'motherfuggah', 'thought', 'thi', 'would', 'be', 'fittinganorex', 'or', 'obeseim', 'also', 'an', 'w', 'but', 'im', 'onli', 'concern', 'about', 'be', 'vulner', 'i', 'hate', 'be', 'in', 'a', 'posit', 'where', 'id', 'have', 'to', 'appeal', 'to', 'someon', 'i', 'guess', 'it', 'more', 'the', 'fact', 'that', 'i', 'dont', 'think', 'ill', 'ever', 'be', 'understoodmak', 'peopl', 'laugh', 'is', 'alway', 'a', 'great', 'time', 'you', 'guy', 'are', 'way', 'too', 'creativ', 'for', 'me', 'i', 'would', 'wake', 'up', 'as', 'a', 'walk', 'dead', 'lelgo', 'to', 'have', 'to', 'say', 'yesgood', 'ol', 'georg', 'carlin', 'said', 'environmentalist', 'dont', 'give', 'a', 'shit', 'about', 'the', 'earth', 'it', 'our', 'selfish', 'to', 'want', 'to', 'continu', 'surviv', 'as', 'a', 'speci', 'the', 'earth', 'will', 'take', 'it', 'cours', 'and', 'wipe', 'us', 'outhaha', 'i', 'dont', 'know', 'man', 'relationship', 'or', 'not', 'ive', 'alway', 'been', 'happi', 'relationship', 'were', 'and', 'are', 'never', 'a', 'prioriti', 'to', 'me', 'lolhahaha', 'befor', 'i', 'answer', 'i', 'just', 'want', 'to', 'say', 'that', 'most', 'of', 'my', 'favorit', 'convers', 'are', 'with', 'intjintp', 'it', 'the', 'idea', 'of', 'compass', 'just', 'allow', 'him', 'to', 'follow', 'hi', 'path', 'into', 'hi', 'nirvana', 'is', 'fine', 'withi', 'am', 'pretti', 'damn', 'happi', 'about', 'it', 'less', 'stress', 'and', 'carefre', 'i', 'can', 'do', 'thing', 'and', 'not', 'be', 'critic', 'for', 'it', 'but', 'then', 'again', 'i', 'probabl', 'havent', 'been', 'in', 'a', 'great', 'relationship', 'yet', 'i', 'can', 'do', 'with', 'or', 'withouti', 'tend', 'to', 'debat', 'quit', 'a', 'bit', 'on', 'that', 'issu', 'i', 'am', 'also', 'practic', 'buddhist', 'philosophi', 'though', 'i', 'know', 'when', 'to', 'stop', 'and', 'allow', 'them', 'to', 'find', 'their', 'own', 'path', 'peopl', 'can', 'believ', 'what', 'they', 'want', 'some', 'arentbut', 'then', 'again', 'there', 'is', 'no', 'such', 'thing', 'as', 'dark', 'assum', 'that', 'the', 'einstein', 'theori', 'is', 'true', 'it', 'defin', 'as', 'a', 'lack', 'of', 'light', 'now', 'when', 'that', 'appli', 'to', 'selfishselfless', 'it', 'mayb', 'onli', 'on', 'aa', 'harsh', 'as', 'eggsi', 'seem', 'he', 'right', 'you', 'cant', 'offer', 'emot', 'to', 'peopl', 'who', 'dont', 'appreci', 'them', 'ye', 'i', 'know', 'the', 'feel', 'of', 'pour', 'yourself', 'out', 'and', 'get', 'taken', 'for', 'grant', 'there', 'a', 'lesson', 'oni', 'wish', 'i', 'wa', 'a', 'littl', 'bit', 'taller', 'i', 'wish', 'i', 'wa', 'a', 'baller', 'i', 'wish', 'i', 'had', 'a', 'girl', 'who', 'look', 'good', 'i', 'would', 'call', 'her', 'lol', 'on', 'the', 'real', 'doe', 'i', 'wish', 'i', 'had', 'more', 'of', 'a', 'drive', 'to', 'pursu', 'my', 'dream', 'than', 'human', 'alway', 'had', 'thi', 'thought', 'about', 'selfish', 'in', 'a', 'sens', 'our', 'innat', 'desir', 'to', 'make', 'peopl', 'happi', 'make', 'us', 'happi', 'doe', 'that', 'make', 'us', 'selfish', 'or', 'selfless', 'is', 'selfish', 'measur', 'upon', 'a', 'spectrum', 'and', 'i', 'dont', 'know', 'how', 'i', 'feel', 'about', 'thiscan', 'we', 'all', 'just', 'get', 'along', 'i', 'also', 'have', 'myself', 'a', 'copi', 'ive', 'gotten', 'to', 'the', 'first', 'page', 'and', 'put', 'it', 'down', 'though', 'loldo', 'you', 'draw', 'scrap', 'book', 'watercolor', 'paint', 'have', 'tattoo', 'like', 'dinosuar', 'eat', 'turtl', 'make', 'guitar', 'play', 'piano', 'have', 'pet', 'favorit', 'danim', 'list', 'some', 'ish', 'yall', 'like', 'to', 'do', 'thi', 'forum', 'is', 'just', 'too', 'neither', 'have', 'an', 'interview', 'today', 'doe', 'should', 'be', 'bangin', 'write', 'enjoy', 'the', 'morn', 'weather', 'with', 'coffe', 'when', 'night', 'hit', 'though', 'im', 'up', 'all', 'night', 'for', 'good', 'fun', 'and', 'meet', 'folk', 'iwel', 'i', 'had', 'a', 'lot', 'of', 'time', 'to', 'figur', 'that', 'out', 'when', 'i', 'wa', 'younger', 'ive', 'alway', 'been', 'interest', 'in', 'environment', 'scienc', 'and', 'do', 'thing', 'that', 'would', 'posit', 'affect', 'my', 'commun', 'but', 'that', 'my', 'fallsubmit', 'good', 'surveyi', 'think', 'you', 'should', 'tell', 'him', 'how', 'you', 'cant', 'alway', 'have', 'a', 'heart', 'to', 'heart', 'moment', 'with', 'him', 'and', 'that', 'you', 'guy', 'should', 'focu', 'on', 'yourselv', 'for', 'now', 'and', 'just', 'hangout', 'when', 'we', 'enfj', 'fall', 'in', 'love', 'it', 'liketh', 'asshol', 'when', 'shit', 'hit', 'the', 'fan', 'major', 'in', 'environment', 'scienc', 'im', 'interest', 'in', 'the', 'connect', 'between', 'the', 'natur', 'world', 'and', 'humanssuch', 'a', 'vaug', 'question', 'but', 'ye', 'most', 'of', 'the', 'time', 'i', 'dont', 'give', 'up', 'on', 'thing', 'im', 'interest', 'in', 'person', 'i', 'hate', 'losingbecaus', 'the', 'descript', 'of', 'an', 'enfj', 'fit', 'me', 'more', 'as', 'far', 'as', 'i', 'know']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in X_data:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        if word not in stop_words:\n",
    "            tempData.append(ps.stem(word))\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "print(stemData[0])\n",
    "stemData = np.array(stemData)\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]\n",
    "del [[mbti]]\n",
    "gc.collect()"
   ]
  },
  {
   "source": [
    "# Vectorization (tfidf)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.00073627 0.00497775 0.01191725 ... 0.00053569 0.00023727 0.00324838]\n [0.00073627 0.00497775 0.01191725 ... 0.00053569 0.00023727 0.00324838]\n [0.00073627 0.00497775 0.01191725 ... 0.00053569 0.00023727 0.00324838]\n [0.00073627 0.00497775 0.01191725 ... 0.00053569 0.00023727 0.00324838]\n [0.00073627 0.00497775 0.01191725 ... 0.00053569 0.00023727 0.00324838]]\n"
     ]
    }
   ],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "print(tfidf.transform(stemData).toarray())\n",
    "X = tfidf.transform(stemData)\n",
    "y = np.array(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'ENFJ': 1832,\n",
       "         'ENFP': 1832,\n",
       "         'ENTJ': 1832,\n",
       "         'ENTP': 1832,\n",
       "         'ESFJ': 1832,\n",
       "         'ESFP': 1832,\n",
       "         'ESTJ': 1832,\n",
       "         'ESTP': 1832,\n",
       "         'INFJ': 1832,\n",
       "         'INFP': 1832,\n",
       "         'INTJ': 1832,\n",
       "         'INTP': 1832,\n",
       "         'ISFJ': 1832,\n",
       "         'ISFP': 1832,\n",
       "         'ISTJ': 1832,\n",
       "         'ISTP': 1832})"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "'''\n",
    "X_data = mbti['posts']\n",
    "X = X_data.values.reshape(-1, 1)\n",
    "\n",
    "y_data = mbti['type']\n",
    "y = y_data.values.reshape(-1, 1)'''\n",
    "\n",
    "Counter(y_data)\n",
    "X_resampled, y_resampled = SMOTE(random_state=0).fit_sample(X, y)\n",
    "#format(Counter(y_resampled))\n",
    "X_resampled #ndarray\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "source": [
    "# XGBoost_Oversampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building Start\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.03, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=500, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n",
    "\n",
    "param = {}\n",
    "param['learning_rate'] = 0.03\n",
    "param['n_estimators'] = 500\n",
    "param['max_depth'] = 10\n",
    "\n",
    "\n",
    "print(\"Model building Start\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size= 0.3, random_state=1234)\n",
    "model = XGBClassifier(**param)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building End\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "print(\"Model building End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['XGBoost_smote_parameters.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'XGBoost_smote_parameters.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation Start\n0.9160791448715033\n[0.97012302 0.90757856 0.95978062 0.87921847 0.99415205 0.99620493\n 1.         0.98327138 0.77094972 0.73127036 0.8359375  0.8036036\n 0.98568873 0.95644283 0.96346154 0.93965517]\n[0.97354497 0.87678571 0.95454545 0.87610619 0.99029126 0.98499062\n 0.98954704 0.98878505 0.76243094 0.78496503 0.81992337 0.79218472\n 0.97868561 0.95644283 0.96905222 0.96631206]\n[0.97183099 0.89191644 0.95715588 0.87765957 0.9922179  0.99056604\n 0.99474606 0.9860205  0.76666667 0.75716695 0.827853   0.79785331\n 0.98217469 0.95644283 0.96624879 0.9527972 ]\nEvaluation END\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"Evaluation Start\")\n",
    "# labels과 guesses\n",
    "labels = preds\n",
    "guesses = y_test\n",
    "\n",
    "print(accuracy_score(labels, guesses))\n",
    "print(recall_score(labels, guesses, average=None))\n",
    "print(precision_score(labels, guesses, average=None))\n",
    "print(f1_score(labels, guesses, average=None))\n",
    "\n",
    "print(\"Evaluation END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4b67a3d5e037>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mskf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskf\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[0;32m    402\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    240\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[0;32m    241\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[1;32m--> 242\u001b[1;33m     scores = parallel(\n\u001b[0m\u001b[0;32m    243\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    244\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    826\u001b[0m                                 missing=self.missing, nthread=self.n_jobs)\n\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m         self._Booster = train(xgb_options, train_dmatrix,\n\u001b[0m\u001b[0;32m    829\u001b[0m                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m                               \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     return _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    209\u001b[0m                            \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1159\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1160\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1161\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "score = cross_val_score(model, X_train, y_train, cv=skf )\n",
    "print(score)\n",
    "print(score.mean())"
   ]
  },
  {
   "source": [
    "# HarryPotter MBTI"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['.\\\\data\\\\\\\\test\\\\HP_DUMBLEDORE.csv', '.\\\\data\\\\\\\\test\\\\HP_HAGRID.csv', '.\\\\data\\\\\\\\test\\\\HP_HARRY.csv', '.\\\\data\\\\\\\\test\\\\HP_HERMIONE.csv', '.\\\\data\\\\\\\\test\\\\HP_RON.csv']\n",
      "stemming 시작\n",
      "stemming 완료\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import glob\n",
    "\n",
    "'''HP data Normalization and Stemming'''\n",
    "h_normalized_text = []\n",
    "h_stemData = []\n",
    "\n",
    "ps=PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "input_file = r'.\\data\\\\test\\\\'\n",
    "\n",
    "allHPfile_list = glob.glob(os.path.join(input_file, 'HP_*'))\n",
    "print(allHPfile_list)\n",
    "\n",
    "harryPotter={}\n",
    "for file in allHPfile_list:\n",
    "    df = pd.read_csv(file, sep=',', encoding='iso-8859-1')\n",
    "    harryPotter[df['Character'][0]] = df['Sentence'].ravel()\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "for element in harryPotter.keys():\n",
    "    for sentence in harryPotter[element]:\n",
    "        h_clean_sentence = re.sub('[^A-Za-z\\s]+', '', sentence.lower())\n",
    "        h_normalized_text.append(h_clean_sentence)\n",
    "        for sentence in h_normalized_text:\n",
    "            tokenData = nltk.word_tokenize(sentence)\n",
    "            for word in tokenData:\n",
    "                if word not in stop_words:\n",
    "                    h_stemData.append(ps.stem(word))\n",
    "    harryPotter[element] = h_stemData\n",
    "\n",
    "print(\"stemming 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NotFittedError",
     "evalue": "The TF-IDF vectorizer is not fitted",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c5da35efd07e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mhy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mHX_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHX_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mHY_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents, copy)\u001b[0m\n\u001b[0;32m   1870\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1871\u001b[0m         \"\"\"\n\u001b[1;32m-> 1872\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1874\u001b[0m         \u001b[1;31m# FIXME Remove copy parameter support in 0.24\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1019\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: The TF-IDF vectorizer is not fitted"
     ]
    }
   ],
   "source": [
    "'''HP data Vectorization'''\n",
    "\n",
    "hx_stemData = []\n",
    "hy=[]\n",
    "\n",
    "for element in harryPotter.keys():\n",
    "    hx_stemData.append(harryPotter[element])\n",
    "    hy.append(element)\n",
    "\n",
    "HX_data = tfidf.transform(hx_stemData)\n",
    "print(HX_data)\n",
    "HY_data = np.array(hy)\n",
    "print(HY_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ESFP' 'ESFP' 'ESFP' 'ESFP' 'ESFP']\n"
     ]
    }
   ],
   "source": [
    "import joblib  \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n",
    "\n",
    "model = joblib.load('XGBoost_smote_parameters.pkl')\n",
    "# xgb_model_loaded.get_xgb_params()\n",
    "harrypotter_predict = model.predict(HX_data)\n",
    "print(harrypotter_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
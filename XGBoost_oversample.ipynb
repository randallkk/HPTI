{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MBTI별 포스팅 모으기 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('.\\data\\\\training\\\\mbti.csv')"
   ]
  },
  {
   "source": [
    "# tokenize and stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\n",
      "normalize 완료\n"
     ]
    }
   ],
   "source": [
    "X_data = mbti['posts'].ravel()\n",
    "y_data = mbti['type']\n",
    "\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data: #for sentence in X_data.ravel()\n",
    "    rm_urls = re.sub(r'http[s]?://\\S+', '', sentence)\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '', rm_urls.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "X_data = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gc\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n",
      "['i', 'do', 'stuff', 'like', 'disthey', 'no', 'longer', 'becom', 'outlandish', 'onc', 'they', 'start', 'becom', 'realiti', 'i', 'think', 'you', 'also', 'have', 'to', 'alway', 'rememb', 'whi', 'you', 'start', 'to', 'make', 'music', 'in', 'the', 'first', 'place', 'the', 'way', 'it', 'feel', 'or', 'how', 'you', 'make', 'othersim', 'a', 'musician', 'myself', 'but', 'i', 'start', 'record', 'when', 'i', 'wa', 'a', 'junior', 'in', 'high', 'school', 'i', 'start', 'write', 'music', 'that', 'would', 'person', 'appeal', 'to', 'me', 'and', 'honestli', 'it', 'actual', 'work', 'out', 'pretti', 'well', 'sothen', 'again', 'i', 'hate', 'to', 'gener', 'but', 'that', 'wa', 'my', 'experienceboth', 'loldat', 'an', 'esfj', 'wa', 'my', 'last', 'girl', 'but', 'she', 'wa', 'crazi', 'bruh', 'veri', 'on', 'and', 'off', 'i', 'call', 'her', 'out', 'on', 'it', 'becaus', 'it', 'seem', 'like', 'she', 'wa', 'never', 'consist', 'on', 'how', 'she', 'actual', 'felt', 'but', 'it', 'cool', 'doe', 'nikka', 'youextrem', 'attract', 'to', 'esfj', 'but', 'would', 'love', 'to', 'date', 'an', 'infp', 'at', 'one', 'point', 'dyeah', 'happen', 'quit', 'often', 'i', 'guess', 'that', 'whi', 'i', 'usual', 'stay', 'quiet', 'most', 'of', 'the', 'time', 'i', 'listen', 'to', 'thi', 'as', 'i', 'watch', 'the', 'sunsetthat', 'wa', 'creepi', 'af', 'bro', 'what', 'the', 'helli', 'saw', 'thi', 'thi', 'turkey', 'onc', 'an', 'i', 'wa', 'straight', 'chewin', 'that', 'breast', 'to', 'piec', 'bruh', 'left', 'no', 'skin', 'on', 'it', 'ill', 'grab', 'you', 'a', 'turkey', 'with', 'those', 'nice', 'thigh', 'your', 'go', 'to', 'love', 'stickin', 'your', 'face', 'in', 'em', 'manseek', 'is', 'an', 'independ', 'woman', 'that', 'dont', 'need', 'no', 'man', 'but', 'still', 'tryna', 'look', 'good', 'for', 'everyon', 'els', 'can', 'i', 'get', 'a', 'heeellll', 'yeaeveryon', 'eggsi', 'like', 'fuck', 'zebra', 'cake', 'what', 'a', 'kinksterintp', 'are', 'fun', 'to', 'talk', 'to', 'intellectu', 'dont', 'you', 'mean', 'yum', 'circl', 'lolchicken', 'pot', 'pie', 'oh', 'mer', 'gerdd', 'yessssim', 'pretti', 'sarcast', 'but', 'i', 'usual', 'just', 'use', 'extrem', 'bad', 'pun', 'to', 'get', 'laugh', 'lolhigh', 'expect', 'come', 'at', 'a', 'high', 'price', 'we', 'just', 'expect', 'too', 'much', 'well', 'have', 'to', 'learn', 'how', 'to', 'expect', 'less', 'from', 'our', 'potenti', 'signific', 'othersi', 'realli', 'want', 'a', 'shred', 'beef', 'taco', 'or', 'multipl', 'of', 'those', 'right', 'now', 'also', 'funni', 'name', 'for', 'thing', 'carrot', 'dirt', 'rocket', 'pizza', 'yum', 'circl', 'and', 'the', 'one', 'i', 'actual', 'made', 'up', 'hot', 'sauc', 'danger', 'liquidthey', 'remind', 'me', 'of', 'michael', 'bubl', 'with', 'a', 'hint', 'of', 'snoop', 'dogg', 'ya', 'feeli', 'have', 'noooo', 'idea', 'how', 'you', 'feel', 'right', 'now', 'but', 'thank', 'you', 'everyon', 'all', 'aboard', 'the', 'feel', 'traini', 'use', 'to', 'want', 'to', 'bring', 'peac', 'to', 'the', 'world', 'but', 'that', 'all', 'chang', 'when', 'the', 'fire', 'nation', 'attack', 'lol', 'on', 'the', 'real', 'doe', 'i', 'use', 'to', 'think', 'the', 'same', 'way', 'but', 'if', 'you', 'can', 'notic', 'these', 'thing', 'about', 'previouscaus', 'it', 'dope', 'lol', 'not', 'so', 'much', 'a', 'good', 'lyric', 'song', 'but', 'it', 'bump', 'like', 'a', 'motherfuggah', 'thought', 'thi', 'would', 'be', 'fittinganorex', 'or', 'obeseim', 'also', 'an', 'w', 'but', 'im', 'onli', 'concern', 'about', 'be', 'vulner', 'i', 'hate', 'be', 'in', 'a', 'posit', 'where', 'id', 'have', 'to', 'appeal', 'to', 'someon', 'i', 'guess', 'it', 'more', 'the', 'fact', 'that', 'i', 'dont', 'think', 'ill', 'ever', 'be', 'understoodmak', 'peopl', 'laugh', 'is', 'alway', 'a', 'great', 'time', 'you', 'guy', 'are', 'way', 'too', 'creativ', 'for', 'me', 'i', 'would', 'wake', 'up', 'as', 'a', 'walk', 'dead', 'lelgo', 'to', 'have', 'to', 'say', 'yesgood', 'ol', 'georg', 'carlin', 'said', 'environmentalist', 'dont', 'give', 'a', 'shit', 'about', 'the', 'earth', 'it', 'our', 'selfish', 'to', 'want', 'to', 'continu', 'surviv', 'as', 'a', 'speci', 'the', 'earth', 'will', 'take', 'it', 'cours', 'and', 'wipe', 'us', 'outhaha', 'i', 'dont', 'know', 'man', 'relationship', 'or', 'not', 'ive', 'alway', 'been', 'happi', 'relationship', 'were', 'and', 'are', 'never', 'a', 'prioriti', 'to', 'me', 'lolhahaha', 'befor', 'i', 'answer', 'i', 'just', 'want', 'to', 'say', 'that', 'most', 'of', 'my', 'favorit', 'convers', 'are', 'with', 'intjintp', 'it', 'the', 'idea', 'of', 'compass', 'just', 'allow', 'him', 'to', 'follow', 'hi', 'path', 'into', 'hi', 'nirvana', 'is', 'fine', 'withi', 'am', 'pretti', 'damn', 'happi', 'about', 'it', 'less', 'stress', 'and', 'carefre', 'i', 'can', 'do', 'thing', 'and', 'not', 'be', 'critic', 'for', 'it', 'but', 'then', 'again', 'i', 'probabl', 'havent', 'been', 'in', 'a', 'great', 'relationship', 'yet', 'i', 'can', 'do', 'with', 'or', 'withouti', 'tend', 'to', 'debat', 'quit', 'a', 'bit', 'on', 'that', 'issu', 'i', 'am', 'also', 'practic', 'buddhist', 'philosophi', 'though', 'i', 'know', 'when', 'to', 'stop', 'and', 'allow', 'them', 'to', 'find', 'their', 'own', 'path', 'peopl', 'can', 'believ', 'what', 'they', 'want', 'some', 'arentbut', 'then', 'again', 'there', 'is', 'no', 'such', 'thing', 'as', 'dark', 'assum', 'that', 'the', 'einstein', 'theori', 'is', 'true', 'it', 'defin', 'as', 'a', 'lack', 'of', 'light', 'now', 'when', 'that', 'appli', 'to', 'selfishselfless', 'it', 'mayb', 'onli', 'on', 'aa', 'harsh', 'as', 'eggsi', 'seem', 'he', 'right', 'you', 'cant', 'offer', 'emot', 'to', 'peopl', 'who', 'dont', 'appreci', 'them', 'ye', 'i', 'know', 'the', 'feel', 'of', 'pour', 'yourself', 'out', 'and', 'get', 'taken', 'for', 'grant', 'there', 'a', 'lesson', 'oni', 'wish', 'i', 'wa', 'a', 'littl', 'bit', 'taller', 'i', 'wish', 'i', 'wa', 'a', 'baller', 'i', 'wish', 'i', 'had', 'a', 'girl', 'who', 'look', 'good', 'i', 'would', 'call', 'her', 'lol', 'on', 'the', 'real', 'doe', 'i', 'wish', 'i', 'had', 'more', 'of', 'a', 'drive', 'to', 'pursu', 'my', 'dream', 'than', 'human', 'alway', 'had', 'thi', 'thought', 'about', 'selfish', 'in', 'a', 'sens', 'our', 'innat', 'desir', 'to', 'make', 'peopl', 'happi', 'make', 'us', 'happi', 'doe', 'that', 'make', 'us', 'selfish', 'or', 'selfless', 'is', 'selfish', 'measur', 'upon', 'a', 'spectrum', 'and', 'i', 'dont', 'know', 'how', 'i', 'feel', 'about', 'thiscan', 'we', 'all', 'just', 'get', 'along', 'i', 'also', 'have', 'myself', 'a', 'copi', 'ive', 'gotten', 'to', 'the', 'first', 'page', 'and', 'put', 'it', 'down', 'though', 'loldo', 'you', 'draw', 'scrap', 'book', 'watercolor', 'paint', 'have', 'tattoo', 'like', 'dinosuar', 'eat', 'turtl', 'make', 'guitar', 'play', 'piano', 'have', 'pet', 'favorit', 'danim', 'list', 'some', 'ish', 'yall', 'like', 'to', 'do', 'thi', 'forum', 'is', 'just', 'too', 'neither', 'have', 'an', 'interview', 'today', 'doe', 'should', 'be', 'bangin', 'write', 'enjoy', 'the', 'morn', 'weather', 'with', 'coffe', 'when', 'night', 'hit', 'though', 'im', 'up', 'all', 'night', 'for', 'good', 'fun', 'and', 'meet', 'folk', 'iwel', 'i', 'had', 'a', 'lot', 'of', 'time', 'to', 'figur', 'that', 'out', 'when', 'i', 'wa', 'younger', 'ive', 'alway', 'been', 'interest', 'in', 'environment', 'scienc', 'and', 'do', 'thing', 'that', 'would', 'posit', 'affect', 'my', 'commun', 'but', 'that', 'my', 'fallsubmit', 'good', 'surveyi', 'think', 'you', 'should', 'tell', 'him', 'how', 'you', 'cant', 'alway', 'have', 'a', 'heart', 'to', 'heart', 'moment', 'with', 'him', 'and', 'that', 'you', 'guy', 'should', 'focu', 'on', 'yourselv', 'for', 'now', 'and', 'just', 'hangout', 'when', 'we', 'enfj', 'fall', 'in', 'love', 'it', 'liketh', 'asshol', 'when', 'shit', 'hit', 'the', 'fan', 'major', 'in', 'environment', 'scienc', 'im', 'interest', 'in', 'the', 'connect', 'between', 'the', 'natur', 'world', 'and', 'humanssuch', 'a', 'vaug', 'question', 'but', 'ye', 'most', 'of', 'the', 'time', 'i', 'dont', 'give', 'up', 'on', 'thing', 'im', 'interest', 'in', 'person', 'i', 'hate', 'losingbecaus', 'the', 'descript', 'of', 'an', 'enfj', 'fit', 'me', 'more', 'as', 'far', 'as', 'i', 'know']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stopwords = [\"http*\"]\n",
    "# print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in X_data:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        tempData.append(ps.stem(word))\n",
    "        tempData = [word for word in tempData if not word in stopwords] # 불용어 제거\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "print(stemData[1])\n",
    "\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]\n",
    "del [[mbti]]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.15769379 0.         0.         ... 0.         0.         0.        ]\n [0.22235513 0.0403856  0.         ... 0.         0.         0.        ]\n [0.14209875 0.         0.         ... 0.         0.         0.        ]\n ...\n [0.18852251 0.         0.         ... 0.         0.         0.        ]\n [0.19113407 0.         0.         ... 0.         0.         0.        ]\n [0.13241067 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#vectorization (tfidf)\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "print(tfidf.transform(stemData).toarray())\n",
    "X = tfidf.transform(stemData)\n",
    "y = np.array(y_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'ENFJ': 1832,\n",
       "         'ENFP': 1832,\n",
       "         'ENTJ': 1832,\n",
       "         'ENTP': 1832,\n",
       "         'ESFJ': 1832,\n",
       "         'ESFP': 1832,\n",
       "         'ESTJ': 1832,\n",
       "         'ESTP': 1832,\n",
       "         'INFJ': 1832,\n",
       "         'INFP': 1832,\n",
       "         'INTJ': 1832,\n",
       "         'INTP': 1832,\n",
       "         'ISFJ': 1832,\n",
       "         'ISFP': 1832,\n",
       "         'ISTJ': 1832,\n",
       "         'ISTP': 1832})"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "'''\n",
    "X_data = mbti['posts']\n",
    "X = X_data.values.reshape(-1, 1)\n",
    "\n",
    "y_data = mbti['type']\n",
    "y = y_data.values.reshape(-1, 1)'''\n",
    "\n",
    "Counter(y_data)\n",
    "X_resampled, y_resampled = SMOTE(random_state=0).fit_sample(X, y)\n",
    "#format(Counter(y_resampled))\n",
    "X_resampled #ndarray\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "source": [
    "# XGBoost_Oversampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building Start\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "print(\"Model building Start\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size= 0.3, random_state=1234)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building End\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "print(\"Model building End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['XGBoost_oversample.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'XGBoost_SMOTEoversample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation Start\n0.9388219240391176\n[0.96068376 0.93895871 0.9751773  0.90893471 0.99613153 0.99626168\n 0.99480069 0.98345588 0.8313253  0.81782178 0.89421158 0.85480944\n 0.97913043 0.93526405 0.9754717  0.9556314 ]\n[0.99118166 0.93392857 1.         0.93628319 1.         1.\n 1.         1.         0.76243094 0.72202797 0.85823755 0.8365897\n 1.         0.99637024 1.         0.9929078 ]\n[0.97569444 0.93643688 0.98743268 0.92240628 0.99806202 0.99812734\n 0.99739357 0.99165894 0.79538905 0.76694522 0.87585533 0.84560144\n 0.98945518 0.96485062 0.98758357 0.97391304]\nEvaluation END\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"Evaluation Start\")\n",
    "# labels과 guesses\n",
    "labels = preds\n",
    "guesses = y_test\n",
    "\n",
    "print(accuracy_score(labels, guesses))\n",
    "print(recall_score(labels, guesses, average=None))\n",
    "print(precision_score(labels, guesses, average=None))\n",
    "print(f1_score(labels, guesses, average=None))\n",
    "\n",
    "print(\"Evaluation END\")\n",
    "# https://eunsukimme.github.io/ml/2019/10/21/Accuracy-Recall-Precision-F1-score/"
   ]
  }
 ]
}
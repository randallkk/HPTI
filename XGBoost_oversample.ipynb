{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MBTI별 포스팅 모으기 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('.\\data\\\\training\\\\mbti.csv')"
   ]
  },
  {
   "source": [
    "# OverSampling and Undersampling (데이터 500개 정도로 맞추기)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'ENFJ': 39,\n",
       "         'ENFP': 39,\n",
       "         'ENTJ': 39,\n",
       "         'ENTP': 39,\n",
       "         'ESFJ': 39,\n",
       "         'ESFP': 39,\n",
       "         'ESTJ': 39,\n",
       "         'ESTP': 39,\n",
       "         'INFJ': 39,\n",
       "         'INFP': 39,\n",
       "         'INTJ': 39,\n",
       "         'INTP': 39,\n",
       "         'ISFJ': 39,\n",
       "         'ISFP': 39,\n",
       "         'ISTJ': 39,\n",
       "         'ISTP': 39})"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "X_data = mbti['posts']\n",
    "X = X_data.values.reshape(-1, 1)\n",
    "\n",
    "y_data = mbti['type']\n",
    "y = y_data.values.reshape(-1, 1)\n",
    "\n",
    "Counter(y_data)\n",
    "X_resampled, y_resampled = RandomUnderSampler(random_state=0).fit_sample(X, y)\n",
    "#format(Counter(y_resampled))\n",
    "X_resampled #ndarray\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "source": [
    "# tokenize and stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\nnormalize 완료\n"
     ]
    }
   ],
   "source": [
    "X_data = X_resampled.ravel()\n",
    "y_data = y_resampled\n",
    "\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data: #for sentence in X_data.ravel()\n",
    "    rm_urls = re.sub(r'http[s]?://\\S+', '', sentence)\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '', rm_urls.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "X_data = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gc\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n",
      "['sorri', 'i', 'know', 'thi', 'post', 'is', 'a', 'few', 'year', 'old', 'but', 'i', 'wa', 'so', 'happi', 'to', 'read', 'it', 'becaus', 'it', 'make', 'so', 'much', 'sens', 'to', 'me', 'especi', 'now', 'that', 'i', 'realiz', 'im', 'truli', 'an', 'enfj', 'and', 'not', 'an', 'esfj', 'i', 'wa', 'rais', 'byentp', 'for', 'sure', 'winkhi', 'i', 'liter', 'just', 'saw', 'thi', 'today', 'havent', 'been', 'on', 'perc', 'in', 'a', 'few', 'month', 'i', 'might', 'retir', 'i', 'just', 'dont', 'think', 'thi', 'forum', 'is', 'healti', 'for', 'me', 'unsurethi', 'wa', 'hilari', 'hahaha', 'blue', 'you', 'are', 'just', 'too', 'awesom', 'for', 'word', 'super', 'hugsi', 'wouldnt', 'say', 'bitch', 'but', 'you', 'do', 'seem', 'a', 'littl', 'selfcent', 'mayb', 'you', 'need', 'a', 'wake', 'up', 'call', 'no', 'offens', 'or', 'anythingmast', 'wolf', 'isnt', 'jealou', 'of', 'all', 'my', 'admir', 'becaus', 'we', 'are', 'just', 'friend', 'in', 'fact', 'he', 'like', 'a', 'brother', 'to', 'me', 'ewwwi', 'wasnt', 'i', 'think', 'it', 'becaus', 'i', 'didnt', 'have', 'a', 'clear', 'idea', 'what', 'i', 'wa', 'suppos', 'to', 'do', 'at', 'first', 'xd', 'but', 'i', 'caught', 'on', 'im', 'a', 'quick', 'learner', 'so', 'the', 'next', 'vers', 'i', 'want', 'to', 'sing', 'the', 'main', 'part', 'and', 'thenit', 'frustrat', 'me', 'when', 'introvert', 'expect', 'me', 'to', 'read', 'their', 'mind', 'or', 'someth', 'boredi', 'love', 'make', 'music', 'with', 'you', 'i', 'think', 'it', 'just', 'hit', 'me', 'right', 'now', 'how', 'fun', 'that', 'wa', 'thank', 'i', 'love', 'you', 'just', 'as', 'friend', 'of', 'cours', 'winkdear', 'entj', 'type', 'what', 'is', 'your', 'problem', 'whi', 'do', 'you', 'look', 'at', 'me', 'like', 'that', 'and', 'cring', 'anytim', 'you', 'are', 'forc', 'to', 'compliment', 'me', 'on', 'someth', 'what', 'did', 'i', 'ever', 'do', 'to', 'deserv', 'be', 'treat', 'like', 'thi', 'whi', 'is', 'it', 'so', 'amaz', 'to', 'me', 'to', 'have', 'someon', 'stick', 'up', 'for', 'me', 'or', 'do', 'someth', 'for', 'me', 'like', 'i', 'get', 'to', 'a', 'stuck', 'point', 'where', 'i', 'dont', 'think', 'i', 'can', 'take', 'care', 'of', 'someth', 'by', 'myself', 'and', 'then', 'someon', 'comesholla', 'back', 'your', 'quit', 'gorgeou', 'yourself', 'winkanyon', 'up', 'for', 'chat', 'tonight', 'today', 'is', 'my', 'friday', 'of', 'sort', 'happi', 'mibbit', 'tc', 'let', 'me', 'knowdawww', 'couldnt', 'be', 'the', 'love', 'car', 'seat', 'in', 'the', 'background', 'could', 'it', 'laughinghello', 'again', 'fellow', 'esfj', 'had', 'a', 'pretti', 'crappi', 'day', 'at', 'work', 'today', 'anyth', 'and', 'everyth', 'that', 'could', 'go', 'wrong', 'did', 'luckili', 'i', 'wa', 'abl', 'to', 'let', 'a', 'lot', 'of', 'it', 'go', 'but', 'man', 'day', 'like', 'thi', 'just', 'suck', 'the', 'life', 'out', 'of', 'me', 'ican', 'someon', 'explain', 'to', 'me', 'whi', 'esfj', 'is', 'an', 'eleph', 'becaus', 'they', 'have', 'good', 'memori', 'or', 'someth', 'confused', 'said', 'it', 'befor', 'and', 'ill', 'say', 'it', 'again', 'thank', 'infp', 'for', 'put', 'up', 'with', 'my', 'presenc', 'and', 'my', 'post', 'hug', 'infpsim', 'bring', 'thi', 'thread', 'aliv', 'again', 'thi', 'should', 'be', 'sticki', 'ugh', 'i', 'feel', 'like', 'such', 'a', 'bitch', 'today', 'i', 'dont', 'know', 'what', 'my', 'problem', 'is', 'there', 'are', 'certain', 'thread', 'that', 'i', 'realli', 'enjoy', 'except', 'amho', 'but', 'i', 'think', 'she', 'creat', 'the', 'problem', 'here', 'by', 'have', 'her', 'boyfriend', 'move', 'in', 'that', 'creat', 'an', 'awkward', 'situat', 'to', 'begin', 'with', 'doesnt', 'sound', 'like', 'thi', 'esfj', 'is', 'in', 'a', 'veri', 'healthi', 'state', 'right', 'now', 'ifwow', 'meep', 'good', 'go', 'i', 'think', 'it', 'all', 'the', 'hundr', 'of', 'hug', 'you', 'doll', 'out', 'on', 'a', 'daili', 'basi', 'tongu', 'keep', 'up', 'the', 'good', 'work', 'have', 'you', 'ask', 'yourself', 'whi', 'you', 'are', 'feel', 'noth', 'especi', 'if', 'thi', 'person', 'wa', 'one', 'of', 'the', 'most', 'import', 'peopl', 'to', 'you', 'mayb', 'you', 'are', 'just', 'in', 'the', 'denial', 'stage', 'of', 'grief', 'stage', 'of', 'griefthi', 'is', 'great', 'advic', 'thank', 'you', 'for', 'post', 'thi', 'you', 'sound', 'like', 'youv', 'learn', 'a', 'lot', 'in', 'life', 'and', 'have', 'put', 'practic', 'and', 'moral', 'applic', 'behind', 'it', 'you', 'show', 'matur', 'and', 'growth', 'in', 'thi', 'post', 'and', 'ii', 'hear', 'you', 'and', 'feel', 'you', 'for', 'me', 'it', 'not', 'like', 'i', 'act', 'like', 'a', 'differ', 'person', 'but', 'im', 'so', 'good', 'at', 'read', 'peopl', 'and', 'im', 'so', 'interest', 'in', 'other', 'perspect', 'that', 'i', 'tri', 'to', 'be', 'more', 'like', 'them', 'in', 'aa', 'few', 'base', 'on', 'my', 'opinion', 'entj', 'bear', 'entp', 'cross', 'between', 'a', 'monkey', 'and', 'a', 'fox', 'laugh', 'ill', 'have', 'to', 'think', 'about', 'thi', 'and', 'come', 'back', 'for', 'more', 'happyher', 'a', 'good', 'resourc', 'too', 'stop', 'walk', 'on', 'eggshel', 'take', 'your', 'life', 'back', 'when', 'someon', 'you', 'care', 'about', 'ha', 'borderlin', 'person', 'disord', 'paul', 'mason', 'ms', 'randi', 'kreger', 'i', 'have', 'an', 'ex', 'who', 'hasgener', 'question', 'for', 'anyal', 'enfj', 'read', 'thi', 'are', 'all', 'enfj', 'veri', 'posit', 'peopl', 'most', 'that', 'i', 'have', 'met', 'are', 'veri', 'posit', 'glass', 'half', 'fulltyp', 'also', 'what', 'enneagram', 'are', 'youi', 'have', 'decid', 'that', 'i', 'need', 'to', 'hang', 'out', 'in', 'the', 'enfj', 'forum', 'more', 'often', 'i', 'relat', 'so', 'much', 'to', 'enfj', 'and', 'you', 'guy', 'are', 'all', 'just', 'so', 'awesom', 'happyim', 'so', 'sorri', 'you', 'are', 'go', 'through', 'thi', 'ive', 'been', 'there', 'hug', 'listen', 'to', 'music', 'and', 'go', 'on', 'walk', 'help', 'also', 'surround', 'yourself', 'with', 'peopl', 'who', 'care', 'about', 'you', 'and', 'keep', 'busi', 'help', 'tooim', 'total', 'like', 'that', 'too', 'although', 'everi', 'now', 'then', 'i', 'enjoy', 'some', 'downtim', 'winkoh', 'i', 'rememb', 'you', 'now', 'your', 'the', 'guy', 'who', 'told', 'me', 'about', 'your', 'fetish', 'for', 'wear', 'women', 'underwearmayb', 'the', 'better', 'question', 'is', 'who', 'are', 'you', 'drysimilar', 'to', 'me', 'im', 'w', 'w', 'w', 'i', 'like', 'to', 'base', 'person', 'on', 'cognit', 'function', 'which', 'cognit', 'function', 'you', 'use', 'most', 'have', 'more', 'to', 'do', 'with', 'person', 'than', 'just', 'ei', 'prefer', 'imho', 'dang', 'i', 'alway', 'miss', 'all', 'the', 'fun', 'stuff', 'sadmi', 'infj', 'best', 'friend', 'introduc', 'me', 'and', 'i', 'wa', 'hook', 'i', 'love', 'learn', 'about', 'peopl', 'and', 'tri', 'to', 'figur', 'them', 'out', 'so', 'learn', 'more', 'about', 'person', 'psycholog', 'and', 'self', 'awar', 'realli', 'stimul', 'mei', 'can', 'kinda', 'relat', 'im', 'a', 'w', 'love', 'the', 'enneagram', 'btw', 'wink', 'even', 'as', 'a', 'two', 'i', 'have', 'a', 'realli', 'strong', 'sens', 'of', 'need', 'to', 'feel', 'accomplish', 'and', 'i', 'put', 'a', 'lot', 'on', 'how', 'other', 'see', 'me', 'i', 'relat', 'a', 'lot', 'toi', 'hate', 'my', 'ex', 'he', 'just', 'accident', 'call', 'me', 'and', 'left', 'a', 'minut', 'messag', 'on', 'my', 'vm', 'of', 'him', 'and', 'thi', 'girl', 'he', 'with', 'right', 'now', 'even', 'though', 'i', 'dont', 'want', 'to', 'be', 'with', 'him', 'and', 'get', 'away', 'from', 'him', 'waswond', 'if', 'ani', 'esfj', 'were', 'like', 'me', 'as', 'a', 'child', 'i', 'rememb', 'be', 'so', 'dreami', 'a', 'lot', 'more', 'freespirit', 'than', 'i', 'am', 'now', 'and', 'i', 'wa', 'veri', 'veri', 'sensit', 'i', 'cri', 'about', 'everyth', 'hmmm', 'in', 'a', 'lot', 'ofbecaus', 'esfj', 'are', 'selfless', 'and', 'give', 'and', 'looooov', 'other', 'peopl', 'plu', 'most', 'esfj', 'are', 'good', 'look', 'winki', 'vote', 'esfp', 'lot', 'of', 'posit', 'energi', 'fun', 'love', 'attent', 'etc', 'etcsorri', 'im', 'late', 'to', 'post', 'about', 'thi', 'but', 'i', 'think', 'you', 'knew', 'in', 'your', 'heart', 'of', 'heart', 'that', 'thi', 'guy', 'wa', 'bad', 'news', 'my', 'gut', 'is', 'alway', 'right', 'no', 'matter', 'how', 'much', 'i', 'tri', 'to', 'reason', 'my', 'way', 'into', 'somethingahem', 'i', 'can', 'go', 'toe', 'to', 'toe', 'with', 'an', 'entp', 'ani', 'day', 'tri', 'me', 'drytonight', 'is', 'one', 'of', 'those', 'night', 'where', 'i', 'think', 'it', 'would', 'feel', 'good', 'to', 'just', 'cri', 'and', 'cri', 'pmsisawesomenotreallyanyon', 'chat', 'tonightim', 'so', 'obsess', 'with', 'the', 'song', 'human', 'natur', 'right', 'now', 'includ', 'all', 'the', 'cover', 'and', 'variat', 'on', 'it', 'especi', 'the', 'cover', 'by', 'cello', 'these', 'guy', 'are', 'amaz', 'i', 'wa', 'out', 'to', 'dinner', 'with', 'a', 'group', 'of', 'peopl', 'tonight', 'and', 'across', 'the', 'way', 'i', 'see', 'a', 'guy', 'and', 'girl', 'on', 'a', 'date', 'you', 'can', 'tell', 'they', 'are', 'on', 'a', 'date', 'you', 'can', 'see', 'the', 'tension', 'yet', 'they', 'appear', 'to', 'be', 'have', 'a', 'greaty', 'and', 'those', 'of', 'you', 'who', 'were', 'on', 'tc', 'that', 'night', 'have', 'seen', 'the', 'proof', 'when', 'i', 'had', 'horribl', 'hiccup', 'and', 'it', 'work', 'it', 'realli', 'the', 'onli', 'thing', 'that', 'ha', 'work', 'for', 'me', 'crazyomg', 'i', 'just', 'read', 'thi', 'hahahaha', 'ok', 'so', 'here', 'what', 'happen', 'i', 'have', 'been', 'have', 'a', 'hard', 'time', 'get', 'to', 'sleep', 'late', 'so', 'last', 'night', 'i', 'took', 'one', 'of', 'my', 'ambien', 'prescript', 'it', 'been', 'a', 'long', 'time']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4674"
      ]
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stopwords = [\"http*\"]\n",
    "# print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in X_data:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        tempData.append(ps.stem(word))\n",
    "        tempData = [word for word in tempData if not word in stopwords] # 불용어 제거\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "print(stemData[1])\n",
    "\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]\n",
    "del [[mbti]]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.11362201 0.         0.         ... 0.         0.         0.        ]\n [0.23669652 0.03730275 0.         ... 0.         0.         0.        ]\n [0.10176628 0.         0.         ... 0.         0.         0.        ]\n ...\n [0.21520038 0.03391501 0.         ... 0.         0.         0.        ]\n [0.17876285 0.         0.         ... 0.         0.         0.        ]\n [0.23887051 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#vectorization (tfidf)\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "print(tfidf.transform(stemData).toarray())\n",
    "X = tfidf.transform(stemData).toarray()\n",
    "y = np.array(y_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# XGBoost_Oversampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building Start\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "print(\"Model building Start\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state=1234)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building End\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "print(\"Model building End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['XGBoost_oversample.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'XGBoost_oversample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation Start\n0.4946808510638298\n[0.75       0.45454545 0.54545455 0.16       1.         0.46153846\n 0.61538462 0.71428571 0.4        0.72727273 0.375      0.375\n 0.47368421 0.5        0.5        0.5       ]\n[0.75       0.38461538 0.75       0.4        0.4375     0.54545455\n 0.53333333 0.38461538 0.25       0.61538462 0.33333333 0.3\n 0.6        0.5        0.55555556 0.54545455]\n[0.75       0.41666667 0.63157895 0.22857143 0.60869565 0.5\n 0.57142857 0.5        0.30769231 0.66666667 0.35294118 0.33333333\n 0.52941176 0.5        0.52631579 0.52173913]\nEvaluation END\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"Evaluation Start\")\n",
    "# labels과 guesses\n",
    "labels = preds\n",
    "guesses = y_test\n",
    "\n",
    "print(accuracy_score(labels, guesses))\n",
    "print(recall_score(labels, guesses, average=None))\n",
    "print(precision_score(labels, guesses, average=None))\n",
    "print(f1_score(labels, guesses, average=None))\n",
    "\n",
    "print(\"Evaluation END\")\n",
    "# https://eunsukimme.github.io/ml/2019/10/21/Accuracy-Recall-Precision-F1-score/"
   ]
  }
 ]
}
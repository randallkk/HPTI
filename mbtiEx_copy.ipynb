{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request"
   ]
  },
  {
   "source": [
    "# MBTI 별 posting 모으기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      posts\n",
       "type       \n",
       "ENFJ    190\n",
       "ENFP    675\n",
       "ENTJ    231\n",
       "ENTP    685\n",
       "ESFJ     42\n",
       "ESFP     48\n",
       "ESTJ     39\n",
       "ESTP     89\n",
       "INFJ   1470\n",
       "INFP   1832\n",
       "INTJ   1091\n",
       "INTP   1304\n",
       "ISFJ    166\n",
       "ISFP    271\n",
       "ISTJ    205\n",
       "ISTP    337"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posts</th>\n    </tr>\n    <tr>\n      <th>type</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ENFJ</th>\n      <td>190</td>\n    </tr>\n    <tr>\n      <th>ENFP</th>\n      <td>675</td>\n    </tr>\n    <tr>\n      <th>ENTJ</th>\n      <td>231</td>\n    </tr>\n    <tr>\n      <th>ENTP</th>\n      <td>685</td>\n    </tr>\n    <tr>\n      <th>ESFJ</th>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>ESFP</th>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>ESTJ</th>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>ESTP</th>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>INFJ</th>\n      <td>1470</td>\n    </tr>\n    <tr>\n      <th>INFP</th>\n      <td>1832</td>\n    </tr>\n    <tr>\n      <th>INTJ</th>\n      <td>1091</td>\n    </tr>\n    <tr>\n      <th>INTP</th>\n      <td>1304</td>\n    </tr>\n    <tr>\n      <th>ISFJ</th>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>ISFP</th>\n      <td>271</td>\n    </tr>\n    <tr>\n      <th>ISTJ</th>\n      <td>205</td>\n    </tr>\n    <tr>\n      <th>ISTP</th>\n      <td>337</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "mbti=pd.read_csv('./data/mbti_1.csv')\n",
    "\n",
    "types = mbti.groupby('type').count()\n",
    "\n",
    "output_file = '.\\data\\\\training\\\\mbti.csv' # raw string이 아니라 '\\'를 쓰려면 \\\\라고 해야 함\n",
    "allData = []\n",
    "\n",
    "for type in types.index:\n",
    "    condition = mbti['type'] == type # condition: mbti['type']가 topfive의 원소인 type 같을 bool 조건\n",
    "    ownsentence = mbti[condition]  # ownsentence: condition에 맞는 row만 filtering한 dataframe\n",
    "    allData.append(ownsentence)\n",
    "dataCombine = pd.concat(allData, axis=0, ignore_index=True)\n",
    "dataCombine.to_csv(output_file, index=False)\n",
    "\n",
    "types\n",
    "\n",
    "    #input_file = r'.\\data\\training'\n",
    "    #allFile_list = glob.glob(os.path.join(input_file, 'mbti_*'))\n",
    "    #for file in allFile_list:\n",
    "    #    csv = pd.read_csv(file,sep=';', encoding='iso-8859-1') # for구문으로 csv파일들을 읽어 들인다\n",
    "    #    cleanMbti = csv['posts'].str.replace('[^A-Za-z\\s]+', '')"
   ]
  },
  {
   "source": [
    "# Resampling (Undersampling and Oversampling)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Tokenize and Stemming\n",
    "morph analysis 기법."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\n",
      "normalize 완료\n"
     ]
    }
   ],
   "source": [
    "mbti= dataCombine\n",
    "X_data = mbti['posts']\n",
    "y_data = mbti['type']\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data.tolist():\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '',sentence.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "mbti.posts = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 19.1 GiB for an array with shape (8675, 295647) and data type float64",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-35f67e9dcdf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m '''tf = TfidfVectorizer().fit(stemData)\n\u001b[0;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 19.1 GiB for an array with shape (8675, 295647) and data type float64"
     ]
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stopwords = ['http*']\n",
    "#print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in mbti['posts']:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        tempData.append(ps.stem(word))\n",
    "        #tempData = [word for word in tempData if not word in stopwords] # 불용어 제거\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "#print(stemData[0])\n",
    "\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]\n",
    "\n",
    "#vectorization (tfidf)\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "print(tfidf.transform(stemData).toarray())\n",
    "'''tf = TfidfVectorizer().fit(stemData)\n",
    "print(tf.transform(stemData).toarray())\n",
    "print(tf.vocabulary_)\n",
    "#print(vector.vocabulary_)'''\n"
   ]
  },
  {
   "source": [
    "# Model building"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"X_data = np.array(X_token).reshape(-1,1)\\ny_data = np.array(mbti['type']).reshape(-1,1)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size= 0.3, random_state=1234)\\nmodel = XGBClassifier()\\nmodel.fit(X_train, y_train)\\npreds = classifier.predict(X_test)\\n\\nplot_importance(model)\\npyplot.show()\""
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size= 0.3, random_state=1234)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "preds = classifier.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
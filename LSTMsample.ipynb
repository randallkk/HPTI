{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('.\\data\\\\training\\\\mbti.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP', 'ESTJ', 'ESTP',\n",
       "       'INFJ', 'INFP', 'INTJ', 'INTP', 'ISFJ', 'ISFP', 'ISTJ', 'ISTP'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "mbti['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(df):\n",
    "    \n",
    "    texts = df['posts'].copy()\n",
    "    labels = df['type'].copy()\n",
    "    \n",
    "    # Process text data\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    texts = [text.lower() for text in texts]\n",
    "    texts = [text.split() for text in texts]\n",
    "    texts = [[word.strip() for word in text] for text in texts]\n",
    "    texts = [[word for word in text if word not in stop_words] for text in texts]\n",
    "    \n",
    "    vocab_length = 10000\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=vocab_length)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    texts = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    max_seq_length = np.max([len(text) for text in texts])\n",
    "    \n",
    "    texts = pad_sequences(texts, maxlen=max_seq_length, padding='post')\n",
    "    \n",
    "    # Process label data\n",
    "    label_values = [\n",
    "        'INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ'\n",
    "    ]\n",
    "    \n",
    "    label_mapping = {label: np.int(label[0] == 'E') for label in label_values}\n",
    "    \n",
    "    labels = labels.replace(label_mapping)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return texts, labels, max_seq_length, vocab_length, label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels, max_seq_length, vocab_length, label_mapping = preprocess_inputs(mbti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text sequences:\n (8675, 860)\n\nLabels:\n (8675,)\n\nMax sequence length:\n 860\n\nVocab length:\n 10000\n\nLabel mapping:\n {'INFJ': 0, 'ENTP': 1, 'INTP': 0, 'INTJ': 0, 'ENTJ': 1, 'ENFJ': 1, 'INFP': 0, 'ENFP': 1, 'ISFP': 0, 'ISTP': 0, 'ISFJ': 0, 'ISTJ': 0, 'ESTP': 1, 'ESFP': 1, 'ESTJ': 1, 'ESFJ': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Text sequences:\\n\", texts.shape)\n",
    "print(\"\\nLabels:\\n\", labels.shape)\n",
    "print(\"\\nMax sequence length:\\n\", max_seq_length)\n",
    "print(\"\\nVocab length:\\n\", vocab_length)\n",
    "print(\"\\nLabel mapping:\\n\", label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-25-23173eb62967>, line 9)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-25-23173eb62967>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    inputs))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(max_seq_length,))\n",
    "embedding_dim = 512\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=vocab_length,\n",
    "    output_dim=embedding_dim,\n",
    "    input_length=max_seq_length, \n",
    "    inputs))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(16, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(texts_train, labels_train, epochs=5, batch_size=32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(texts_test, labels_test)"
   ]
  }
 ]
}
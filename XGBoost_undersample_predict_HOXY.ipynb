{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# (MBTI 별 Data 모으기: 생략) MBTI file 불러오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('.\\data\\\\training\\\\mbti.csv')"
   ]
  },
  {
   "source": [
    "# UnderSampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0       ENFJ\n1       ENFJ\n2       ENFJ\n3       ENFJ\n4       ENFJ\n        ... \n8670    ISTP\n8671    ISTP\n8672    ISTP\n8673    ISTP\n8674    ISTP\nName: type, Length: 8675, dtype: object\n[['ENFJ']\n ['ENFJ']\n ['ENFJ']\n ...\n ['ISTP']\n ['ISTP']\n ['ISTP']]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'ENFJ': 39,\n",
       "         'ENFP': 39,\n",
       "         'ENTJ': 39,\n",
       "         'ENTP': 39,\n",
       "         'ESFJ': 39,\n",
       "         'ESFP': 39,\n",
       "         'ESTJ': 39,\n",
       "         'ESTP': 39,\n",
       "         'INFJ': 39,\n",
       "         'INFP': 39,\n",
       "         'INTJ': 39,\n",
       "         'INTP': 39,\n",
       "         'ISFJ': 39,\n",
       "         'ISFP': 39,\n",
       "         'ISTJ': 39,\n",
       "         'ISTP': 39})"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X_data = mbti['posts']\n",
    "X = X_data.values.reshape(-1, 1)\n",
    "\n",
    "y_data = mbti['type']\n",
    "print(y_data)\n",
    "y = y_data.values.reshape(-1, 1)\n",
    "print(y)\n",
    "\n",
    "X_resampled, y_resampled = RandomUnderSampler(random_state=0).fit_sample(X, y)\n",
    "#format(Counter(y_resampled))\n",
    "X_resampled #ndarray\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "source": [
    "# Type value "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ENFJ']\n[[1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "b_Types = {'I':0, 'E':1, 'N':0, \"S\":1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "b_Types_list =[{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    return[b_Types[i] for i in personality]\n",
    "\n",
    "def translate_back(personality):\n",
    "    s = \"\"\n",
    "    for i, j in enumerate(personality):\n",
    "        s += b_Types_list[i][j]\n",
    "    return s\n",
    "\n",
    "d = y[0]\n",
    "print(d)\n",
    "list_personality_b = np.array([translate_personality(p) for p in d])\n",
    "print(list_personality_b)"
   ]
  },
  {
   "source": [
    "# tokenize and stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\nnormalize 완료\n"
     ]
    }
   ],
   "source": [
    "X_data = X_resampled.ravel()\n",
    "y_data = y_resampled\n",
    "\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data: #for sentence in X_data.ravel()\n",
    "    rm_urls = re.sub(r'http[s]?://\\S+', '', sentence)\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '', rm_urls.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "X_data = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gc\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n",
      "['want', 'know', 'feel', 'intp', 'actual', 'intp', 'idea', 'intp', 'havent', 'met', 'anyten', 'million', 'hug', 'mani', 'amaz', 'convers', 'held', 'absenc', 'vow', 'never', 'leav', 'realli', 'lot', 'written', 'recent', 'terriblyfacebook', 'way', 'emphas', 'other', 'error', 'cognit', 'becom', 'obviou', 'lead', 'wonder', 'im', 'friend', 'mani', 'peopl', 'rememb', 'peopl', 'moremmmmhm', 'ive', 'wonder', 'late', 'ei', 'think', 'definit', 'much', 'needwant', 'compani', 'around', 'regardless', 'time', 'togeth', 'spenthahah', 'love', 'think', 'that', 'go', 'topic', 'excel', 'convers', 'thank', 'particip', 'indulg', 'interest', 'enlighteningsomeon', 'creat', 'chitchat', 'thread', 'enfj', 'forum', 'enfj', 'vigilantli', 'await', 'type', 'wander', 'start', 'conversationswa', 'enfp', 'ive', 'caught', 'mani', 'text', 'look', 'around', 'shini', 'object', 'give', 'midsyl', 'mm', 'hmm', 'alway', 'mani', 'andi', 'tire', 'extravert', 'energysap', 'sentiment', 'arent', 'batshit', 'loud', 'outgo', 'even', 'dryon', 'topic', 'guy', 'find', 'get', 'better', 'extrovert', 'onlinealso', 'iamken', 'one', 'yall', 'sure', 'type', 'lot', 'think', 'introvert', 'thing', 'rather', 'fi', 'thing', 'though', 'intp', 'fe', 'way', 'intp', 'infp', 'actual', 'haveb', 'contrari', 'think', 'initi', 'interact', 'begin', 'selffocu', 'realli', 'depend', 'im', 'feel', 'though', 'im', 'insecur', 'kind', 'day', 'someon', 'isth', 'hand', 'yesi', 'attribut', 'credibl', 'advic', 'follow', 'experi', 'date', 'hous', 'keep', 'intp', 'fed', 'littl', 'longer', 'two', 'year', 'find', 'intp', 'rareyour', 'abil', 'candid', 'refresh', 'thing', 'unhealthi', 'usi', 'clarifi', 'rambl', 'lie', 'mean', 'less', 'valid', 'next', 'less', 'reliabl', 'misconstru', 'intp', 'differ', 'other', 'mean', 'vastli', 'aeffi', 'iamtp', 'interest', 'thought', 'intp', 'behav', 'like', 'infp', 'think', 'ive', 'seen', 'happen', 'intp', 'current', 'think', 'intp', 'differ', 'fromth', 'cognit', 'function', 'test', 'differ', 'mbti', 'magicwhoa', 'much', 'happen', 'day', 'want', 'take', 'opportun', 'make', 'observ', 'page', 'sinc', 'feel', 'like', 'miss', 'thismight', 'enneagram', 'type', 'thing', 'relat', 'im', 'enfj', 'think', 'ask', 'make', 'enfj', 'happi', 'peopl', 'make', 'us', 'happyhistoir', 'soo', 'unfold', 'make', 'move', 'yet', 'peopl', 'ha', 'guarante', 'like', 'quieter', 'usual', 'also', 'dont', 'confus', 'introvers', 'lack', 'confid', 'samei', 'think', 'odd', 'behavior', 'enfj', 'behavior', 'your', 'describ', 'sound', 'introvert', 'mayb', 'he', 'intimidatedi', 'cant', 'speak', 'everyon', 'els', 'person', 'experi', 'alway', 'think', 'even', 'someth', 'overthink', 'even', 'person', 'interact', 'seem', 'absorb', 'convers', 'butno', 'absolut', 'wouldnt', 'overwhelm', 'think', 'person', 'explain', 'relat', 'would', 'even', 'thing', 'way', 'plan', 'know', 'mean', 'youjust', 'got', 'thing', 'left', 'eye', 'still', 'hurt', 'rawrwel', 'im', 'go', 'honest', 'someon', 'walk', 'like', 'cat', 'fall', 'fit', 'rage', 'burst', 'laugh', 'second', 'later', 'sound', 'aw', 'immatur', 'unstabl', 'possiblyr', 'someon', 'field', 'true', 'constantli', 'odd', 'buy', 'ive', 'ask', 'other', 'forum', 'dont', 'think', 'mani', 'peopl', 'inbait', 'approv', 'would', 'bug', 'answer', 'would', 'cant', 'make', 'stop', 'period', 'that', 'itheheheheh', 'idk', 'id', 'say', 'im', 'pretti', 'organ', 'schedul', 'that', 'natur', 'j', 'said', 'dont', 'think', 'anyth', 'sex', 'drive', 'like', 'lofti', 'said', 'probablyidk', 'well', 'sex', 'food', 'correl', 'person', 'type', 'mayb', 'thatd', 'interest', 'thing', 'researchi', 'also', 'disagre', 'think', 'simpli', 'obviou', 'extravert', 'spastic', 'obnoxi', 'natur', 'extravers', 'mere', 'feel', 'divulg', 'easili', 'process', 'externallyhahaha', 'someon', 'sound', 'excit', 'forgot', 'mention', 'poni', 'also', 'might', 'want', 'chang', 'word', 'bit', 'screw', 'need', 'protect', 'im', 'sorri', 'blanket', 'statement', 'peopl', 'run', 'gamut', 'e', 'black', 'white', 'youll', 'find', 'plenti', 'extravert', 'person', 'prefer', 'spend', 'time', 'home', 'one', 'twohahaha', 'connoisseur', 'reject', 'play', 'dumb', 'alway', 'goto', 'women', 'tend', 'get', 'shove', 'face', 'eventu', 'total', 'possibl', 'id', 'send', 'pictur', 'might', 'awkward', 'poor', 'intp', 'intens', 'stori', 'cant', 'wait', 'see', 'happen', 'hope', 'run', 'keep', 'mind', 'infpenfj', 'interact', 'weird', 'go', 'head', 'mightkittydian', 'true', 'said', 'pretti', 'much', 'demand', 'introduc', 'two', 'year', 'ago', 'nbd', 'wont', 'chang', 'opinion', 'thing', 'realiz', 'howdo', 'tell', 'im', 'intrigu', 'happen', 'intp', 'fire', 'readythi', 'ador', 'perhap', 'also', 'conflict', 'two', 'interact', 'like', 'typic', 'curiou', 'shi', 'approach', 'someon', 'dont', 'expecti', 'cant', 'speak', 'everyon', 'ill', 'speak', 'sign', 'enfj', 'give', 'away', 'attract', 'your', 'attract', 'depend', 'much', 'inform', 'aboutif', 'read', 'link', 'youll', 'see', 'extinguish', 'partner', 'infp', 'actual', 'enfp', 'love', 'japanes', 'literatur', 'someth', 'look', 'mayb', 'share', 'favorit', 'book', 'note', 'one', 'favorit', 'novella', 'banana', 'yoshimotosometim', 'there', 'effect', 'wholesom', 'isnt', 'intrigu', 'challeng', 'enough', 'factor', 'young', 'boy', 'chang', 'friend', 'get', 'older', 'tell', 'absolut', 'toim', 'go', 'say', 'word', 'caution', 'onlin', 'survey', 'least', 'reliabl', 'method', 'take', 'poll', 'two', 'big', 'reason', 'dont', 'know', 'person', 'answer', 'question', 'theythi', 'terrifi', 'thank', 'share', 'she', 'beauti']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in X_data:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        if word not in stop_words:\n",
    "            tempData.append(ps.stem(word))\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "print(stemData[0])\n",
    "stemData = np.array(stemData)\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]\n",
    "del [[mbti]]\n",
    "gc.collect()"
   ]
  },
  {
   "source": [
    "# Vectorization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n [0.05858548 0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n ...\n [0.04542902 0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]]\n  (0, 38616)\t0.031996512659156894\n  (0, 38578)\t0.03286039560761366\n  (0, 38565)\t0.057208490852473474\n  (0, 38556)\t0.06369299789656549\n  (0, 38482)\t0.07370640937171992\n  (0, 38432)\t0.023110327077128505\n  (0, 38411)\t0.056118144533601276\n  (0, 38346)\t0.030911604753450596\n  (0, 38304)\t0.050319331085647706\n  (0, 38051)\t0.03852987969548263\n  (0, 37982)\t0.021758208967633117\n  (0, 37964)\t0.035121536565144516\n  (0, 37804)\t0.03987971435535688\n  (0, 37774)\t0.02453157411311739\n  (0, 37752)\t0.04127966139118793\n  (0, 37742)\t0.028172244380434894\n  (0, 37325)\t0.0661315560087557\n  (0, 37294)\t0.03768883378544042\n  (0, 36966)\t0.01415982878528671\n  (0, 36907)\t0.027594770996755055\n  (0, 36748)\t0.03757714675307268\n  (0, 36558)\t0.03656095168641043\n  (0, 36549)\t0.048543291170637055\n  (0, 36521)\t0.02667071066291597\n  (0, 36498)\t0.025887246687884707\n  :\t:\n  (623, 3006)\t0.02574883097642245\n  (623, 2813)\t0.05243445873884666\n  (623, 2780)\t0.031716440572564016\n  (623, 2737)\t0.06295911307350274\n  (623, 2649)\t0.04215903884038907\n  (623, 2218)\t0.015310517045385837\n  (623, 1815)\t0.028669361692637427\n  (623, 1768)\t0.035137380362721175\n  (623, 1694)\t0.016707396910807235\n  (623, 1678)\t0.019409600985613912\n  (623, 1634)\t0.04431772115029567\n  (623, 1552)\t0.018832523747806203\n  (623, 1521)\t0.06595214553905156\n  (623, 1402)\t0.05243445873884666\n  (623, 1026)\t0.027005911947930575\n  (623, 973)\t0.0628233165247929\n  (623, 962)\t0.036273485311078525\n  (623, 951)\t0.02318278270602633\n  (623, 937)\t0.046252170483722786\n  (623, 873)\t0.0701705878697547\n  (623, 346)\t0.0701705878697547\n  (623, 329)\t0.058356646972364014\n  (623, 235)\t0.03218928947429435\n  (623, 153)\t0.024843568184123425\n  (623, 57)\t0.060637536731017114\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    max_features=1500,\n",
    "    tokenizer=None,\n",
    "    stop_words=None,\n",
    "    max_df=0.7,\n",
    "    min_df=0.1\n",
    ")\n",
    "\n",
    "#X_ct = count_vectorizer.fit_transform(stemData)\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "print(tfidf.transform(stemData).toarray())\n",
    "X = tfidf.transform(stemData)\n",
    "print(X)\n",
    "y = np.array(y_data)"
   ]
  },
  {
   "source": [
    "# XGBoost_Undersampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state=1234)\n"
   ]
  },
  {
   "source": [
    "# Model Building & Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building Start\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "print(\"Model building Start\")\n",
    "#print(max(len(l) for l in X_train))\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train, verbose=1)\n",
    "#model.get_booster().get_score(importance_type=\"gain\") #변수의 중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building End\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "#test_hp = xgb.DMatrix(data=VHX_data, label=hy)\n",
    "preds = model.predict(X_test)\n",
    "print(\"Model building End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['XGBoost_undersampleHOXY.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'XGBoost_undersampleHOXY.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation Start\n",
      "0.4946808510638298\n",
      "[0.71428571 0.4        0.27272727 0.23529412 0.81818182 0.41666667\n",
      " 0.54545455 1.         0.33333333 0.7        0.375      0.16666667\n",
      " 0.5        0.46666667 0.61111111 0.58333333]\n",
      "[0.625      0.30769231 0.375      0.4        0.5625     0.45454545\n",
      " 0.4        0.61538462 0.25       0.53846154 0.33333333 0.2\n",
      " 0.66666667 0.7        0.61111111 0.63636364]\n",
      "[0.66666667 0.34782609 0.31578947 0.2962963  0.66666667 0.43478261\n",
      " 0.46153846 0.76190476 0.28571429 0.60869565 0.35294118 0.18181818\n",
      " 0.57142857 0.56       0.61111111 0.60869565]\n",
      "Evaluation END\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"Evaluation Start\")\n",
    "# labels과 guesses\n",
    "labels = preds\n",
    "guesses = y_test\n",
    "\n",
    "print(accuracy_score(labels, guesses))\n",
    "print(recall_score(labels, guesses, average=None))\n",
    "print(precision_score(labels, guesses, average=None))\n",
    "print(f1_score(labels, guesses, average=None))\n",
    "\n",
    "print(\"Evaluation END\")\n",
    "# https://eunsukimme.github.io/ml/2019/10/21/Accuracy-Recall-Precision-F1-score/"
   ]
  },
  {
   "source": [
    "# Harry Potter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['.\\\\data\\\\\\\\test\\\\HP_DUMBLEDORE.csv', '.\\\\data\\\\\\\\test\\\\HP_HAGRID.csv', '.\\\\data\\\\\\\\test\\\\HP_HARRY.csv', '.\\\\data\\\\\\\\test\\\\HP_HERMIONE.csv', '.\\\\data\\\\\\\\test\\\\HP_RON.csv']\n",
      "stemming 시작\n",
      "stemming 완료\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import glob   \n",
    "\n",
    "'''HP data Normalization and Stemming'''\n",
    "# bag_of_words = {} #{등장인물 : 해당인물의 stemData, ~}\n",
    "h_normalized_text = []\n",
    "h_stemData = []\n",
    "\n",
    "ps=PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "input_file = r'.\\data\\\\test\\\\'\n",
    "\n",
    "allHPfile_list = glob.glob(os.path.join(input_file, 'HP_*'))\n",
    "print(allHPfile_list)\n",
    "\n",
    "harryPotter={}\n",
    "for file in allHPfile_list:\n",
    "    df = pd.read_csv(file, sep=',', encoding='iso-8859-1')\n",
    "    harryPotter[df['Character'][0]] = df['Sentence'].ravel()\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "for element in harryPotter.keys():\n",
    "    for sentence in harryPotter[element]:\n",
    "        h_clean_sentence = re.sub('[^A-Za-z\\s]+', '', sentence.lower())\n",
    "        h_normalized_text.append(h_clean_sentence)\n",
    "        for sentence in h_normalized_text:\n",
    "            tokenData = nltk.word_tokenize(sentence)\n",
    "            for word in tokenData:\n",
    "                if word not in stop_words:\n",
    "                    h_stemData.append(ps.stem(word))\n",
    "    harryPotter[element] = h_stemData\n",
    "\n",
    "print(\"stemming 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 38777)\t0.0013169134088659989\n  (0, 38733)\t0.027392104038965377\n  (0, 38708)\t0.0086032637148345\n  (0, 38616)\t0.057307869763401256\n  (0, 38587)\t0.0015145477386770783\n  (0, 38578)\t0.015677904024057377\n  (0, 38565)\t0.035331674498240935\n  (0, 38502)\t0.011336388220755942\n  (0, 38432)\t0.004425140166460562\n  (0, 38423)\t0.0011010479203341467\n  (0, 38400)\t0.006040401794255721\n  (0, 38395)\t0.0035042319995763132\n  (0, 38359)\t0.010278407406606448\n  (0, 38346)\t0.031576821869970045\n  (0, 38328)\t0.025968159038507896\n  (0, 38324)\t0.04338432539669251\n  (0, 38289)\t0.006678807481014122\n  (0, 38083)\t0.00748275380908028\n  (0, 38055)\t0.009590532552081355\n  (0, 38051)\t0.008782777986629348\n  (0, 38011)\t1.2650613906402689e-05\n  (0, 37999)\t0.003426623993556433\n  (0, 37982)\t0.005140786300581928\n  (0, 37964)\t0.037196378622580975\n  (0, 37954)\t0.00017487450343866594\n  :\t:\n  (4, 691)\t0.015495337032055372\n  (4, 669)\t0.04230744322019207\n  (4, 648)\t0.0030658006668705133\n  (4, 629)\t0.011534367632245093\n  (4, 541)\t0.010531649681954068\n  (4, 533)\t0.027805498991517283\n  (4, 506)\t0.0025153496660833326\n  (4, 505)\t0.009947346569463961\n  (4, 407)\t0.012152885841635543\n  (4, 371)\t0.0015033329360946623\n  (4, 329)\t0.0018503336393436067\n  (4, 312)\t0.005112361933183876\n  (4, 297)\t0.005224507924116449\n  (4, 288)\t0.00676328808213235\n  (4, 287)\t0.00789715714245229\n  (4, 269)\t0.009868045828754663\n  (4, 263)\t0.005664718506879062\n  (4, 246)\t0.00734246191381947\n  (4, 223)\t0.006928186605189731\n  (4, 207)\t0.005093907889571726\n  (4, 199)\t0.0034387723424129\n  (4, 153)\t0.0039109396845632\n  (4, 69)\t0.0006594292193741959\n  (4, 61)\t0.009143717028219606\n  (4, 53)\t0.005544154213654857\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'for element in harryPotter.keys():\\n    tfidf.fit(harryPotter[element])\\n    script = tfidf.transform(harryPotter[element])\\n    X.append(script)\\n    y.append(element)\\n\\nprint(X.shape)'"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "'''HP data Vectorization'''\n",
    "# import numpy as np\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stemData = []\n",
    "hy=[]\n",
    "for element in harryPotter.keys():\n",
    "    stemData.append(harryPotter[element])\n",
    "    hy.append(element)\n",
    "HX_data = tfidf.transform(stemData)\n",
    "print(HX_data)\n",
    "HY_data = np.array(hy)\n",
    "\n",
    "'''for element in harryPotter.keys():\n",
    "    tfidf.fit(harryPotter[element])\n",
    "    script = tfidf.transform(harryPotter[element])\n",
    "    X.append(script)\n",
    "    y.append(element)\n",
    "\n",
    "print(X.shape)'''"
   ]
  },
  {
   "source": [
    "# Predict & Get Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ENTP' 'ENTP' 'ENTP' 'ENTP' 'ENTP']\n"
     ]
    }
   ],
   "source": [
    "import joblib  \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n",
    "\n",
    "model = joblib.load('XGBoost_undersample.pkl')\n",
    "# xgb_model_loaded.get_xgb_params()    # Get xgboost specific parameters.\n",
    "harrypotter_predict = model.predict(HX_data)\n",
    "print(harrypotter_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "295647"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "cols_when_model_builds = model.get_booster().feature_names\n",
    "len(cols_when_model_builds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
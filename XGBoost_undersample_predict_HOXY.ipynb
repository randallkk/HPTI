{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# (MBTI 별 Data 모으기: 생략) MBTI file 불러오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('.\\data\\\\training\\\\mbti.csv')"
   ]
  },
  {
   "source": [
    "# UnderSampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X_data = mbti['posts']\n",
    "X = X_data.values.reshape(-1, 1)\n",
    "\n",
    "y_data = mbti['type']\n",
    "print(y_data)\n",
    "y = y_data.values.reshape(-1, 1)\n",
    "print(y)\n",
    "\n",
    "X_resampled, y_resampled = RandomUnderSampler(random_state=0).fit_sample(X, y)\n",
    "#format(Counter(y_resampled))\n",
    "X_resampled #ndarray\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "source": [
    "# Type value "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_Types = {'I':0, 'E':1, 'N':0, \"S\":1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "b_Types_list =[{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    return[b_Types[i] for i in personality]\n",
    "\n",
    "def translate_back(personality):\n",
    "    s = \"\"\n",
    "    for i, j in enumerate(personality):\n",
    "        s += b_Types_list[i][j]\n",
    "    return s\n",
    "\n",
    "d = y[0]\n",
    "print(d)\n",
    "list_personality_b = np.array([translate_personality(p) for p in d])\n",
    "print(list_personality_b)"
   ]
  },
  {
   "source": [
    "# tokenize and stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_resampled.ravel()\n",
    "y_data = y_resampled\n",
    "\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data: #for sentence in X_data.ravel()\n",
    "    rm_urls = re.sub(r'http[s]?://\\S+', '', sentence)\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '', rm_urls.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "X_data = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gc\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in X_data:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        if word not in stop_words:\n",
    "            tempData.append(ps.stem(word))\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "print(stemData[0])\n",
    "stemData = np.array(stemDaa)\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]\n",
    "del [[mbti]]\n",
    "gc.collect()"
   ]
  },
  {
   "source": [
    "# Vectorization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    max_features=1500,\n",
    "    tokenizer=None,\n",
    "    stop_words=None,\n",
    "    max_df=0.7,\n",
    "    min_df=0.1\n",
    ")\n",
    "\n",
    "#X_ct = count_vectorizer.fit_transform(stemData)\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "print(tfidf.transform(stemData).toarray())\n",
    "X = tfidf.transform(stemData)\n",
    "print(X)\n",
    "y = np.array(y_data)"
   ]
  },
  {
   "source": [
    "# XGBoost_Undersampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state=1234)\n"
   ]
  },
  {
   "source": [
    "# Model Building & Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model building Start\")\n",
    "print(max(len(l) for l in X_train)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train, verbose=1)\n",
    "#model.get_booster().get_score(importance_type=\"gain\") #변수의 중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#test_hp = xgb.DMatrix(data=VHX_data, label=hy)\n",
    "#preds = model.predict(X_test)\n",
    "print(\"Model building End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'XGBoost_undersample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"Evaluation Start\")\n",
    "# labels과 guesses\n",
    "labels = preds\n",
    "guesses = y_test\n",
    "\n",
    "print(accuracy_score(labels, guesses))\n",
    "print(recall_score(labels, guesses, average=None))\n",
    "print(precision_score(labels, guesses, average=None))\n",
    "print(f1_score(labels, guesses, average=None))\n",
    "\n",
    "print(\"Evaluation END\")\n",
    "# https://eunsukimme.github.io/ml/2019/10/21/Accuracy-Recall-Precision-F1-score/"
   ]
  },
  {
   "source": [
    "# Harry Potter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['.\\\\data\\\\\\\\test\\\\HP_DUMBLEDORE.csv', '.\\\\data\\\\\\\\test\\\\HP_HAGRID.csv', '.\\\\data\\\\\\\\test\\\\HP_HARRY.csv', '.\\\\data\\\\\\\\test\\\\HP_HERMIONE.csv', '.\\\\data\\\\\\\\test\\\\HP_RON.csv']\n",
      "stemming 시작\n",
      "stemming 완료\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import glob   \n",
    "\n",
    "'''HP data Normalization and Stemming'''\n",
    "# bag_of_words = {} #{등장인물 : 해당인물의 stemData, ~}\n",
    "h_normalized_text = []\n",
    "h_stemData = []\n",
    "\n",
    "ps=PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "input_file = r'.\\data\\\\test\\\\'\n",
    "\n",
    "allHPfile_list = glob.glob(os.path.join(input_file, 'HP_*'))\n",
    "print(allHPfile_list)\n",
    "\n",
    "harryPotter={}\n",
    "for file in allHPfile_list:\n",
    "    df = pd.read_csv(file, sep=',', encoding='iso-8859-1')\n",
    "    harryPotter[df['Character'][0]] = df['Sentence'].ravel()\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "for element in harryPotter.keys():\n",
    "    for sentence in harryPotter[element]:\n",
    "        h_clean_sentence = re.sub('[^A-Za-z\\s]+', '', sentence.lower())\n",
    "        h_normalized_text.append(h_clean_sentence)\n",
    "        for sentence in h_normalized_text:\n",
    "            tokenData = nltk.word_tokenize(sentence)\n",
    "            for word in tokenData:\n",
    "                if word not in stop_words:\n",
    "                    h_stemData.append(ps.stem(word))\n",
    "    harryPotter[element] = h_stemData\n",
    "\n",
    "print(\"stemming 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (0, 1499)\t7.338210456553937e-06\n  (0, 89)\t7.338210456553937e-06\n  (0, 1154)\t1.7122491065292518e-05\n  (0, 169)\t4.158319258713898e-05\n  (0, 346)\t0.00010273494639175512\n  (0, 733)\t7.093603441335472e-05\n  (0, 1365)\t7.093603441335472e-05\n  (0, 36)\t7.827424486990865e-05\n  (0, 1372)\t9.784280608738582e-05\n  (0, 596)\t0.00010028887623957047\n  (0, 1495)\t0.0001614406300441866\n  (0, 1310)\t0.00029597448841434213\n  (0, 783)\t0.00016388670019637126\n  (0, 133)\t0.00016388670019637126\n  (0, 554)\t0.00017367098080510983\n  (0, 228)\t0.00017611705095729448\n  (0, 825)\t0.00017611705095729448\n  (0, 1441)\t0.00018590133156603307\n  (0, 1195)\t0.000193239542022587\n  (0, 857)\t0.00019568561217477164\n  (0, 1393)\t0.00020057775247914094\n  (0, 43)\t0.0002030238226313256\n  (0, 1467)\t0.0002030238226313256\n  (0, 1463)\t0.0002030238226313256\n  (0, 128)\t0.0002030238226313256\n  :\t:\n  (4, 443)\t0.007325980105793013\n  (4, 603)\t0.06672390161129277\n  (4, 416)\t0.04377487144349642\n  (4, 437)\t0.02377824794938694\n  (4, 962)\t0.032383522744772526\n  (4, 731)\t0.0381538022337761\n  (4, 1114)\t0.02042468577074179\n  (4, 1336)\t0.15738993787216884\n  (4, 1052)\t0.04587115356391866\n  (4, 29)\t0.017371990220815352\n  (4, 1015)\t0.005256604757044803\n  (4, 743)\t0.01573312321885164\n  (4, 1389)\t0.014316848600736731\n  (4, 24)\t0.02899082344369242\n  (4, 160)\t0.009089596685518142\n  (4, 582)\t0.16656514701301345\n  (4, 88)\t0.021938803194944085\n  (4, 552)\t0.091632233970989\n  (4, 20)\t0.024448471171085534\n  (4, 665)\t0.16711062065695062\n  (4, 809)\t0.02406199208704036\n  (4, 1017)\t0.173580476209479\n  (4, 1496)\t0.09525975600667884\n  (4, 713)\t0.0052688351078057265\n  (4, 1169)\t0.0052688351078057265\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'for element in harryPotter.keys():\\n    tfidf.fit(harryPotter[element])\\n    script = tfidf.transform(harryPotter[element])\\n    X.append(script)\\n    y.append(element)\\n\\nprint(X.shape)'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "'''HP data Vectorization'''\n",
    "# import numpy as np\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stemData = []\n",
    "hy=[]\n",
    "for element in harryPotter.keys():\n",
    "    stemData.append(harryPotter[element])\n",
    "    hy.append(element)\n",
    "HX_data = tfidf.transform(stemData)\n",
    "print(HX_data)\n",
    "HY_data = np.array(hy)\n",
    "\n",
    "'''for element in harryPotter.keys():\n",
    "    tfidf.fit(harryPotter[element])\n",
    "    script = tfidf.transform(harryPotter[element])\n",
    "    X.append(script)\n",
    "    y.append(element)\n",
    "\n",
    "print(X.shape)'''"
   ]
  },
  {
   "source": [
    "# Predict & Get Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ESFP' 'ESFP' 'ESFP' 'ESFP' 'ESFP']\n"
     ]
    }
   ],
   "source": [
    "import joblib  \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n",
    "\n",
    "model = joblib.load('XGBoost_undersample.pkl')\n",
    "# xgb_model_loaded.get_xgb_params()    # Get xgboost specific parameters.\n",
    "harrypotter_predict = model.predict(HX_data)\n",
    "print(harrypotter_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "38863"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "cols_when_model_builds = model.get_booster().feature_names\n",
    "len(cols_when_model_builds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
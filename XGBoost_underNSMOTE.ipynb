{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# (MBTI 별 Data 모으기: 생략) MBTI file 불러오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('.\\data\\\\training\\\\mbti.csv')"
   ]
  },
  {
   "source": [
    "# tokenize and stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\n",
      "normalize 완료\n"
     ]
    }
   ],
   "source": [
    "X_data = mbti['posts'].ravel()\n",
    "y_data = mbti['type'].ravel()\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data: #for sentence in X_data.ravel()\n",
    "    rm_urls = re.sub(r'http[s]?://\\S+', '', sentence)\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '', rm_urls.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "X_data = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gc\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n",
      "['oi', 'went', 'break', 'month', 'ago', 'togeth', 'year', 'plan', 'life', 'around', 'relationship', 'wasnt', 'one', 'break', 'relationship', 'might', 'imagin', 'ourenfj', 'pun', 'mani', 'punswel', 'person', 'dont', 'go', 'much', 'attract', 'gener', 'see', 'chang', 'that', 'good', 'alreadi', 'may', 'ask', 'want', 'mere', 'sexualsorri', 'infp', 'im', 'realli', 'postrock', 'post', 'go', 'leav', 'sneak', 'thread', 'dthat', 'doesnt', 'sound', 'enfj', 'think', 'x', 'id', 'never', 'act', 'cold', 'toward', 'romant', 'interest', 'get', 'warm', 'side', 'think', 'like', 'someon', 'way', 'want', 'straight', 'themnot', 'realli', 'im', 'mostli', 'guitar', 'player', 'bass', 'obsess', 'im', 'person', 'ask', 'bass', 'louder', 'least', 'relev', 'like', 'tri', 'new', 'instrument', 'sometim', 'add', 'aguitar', 'mostli', 'add', 'bass', 'glockenspiel', 'harmonica', 'drum', 'ocarinaahahah', 'im', 'sorri', 'im', 'laugh', 'hard', 'realli', 'plagu', 'never', 'thought', 'possiblemostli', 'sound', 'lyric', 'ill', 'obviusli', 'consid', 'listen', 'mostli', 'post', 'math', 'rock', 'dont', 'care', 'much', 'wordshii', 'welcomei', 'two', 'infj', 'close', 'friend', 'never', 'though', 'naiv', 'actual', 'take', 'serious', 'though', 'notic', 'nonnf', 'friend', 'dont', 'seem', 'way', 'usual', 'dont', 'realli', 'know', 'isthi', 'actual', 'shouldnt', 'cooler', 'pollut', 'big', 'deal', 'peopl', 'avoid', 'better', 'isnt', 'like', 'peopl', 'dont', 'car', 'least', 'countri', 'livewhat', 'ignor', 'side', 'effect', 'slightli', 'mess', 'peopl', 'around', 'let', 'tri', 'get', 'pratic', 'exampl', 'x', 'constatli', 'tri', 'get', 'nerv', 'provok', 'orwelcomehi', 'fellow', 'enfj', 'wonder', 'deal', 'direct', 'conflict', 'within', 'group', 'your', 'feel', 'like', 'someon', 'doesnt', 'like', 'much', 'start', 'bother', 'lot', 'justwelcom', 'isnt', 'way', 'see', 'also', 'part', 'person', 'close', 'two', 'infj', 'one', 'low', 'self', 'esteem', 'amazingli', 'pessimist', 'pretti', 'much', 'everyth', 'otherenfj', 'blue', 'violetsorri', 'infj', 'enfj', 'kinda', 'relat', 'im', 'get', 'point', 'right', 'think', 'low', 'self', 'esteem', 'transvers', 'type', 'quit', 'low', 'self', 'esteem', 'im', 'peopleth', 'friend', 'say', 'welcom', 'hiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii', 'ddddi', 'dont', 'think', 'infj', 'thing', 'social', 'behavior', 'mayb', 'someth', 'good', 'make', 'peopl', 'jealou', 'mayb', 'your', 'pretti', 'smart', 'get', 'along', 'peopl', 'want', 'dwelcom', 'dwelcom', 'dwelcom', 'dddthi', 'truei', 'dont', 'know', 'consid', 'compliment', 'thing', 'make', 'day', 'well', 'done', 'pleas', 'dont', 'ever', 'stop', 'your', 'wish', 'abl', 'love', 'somethingi', 'think', 'notic', 'differ', 'behavior', 'drink', 'littl', 'much', 'get', 'fairli', 'happi', 'less', 'alcohol', 'friend', 'bit', 'time', 'never', 'get', 'realli', 'realli', 'drunk', 'probabl', 'iiv', 'walk', 'away', 'estp', 'theyr', 'realli', 'hard', 'get', 'along', 'also', 'istp', 'kinda', 'feel', 'like', 'energi', 'suck', 'longer', 'togeth', 'understand', 'anddamn', 'friendli', 'fun', 'read', 'nice', 'forum', 'envoirnmentinfj', 'probabl', 'favorit', 'type', 'notic', 'behavior', 'differ', 'group', 'peopl', 'two', 'us', 'privat', 'environ', 'usual', 'take', 'lead', 'talk', 'likei', 'agre', 'cursedsoul', 'actual', 'reciev', 'thing', 'object', 'whatev', 'doesnt', 'make', 'happi', 'realli', 'enjoy', 'peopl', 'stuff', 'mean', 'theyr', 'actual', 'give', 'time', 'andthank', 'welcom', 'ye', 'sorri', 'confus', 'thank', 'di', 'think', 'might', 'romant', 'interst', 'mayb', 'said', 'wouldnt', 'flirt', 'tri', 'get', 'feedback', 'im', 'say', 'lot', 'peopl', 'imthank', 'hi', 'wave', 'ive', 'lurk', 'around', 'forum', 'time', 'think', 'time', 'proper', 'introduct', 'im', 'ri', 'enfj', 'im', 'realli', 'interest', 'pretti', 'much']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in X_data:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        if word not in stop_words:\n",
    "            tempData.append(ps.stem(word))\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "print(stemData[0])\n",
    "\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]\n",
    "del [[mbti]]\n",
    "gc.collect()"
   ]
  },
  {
   "source": [
    "# vectorization (tfidf)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.        0.        0.        ... 0.        0.        0.       ]\n [0.0518922 0.        0.        ... 0.        0.        0.       ]\n [0.        0.        0.        ... 0.        0.        0.       ]\n ...\n [0.        0.        0.        ... 0.        0.        0.       ]\n [0.        0.        0.        ... 0.        0.        0.       ]\n [0.        0.        0.        ... 0.        0.        0.       ]]\n"
     ]
    }
   ],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "print(tfidf.transform(stemData).toarray())\n",
    "X = tfidf.transform(stemData)\n",
    "y = np.array(y_data)\n"
   ]
  },
  {
   "source": [
    "# Garbage Collection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "# import os\n",
    "# import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "for clean_sentence in normalized_text:\n",
    "    del clean_sentence\n",
    "del normalized_text\n",
    "\n",
    "del X_data\n",
    "\n",
    "for component in y_data:\n",
    "    del component\n",
    "del y_data\n",
    "\n",
    "for data in stemData:   # delete 'stemData'\n",
    "    for component in data:\n",
    "        del component\n",
    "    del data\n",
    "del stemData\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "source": [
    "# UnderSampling and SMOTE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'ENFJ': 500,\n",
       "         'ENFP': 500,\n",
       "         'ENTJ': 500,\n",
       "         'ENTP': 500,\n",
       "         'ESFJ': 500,\n",
       "         'ESFP': 500,\n",
       "         'ESTJ': 500,\n",
       "         'ESTP': 500,\n",
       "         'INFJ': 500,\n",
       "         'INFP': 500,\n",
       "         'INTJ': 500,\n",
       "         'INTP': 500,\n",
       "         'ISFJ': 500,\n",
       "         'ISFP': 500,\n",
       "         'ISTJ': 500,\n",
       "         'ISTP': 500})"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_resampled, y_resampled = SMOTE(random_state=0).fit_sample(X, y)\n",
    "X_resampled, y_resampled = RandomUnderSampler(random_state=0,\n",
    "sampling_strategy={\n",
    "         'ENFP': 500,\n",
    "         'ENFJ': 500,\n",
    "         'ENTJ': 500,\n",
    "         'ENTP': 500,\n",
    "         'ESFJ': 500,\n",
    "         'ESFP': 500,\n",
    "         'ESTJ': 500,\n",
    "         'ESTP': 500,\n",
    "         'INFJ': 500,\n",
    "         'INFP': 500,\n",
    "         'INTJ': 500,\n",
    "         'INTP': 500,\n",
    "         'ISFJ': 500,\n",
    "         'ISFP': 500,\n",
    "         'ISTJ': 500,\n",
    "         'ISTP': 500\n",
    "         }).fit_sample(X_resampled, y_resampled)\n",
    "#format(Counter(y_resampled))\n",
    "X_resampled #ndarray\n",
    "Counter(y_resampled)"
   ]
  },
  {
   "source": [
    "# XGBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building Start\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "print(\"Model building Start\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state=1234)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building End\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "print(\"Model building End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['XGBoost_smoteNunder.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'XGBoost_smoteNunder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation Start\n0.6477141759508259\n[0.51428571 0.62564103 0.60465116 0.62621359 0.33333333 1.\n 0.2        0.58333333 0.64102564 0.66360856 0.62650602 0.6725\n 0.6875     0.66101695 0.75675676 0.68292683]\n[0.36       0.58937198 0.39393939 0.60280374 0.08333333 0.06666667\n 0.125      0.30434783 0.72544643 0.78909091 0.65       0.68101266\n 0.41509434 0.45348837 0.42424242 0.62222222]\n[0.42352941 0.60696517 0.47706422 0.61428571 0.13333333 0.125\n 0.15384615 0.4        0.68062827 0.72093023 0.63803681 0.67672956\n 0.51764706 0.53793103 0.54368932 0.65116279]\nEvaluation END\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"Evaluation Start\")\n",
    "# labels과 guesses\n",
    "labels = preds\n",
    "guesses = y_test\n",
    "\n",
    "print(accuracy_score(labels, guesses))\n",
    "print(recall_score(labels, guesses, average=None))\n",
    "print(precision_score(labels, guesses, average=None))\n",
    "print(f1_score(labels, guesses, average=None))\n",
    "\n",
    "print(\"Evaluation END\")\n",
    "# https://eunsukimme.github.io/ml/2019/10/21/Accuracy-Recall-Precision-F1-score/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
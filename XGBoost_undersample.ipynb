{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# (MBTI 별 Data 모으기: 생략) MBTI file 불러오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('.\\data\\\\training\\\\mbti.csv')"
   ]
  },
  {
   "source": [
    "# UnderSampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'ENFJ': 39,\n",
       "         'ENFP': 39,\n",
       "         'ENTJ': 39,\n",
       "         'ENTP': 39,\n",
       "         'ESFJ': 39,\n",
       "         'ESFP': 39,\n",
       "         'ESTJ': 39,\n",
       "         'ESTP': 39,\n",
       "         'INFJ': 39,\n",
       "         'INFP': 39,\n",
       "         'INTJ': 39,\n",
       "         'INTP': 39,\n",
       "         'ISFJ': 39,\n",
       "         'ISFP': 39,\n",
       "         'ISTJ': 39,\n",
       "         'ISTP': 39})"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X_data = mbti['posts']\n",
    "X = X_data.values.reshape(-1, 1)\n",
    "\n",
    "y_data = mbti['type']\n",
    "y = y_data.values.reshape(-1, 1)\n",
    "\n",
    "X_resampled, y_resampled = RandomUnderSampler(random_state=0).fit_sample(X, y)\n",
    "#format(Counter(y_resampled))\n",
    "X_resampled #ndarray\n",
    "Counter(y_resampled)\n"
   ]
  },
  {
   "source": [
    "# Harry Potter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['.\\\\data\\\\\\\\test\\\\HP_DUMBLEDORE.csv', '.\\\\data\\\\\\\\test\\\\HP_HAGRID.csv', '.\\\\data\\\\\\\\test\\\\HP_HARRY.csv', '.\\\\data\\\\\\\\test\\\\HP_HERMIONE.csv', '.\\\\data\\\\\\\\test\\\\HP_RON.csv']\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-70562b77f903>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mh_normalized_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_clean_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mh_normalized_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mtokenData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mtempData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenData\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     return [\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     ]\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     return [\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m     ]\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\nltk\\tokenize\\destructive.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# Handles parentheses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\re.py\u001b[0m in \u001b[0;36m_subx\u001b[1;34m(pattern, template)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[1;31m# literal replacement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import glob   \n",
    "\n",
    "bag_of_words = {} #{등장인물 : 해당인물의 stemData, ~}\n",
    "h_normalized_text = []\n",
    "h_stemData = []\n",
    "HY_data = []\n",
    "\n",
    "ps=PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "input_file = r'.\\data\\\\test\\\\'\n",
    "\n",
    "allHPfile_list = glob.glob(os.path.join(input_file, 'HP_*'))\n",
    "print(allHPfile_list)\n",
    "\n",
    "for file in allHPfile_list:\n",
    "    df = pd.read_csv(file, encoding='iso-8859-1')\n",
    "    harryPotter = df['Sentence'].ravel()\n",
    "    for sentence in harryPotter:\n",
    "        h_clean_sentence = re.sub('[^A-Za-z\\s]+', '', sentence.lower())\n",
    "        h_normalized_text.append(h_clean_sentence)\n",
    "        for sentence in h_normalized_text:\n",
    "            tokenData = nltk.word_tokenize(sentence)\n",
    "            tempData = []\n",
    "            for word in tokenData:\n",
    "                if word not in stop_words:\n",
    "                    tempData.append(ps.stem(word))\n",
    "            h_stemData.append(tempData)\n",
    "    HY_data.append(df['Character'][0])\n",
    "\n",
    "print(\"stemming 완료\")\n",
    "print(h_stemData[1])\n",
    "print(h_stemData[2])\n",
    "print(HY_data)"
   ]
  },
  {
   "source": [
    "# harry potter data Vectorization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(h_stemData)\n",
    "print(tfidf.transform(h_stemData).toarray())\n",
    "VHX_data = tfidf.transform(h_stemData).toarray()"
   ]
  },
  {
   "source": [
    "# tokenize and stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\n",
      "normalize 완료\n"
     ]
    }
   ],
   "source": [
    "X_data = X_resampled.ravel()\n",
    "y_data = y_resampled\n",
    "\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data: #for sentence in X_data.ravel()\n",
    "    rm_urls = re.sub(r'http[s]?://\\S+', '', sentence)\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '', rm_urls.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "X_data = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gc\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n",
      "['want', 'know', 'feel', 'intp', 'actual', 'intp', 'idea', 'intp', 'havent', 'met', 'anyten', 'million', 'hug', 'mani', 'amaz', 'convers', 'held', 'absenc', 'vow', 'never', 'leav', 'realli', 'lot', 'written', 'recent', 'terriblyfacebook', 'way', 'emphas', 'other', 'error', 'cognit', 'becom', 'obviou', 'lead', 'wonder', 'im', 'friend', 'mani', 'peopl', 'rememb', 'peopl', 'moremmmmhm', 'ive', 'wonder', 'late', 'ei', 'think', 'definit', 'much', 'needwant', 'compani', 'around', 'regardless', 'time', 'togeth', 'spenthahah', 'love', 'think', 'that', 'go', 'topic', 'excel', 'convers', 'thank', 'particip', 'indulg', 'interest', 'enlighteningsomeon', 'creat', 'chitchat', 'thread', 'enfj', 'forum', 'enfj', 'vigilantli', 'await', 'type', 'wander', 'start', 'conversationswa', 'enfp', 'ive', 'caught', 'mani', 'text', 'look', 'around', 'shini', 'object', 'give', 'midsyl', 'mm', 'hmm', 'alway', 'mani', 'andi', 'tire', 'extravert', 'energysap', 'sentiment', 'arent', 'batshit', 'loud', 'outgo', 'even', 'dryon', 'topic', 'guy', 'find', 'get', 'better', 'extrovert', 'onlinealso', 'iamken', 'one', 'yall', 'sure', 'type', 'lot', 'think', 'introvert', 'thing', 'rather', 'fi', 'thing', 'though', 'intp', 'fe', 'way', 'intp', 'infp', 'actual', 'haveb', 'contrari', 'think', 'initi', 'interact', 'begin', 'selffocu', 'realli', 'depend', 'im', 'feel', 'though', 'im', 'insecur', 'kind', 'day', 'someon', 'isth', 'hand', 'yesi', 'attribut', 'credibl', 'advic', 'follow', 'experi', 'date', 'hous', 'keep', 'intp', 'fed', 'littl', 'longer', 'two', 'year', 'find', 'intp', 'rareyour', 'abil', 'candid', 'refresh', 'thing', 'unhealthi', 'usi', 'clarifi', 'rambl', 'lie', 'mean', 'less', 'valid', 'next', 'less', 'reliabl', 'misconstru', 'intp', 'differ', 'other', 'mean', 'vastli', 'aeffi', 'iamtp', 'interest', 'thought', 'intp', 'behav', 'like', 'infp', 'think', 'ive', 'seen', 'happen', 'intp', 'current', 'think', 'intp', 'differ', 'fromth', 'cognit', 'function', 'test', 'differ', 'mbti', 'magicwhoa', 'much', 'happen', 'day', 'want', 'take', 'opportun', 'make', 'observ', 'page', 'sinc', 'feel', 'like', 'miss', 'thismight', 'enneagram', 'type', 'thing', 'relat', 'im', 'enfj', 'think', 'ask', 'make', 'enfj', 'happi', 'peopl', 'make', 'us', 'happyhistoir', 'soo', 'unfold', 'make', 'move', 'yet', 'peopl', 'ha', 'guarante', 'like', 'quieter', 'usual', 'also', 'dont', 'confus', 'introvers', 'lack', 'confid', 'samei', 'think', 'odd', 'behavior', 'enfj', 'behavior', 'your', 'describ', 'sound', 'introvert', 'mayb', 'he', 'intimidatedi', 'cant', 'speak', 'everyon', 'els', 'person', 'experi', 'alway', 'think', 'even', 'someth', 'overthink', 'even', 'person', 'interact', 'seem', 'absorb', 'convers', 'butno', 'absolut', 'wouldnt', 'overwhelm', 'think', 'person', 'explain', 'relat', 'would', 'even', 'thing', 'way', 'plan', 'know', 'mean', 'youjust', 'got', 'thing', 'left', 'eye', 'still', 'hurt', 'rawrwel', 'im', 'go', 'honest', 'someon', 'walk', 'like', 'cat', 'fall', 'fit', 'rage', 'burst', 'laugh', 'second', 'later', 'sound', 'aw', 'immatur', 'unstabl', 'possiblyr', 'someon', 'field', 'true', 'constantli', 'odd', 'buy', 'ive', 'ask', 'other', 'forum', 'dont', 'think', 'mani', 'peopl', 'inbait', 'approv', 'would', 'bug', 'answer', 'would', 'cant', 'make', 'stop', 'period', 'that', 'itheheheheh', 'idk', 'id', 'say', 'im', 'pretti', 'organ', 'schedul', 'that', 'natur', 'j', 'said', 'dont', 'think', 'anyth', 'sex', 'drive', 'like', 'lofti', 'said', 'probablyidk', 'well', 'sex', 'food', 'correl', 'person', 'type', 'mayb', 'thatd', 'interest', 'thing', 'researchi', 'also', 'disagre', 'think', 'simpli', 'obviou', 'extravert', 'spastic', 'obnoxi', 'natur', 'extravers', 'mere', 'feel', 'divulg', 'easili', 'process', 'externallyhahaha', 'someon', 'sound', 'excit', 'forgot', 'mention', 'poni', 'also', 'might', 'want', 'chang', 'word', 'bit', 'screw', 'need', 'protect', 'im', 'sorri', 'blanket', 'statement', 'peopl', 'run', 'gamut', 'e', 'black', 'white', 'youll', 'find', 'plenti', 'extravert', 'person', 'prefer', 'spend', 'time', 'home', 'one', 'twohahaha', 'connoisseur', 'reject', 'play', 'dumb', 'alway', 'goto', 'women', 'tend', 'get', 'shove', 'face', 'eventu', 'total', 'possibl', 'id', 'send', 'pictur', 'might', 'awkward', 'poor', 'intp', 'intens', 'stori', 'cant', 'wait', 'see', 'happen', 'hope', 'run', 'keep', 'mind', 'infpenfj', 'interact', 'weird', 'go', 'head', 'mightkittydian', 'true', 'said', 'pretti', 'much', 'demand', 'introduc', 'two', 'year', 'ago', 'nbd', 'wont', 'chang', 'opinion', 'thing', 'realiz', 'howdo', 'tell', 'im', 'intrigu', 'happen', 'intp', 'fire', 'readythi', 'ador', 'perhap', 'also', 'conflict', 'two', 'interact', 'like', 'typic', 'curiou', 'shi', 'approach', 'someon', 'dont', 'expecti', 'cant', 'speak', 'everyon', 'ill', 'speak', 'sign', 'enfj', 'give', 'away', 'attract', 'your', 'attract', 'depend', 'much', 'inform', 'aboutif', 'read', 'link', 'youll', 'see', 'extinguish', 'partner', 'infp', 'actual', 'enfp', 'love', 'japanes', 'literatur', 'someth', 'look', 'mayb', 'share', 'favorit', 'book', 'note', 'one', 'favorit', 'novella', 'banana', 'yoshimotosometim', 'there', 'effect', 'wholesom', 'isnt', 'intrigu', 'challeng', 'enough', 'factor', 'young', 'boy', 'chang', 'friend', 'get', 'older', 'tell', 'absolut', 'toim', 'go', 'say', 'word', 'caution', 'onlin', 'survey', 'least', 'reliabl', 'method', 'take', 'poll', 'two', 'big', 'reason', 'dont', 'know', 'person', 'answer', 'question', 'theythi', 'terrifi', 'thank', 'share', 'she', 'beauti']\n",
      "['sorri', 'know', 'post', 'year', 'old', 'happi', 'read', 'make', 'much', 'sens', 'especi', 'realiz', 'im', 'truli', 'enfj', 'esfj', 'rais', 'byentp', 'sure', 'winkhi', 'liter', 'saw', 'today', 'havent', 'perc', 'month', 'might', 'retir', 'dont', 'think', 'forum', 'healti', 'unsurethi', 'hilari', 'hahaha', 'blue', 'awesom', 'word', 'super', 'hugsi', 'wouldnt', 'say', 'bitch', 'seem', 'littl', 'selfcent', 'mayb', 'need', 'wake', 'call', 'offens', 'anythingmast', 'wolf', 'isnt', 'jealou', 'admir', 'friend', 'fact', 'he', 'like', 'brother', 'ewwwi', 'wasnt', 'think', 'didnt', 'clear', 'idea', 'suppos', 'first', 'xd', 'caught', 'im', 'quick', 'learner', 'next', 'vers', 'want', 'sing', 'main', 'part', 'thenit', 'frustrat', 'introvert', 'expect', 'read', 'mind', 'someth', 'boredi', 'love', 'make', 'music', 'think', 'hit', 'right', 'fun', 'thank', 'love', 'friend', 'cours', 'winkdear', 'entj', 'type', 'problem', 'look', 'like', 'cring', 'anytim', 'forc', 'compliment', 'someth', 'ever', 'deserv', 'treat', 'like', 'amaz', 'someon', 'stick', 'someth', 'like', 'get', 'stuck', 'point', 'dont', 'think', 'take', 'care', 'someth', 'someon', 'comesholla', 'back', 'your', 'quit', 'gorgeou', 'winkanyon', 'chat', 'tonight', 'today', 'friday', 'sort', 'happi', 'mibbit', 'tc', 'let', 'knowdawww', 'couldnt', 'love', 'car', 'seat', 'background', 'could', 'laughinghello', 'fellow', 'esfj', 'pretti', 'crappi', 'day', 'work', 'today', 'anyth', 'everyth', 'could', 'go', 'wrong', 'luckili', 'abl', 'let', 'lot', 'go', 'man', 'day', 'like', 'suck', 'life', 'ican', 'someon', 'explain', 'esfj', 'eleph', 'good', 'memori', 'someth', 'confused', 'said', 'ill', 'say', 'thank', 'infp', 'put', 'presenc', 'post', 'hug', 'infpsim', 'bring', 'thread', 'aliv', 'sticki', 'ugh', 'feel', 'like', 'bitch', 'today', 'dont', 'know', 'problem', 'certain', 'thread', 'realli', 'enjoy', 'except', 'amho', 'think', 'creat', 'problem', 'boyfriend', 'move', 'creat', 'awkward', 'situat', 'begin', 'doesnt', 'sound', 'like', 'esfj', 'healthi', 'state', 'right', 'ifwow', 'meep', 'good', 'go', 'think', 'hundr', 'hug', 'doll', 'daili', 'basi', 'tongu', 'keep', 'good', 'work', 'ask', 'feel', 'noth', 'especi', 'person', 'one', 'import', 'peopl', 'mayb', 'denial', 'stage', 'grief', 'stage', 'griefthi', 'great', 'advic', 'thank', 'post', 'sound', 'like', 'youv', 'learn', 'lot', 'life', 'put', 'practic', 'moral', 'applic', 'behind', 'show', 'matur', 'growth', 'post', 'ii', 'hear', 'feel', 'like', 'act', 'like', 'differ', 'person', 'im', 'good', 'read', 'peopl', 'im', 'interest', 'other', 'perspect', 'tri', 'like', 'aa', 'base', 'opinion', 'entj', 'bear', 'entp', 'cross', 'monkey', 'fox', 'laugh', 'ill', 'think', 'come', 'back', 'happyher', 'good', 'resourc', 'stop', 'walk', 'eggshel', 'take', 'life', 'back', 'someon', 'care', 'borderlin', 'person', 'disord', 'paul', 'mason', 'ms', 'randi', 'kreger', 'ex', 'hasgener', 'question', 'anyal', 'enfj', 'read', 'enfj', 'posit', 'peopl', 'met', 'posit', 'glass', 'half', 'fulltyp', 'also', 'enneagram', 'youi', 'decid', 'need', 'hang', 'enfj', 'forum', 'often', 'relat', 'much', 'enfj', 'guy', 'awesom', 'happyim', 'sorri', 'go', 'ive', 'hug', 'listen', 'music', 'go', 'walk', 'help', 'also', 'surround', 'peopl', 'care', 'keep', 'busi', 'help', 'tooim', 'total', 'like', 'although', 'everi', 'enjoy', 'downtim', 'winkoh', 'rememb', 'your', 'guy', 'told', 'fetish', 'wear', 'women', 'underwearmayb', 'better', 'question', 'drysimilar', 'im', 'w', 'w', 'w', 'like', 'base', 'person', 'cognit', 'function', 'cognit', 'function', 'use', 'person', 'ei', 'prefer', 'imho', 'dang', 'alway', 'miss', 'fun', 'stuff', 'sadmi', 'infj', 'best', 'friend', 'introduc', 'hook', 'love', 'learn', 'peopl', 'tri', 'figur', 'learn', 'person', 'psycholog', 'self', 'awar', 'realli', 'stimul', 'mei', 'kinda', 'relat', 'im', 'w', 'love', 'enneagram', 'btw', 'wink', 'even', 'two', 'realli', 'strong', 'sens', 'need', 'feel', 'accomplish', 'put', 'lot', 'see', 'relat', 'lot', 'toi', 'hate', 'ex', 'accident', 'call', 'left', 'minut', 'messag', 'vm', 'girl', 'he', 'right', 'even', 'though', 'dont', 'want', 'get', 'away', 'waswond', 'esfj', 'like', 'child', 'rememb', 'dreami', 'lot', 'freespirit', 'sensit', 'cri', 'everyth', 'hmmm', 'lot', 'ofbecaus', 'esfj', 'selfless', 'give', 'looooov', 'peopl', 'plu', 'esfj', 'good', 'look', 'winki', 'vote', 'esfp', 'lot', 'posit', 'energi', 'fun', 'love', 'attent', 'etc', 'etcsorri', 'im', 'late', 'post', 'think', 'knew', 'heart', 'heart', 'guy', 'bad', 'news', 'gut', 'alway', 'right', 'matter', 'much', 'tri', 'reason', 'way', 'somethingahem', 'go', 'toe', 'toe', 'entp', 'day', 'tri', 'drytonight', 'one', 'night', 'think', 'would', 'feel', 'good', 'cri', 'cri', 'pmsisawesomenotreallyanyon', 'chat', 'tonightim', 'obsess', 'song', 'human', 'natur', 'right', 'includ', 'cover', 'variat', 'especi', 'cover', 'cello', 'guy', 'amaz', 'dinner', 'group', 'peopl', 'tonight', 'across', 'way', 'see', 'guy', 'girl', 'date', 'tell', 'date', 'see', 'tension', 'yet', 'appear', 'greaty', 'tc', 'night', 'seen', 'proof', 'horribl', 'hiccup', 'work', 'realli', 'thing', 'work', 'crazyomg', 'read', 'hahahaha', 'ok', 'here', 'happen', 'hard', 'time', 'get', 'sleep', 'late', 'last', 'night', 'took', 'one', 'ambien', 'prescript', 'long', 'time']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1974"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in X_data:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        if word not in stop_words:\n",
    "            tempData.append(ps.stem(word))\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "print(stemData[0])\n",
    "print(stemData[1])\n",
    "\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]\n",
    "del [[mbti]]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n [0.05858548 0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n ...\n [0.04542902 0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#vectorization (tfidf)\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "print(tfidf.transform(stemData).toarray())\n",
    "X = tfidf.transform(stemData).toarray()\n",
    "y = np.array(y_data)"
   ]
  },
  {
   "source": [
    "# XGBoost_Undersampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building Start\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "print(\"Model building Start\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state=1234)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train, verbose=1)\n",
    "#model.get_booster().get_score(importance_type=\"gain\") #변수의 중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "[WinError -529697949] Windows Error 0xe06d7363",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-29a2462bc778>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_hp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVHX_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#preds = model.predict(X_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_hp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model building End\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdispatch_data_backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m         handle, feature_names, feature_types = dispatch_data_backend(\n\u001b[0m\u001b[0;32m    435\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[0mthreads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnthread\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36mdispatch_data_backend\u001b[1;34m(data, missing, threads, feature_names, feature_types)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_from_scipy_csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_numpy_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m         return _from_numpy_array(data, missing, threads, feature_names,\n\u001b[0m\u001b[0;32m    495\u001b[0m                                  feature_types)\n\u001b[0;32m    496\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36m_from_numpy_array\u001b[1;34m(data, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[0mflatten\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_transform_np_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     _check_call(_LIB.XGDMatrixCreateFromMat_omp(\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0mflatten\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOINTER\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mc_bst_ulong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError -529697949] Windows Error 0xe06d7363"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "test_hp = xgb.DMatrix(VHX_data, label=y_test)\n",
    "#preds = model.predict(X_test)\n",
    "preds = model.predict(test_hp)\n",
    "print(\"Model building End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['XGBoost_undersample.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'XGBoost_undersample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation Start\n0.5\n[0.66666667 0.54545455 0.45454545 0.16666667 0.8        0.29411765\n 0.58333333 0.9        0.33333333 0.90909091 0.42857143 0.125\n 0.42857143 0.42857143 0.47368421 0.53846154]\n[0.75       0.46153846 0.625      0.2        0.5        0.45454545\n 0.46666667 0.69230769 0.125      0.76923077 0.33333333 0.1\n 0.6        0.6        0.5        0.63636364]\n[0.70588235 0.5        0.52631579 0.18181818 0.61538462 0.35714286\n 0.51851852 0.7826087  0.18181818 0.83333333 0.375      0.11111111\n 0.5        0.5        0.48648649 0.58333333]\nEvaluation END\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"Evaluation Start\")\n",
    "# labels과 guesses\n",
    "labels = preds\n",
    "guesses = y_test\n",
    "\n",
    "print(accuracy_score(labels, guesses))\n",
    "print(recall_score(labels, guesses, average=None))\n",
    "print(precision_score(labels, guesses, average=None))\n",
    "print(f1_score(labels, guesses, average=None))\n",
    "\n",
    "print(\"Evaluation END\")\n",
    "# https://eunsukimme.github.io/ml/2019/10/21/Accuracy-Recall-Precision-F1-score/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
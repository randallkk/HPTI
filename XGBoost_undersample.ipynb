{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# MBTI별 포스팅 모으기 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('./data/mbti_1.csv')\n",
    "\n",
    "types = mbti.groupby('type').count()\n",
    "\n",
    "output_file = '.\\data\\\\training\\\\mbti.csv' # raw string이 아니라 '\\'를 쓰려면 \\\\라고 해야 함\n",
    "allData = []\n",
    "\n",
    "for type in types.index:\n",
    "    condition = mbti['type'] == type # condition: mbti['type']가 topfive의 원소인 type 같을 bool 조건\n",
    "    ownsentence = mbti[condition]  # ownsentence: condition에 맞는 row만 filtering한 dataframe\n",
    "    allData.append(ownsentence)\n",
    "dataCombine = pd.concat(allData, axis=0, ignore_index=True)\n",
    "dataCombine.to_csv(output_file, index=False)\n",
    "\n",
    "#input_file = r'.\\data\\training'\n",
    "#allFile_list = glob.glob(os.path.join(input_file, 'mbti_*'))\n",
    "#for file in allFile_list:\n",
    "#    csv = pd.read_csv(file,sep=';', encoding='iso-8859-1') # for구문으로 csv파일들을 읽어 들인다\n",
    "#    cleanMbti = csv['posts'].str.replace('[^A-Za-z\\s]+', '')"
   ]
  },
  {
   "source": [
    "# OverSampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      posts\ntype       \nENFJ    190\nENFP    675\nENTJ    231\nENTP    685\nESFJ     42\nESFP     48\nESTJ     39\nESTP     89\nINFJ   1470\nINFP   1832\nINTJ   1091\nINTP   1304\nISFJ    166\nISFP    271\nISTJ    205\nISTP    337\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'ENFJ': 39,\n",
       "         'ENFP': 39,\n",
       "         'ENTJ': 39,\n",
       "         'ENTP': 39,\n",
       "         'ESFJ': 39,\n",
       "         'ESFP': 39,\n",
       "         'ESTJ': 39,\n",
       "         'ESTP': 39,\n",
       "         'INFJ': 39,\n",
       "         'INFP': 39,\n",
       "         'INTJ': 39,\n",
       "         'INTP': 39,\n",
       "         'ISFJ': 39,\n",
       "         'ISFP': 39,\n",
       "         'ISTJ': 39,\n",
       "         'ISTP': 39})"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "mbti=pd.read_csv('./data/mbti_1.csv')\n",
    "\n",
    "types = mbti.groupby('type').count()\n",
    "\n",
    "output_file = '.\\data\\\\training\\\\mbti.csv' # raw string이 아니라 '\\'를 쓰려면 \\\\라고 해야 함\n",
    "allData = []\n",
    "\n",
    "for type in types.index:\n",
    "    condition = mbti['type'] == type # condition: mbti['type']가 topfive의 원소인 type 같을 bool 조건\n",
    "    ownsentence = mbti[condition]  # ownsentence: condition에 맞는 row만 filtering한 dataframe\n",
    "    allData.append(ownsentence)\n",
    "dataCombine = pd.concat(allData, axis=0, ignore_index=True)\n",
    "dataCombine.to_csv(output_file, index=False)\n",
    "\n",
    "print(types)\n",
    "\n",
    "X_data = mbti['posts']\n",
    "X = X_data.values.reshape(-1, 1)\n",
    "\n",
    "y_data = mbti['type']\n",
    "y = y_data.values.reshape(-1, 1)\n",
    "\n",
    "X_resampled, y_resampled = RandomUnderSampler(random_state=0).fit_sample(X, y)\n",
    "#format(Counter(y_resampled))\n",
    "X_resampled #ndarray\n",
    "Counter(y_resampled)\n"
   ]
  },
  {
   "source": [
    "# tokenize and stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\nnormalize 완료\n"
     ]
    }
   ],
   "source": [
    "mbti= dataCombine\n",
    "X_data = X_resampled.ravel()\n",
    "y_data = y_resampled\n",
    "\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data: #for sentence in X_data.ravel()\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '', sentence.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "X_data = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gc\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stopwords = ['http*']\n",
    "#print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in X_data:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        tempData.append(ps.stem(word))\n",
    "        #tempData = [word for word in tempData if not word in stopwords] # 불용어 제거\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "#print(stemData[0])\n",
    "\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]\n",
    "del [[mbti]]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.11360815 0.         0.         ... 0.         0.         0.        ]\n [0.23612205 0.03721221 0.         ... 0.         0.         0.        ]\n [0.10170195 0.         0.         ... 0.         0.         0.        ]\n ...\n [0.21440755 0.03379006 0.         ... 0.         0.         0.        ]\n [0.1786607  0.         0.         ... 0.         0.         0.        ]\n [0.23497077 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#vectorization (tfidf)\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "print(tfidf.transform(stemData).toarray())\n",
    "X = tfidf.transform(stemData).toarray()\n",
    "y = np.array(y_data)\n"
   ]
  },
  {
   "source": [
    "# XGBoost_Oversampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building Start\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "print(\"Model building Start\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state=1234)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building End\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "print(\"Model building End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['XGBoost_oversample.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'XGBoost_undersample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation Start\n0.5159574468085106\n[0.66666667 0.55555556 0.54545455 0.17391304 0.88888889 0.46153846\n 0.63636364 0.77777778 0.4        0.75       0.3        0.3\n 0.61538462 0.5        0.55555556 0.5       ]\n[0.75       0.38461538 0.75       0.4        0.5        0.54545455\n 0.46666667 0.53846154 0.25       0.69230769 0.33333333 0.3\n 0.53333333 0.7        0.55555556 0.54545455]\n[0.70588235 0.45454545 0.63157895 0.24242424 0.64       0.5\n 0.53846154 0.63636364 0.30769231 0.72       0.31578947 0.3\n 0.57142857 0.58333333 0.55555556 0.52173913]\nEvaluation END\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"Evaluation Start\")\n",
    "# labels과 guesses\n",
    "labels = preds\n",
    "guesses = y_test\n",
    "\n",
    "print(accuracy_score(labels, guesses))\n",
    "print(recall_score(labels, guesses, average=None))\n",
    "print(precision_score(labels, guesses, average=None))\n",
    "print(f1_score(labels, guesses, average=None))\n",
    "\n",
    "print(\"Evaluation END\")\n",
    "# https://eunsukimme.github.io/ml/2019/10/21/Accuracy-Recall-Precision-F1-score/"
   ]
  }
 ]
}
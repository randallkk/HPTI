{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# (MBTI 별 Data 모으기: 생략) MBTI file 불러오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('.\\data\\\\training\\\\mbti.csv')"
   ]
  },
  {
   "source": [
    "# UnderSampling and OverSampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'ENFJ': 500,\n",
       "         'ENFP': 500,\n",
       "         'ENTJ': 500,\n",
       "         'ENTP': 500,\n",
       "         'ESFJ': 500,\n",
       "         'ESFP': 500,\n",
       "         'ESTJ': 500,\n",
       "         'ESTP': 500,\n",
       "         'INFJ': 500,\n",
       "         'INFP': 500,\n",
       "         'INTJ': 500,\n",
       "         'INTP': 500,\n",
       "         'ISFJ': 500,\n",
       "         'ISFP': 500,\n",
       "         'ISTJ': 500,\n",
       "         'ISTP': 500})"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "X_data = mbti['posts']\n",
    "X = X_data.values.reshape(-1, 1)\n",
    "\n",
    "y_data = mbti['type']\n",
    "y = y_data.values.reshape(-1, 1)\n",
    "\n",
    "X_resampled, y_resampled = RandomUnderSampler(random_state=0,\n",
    "sampling_strategy={\n",
    "         'ENFP': 500,\n",
    "         'ENTP': 500,\n",
    "         'INFJ': 500,\n",
    "         'INFP': 500,\n",
    "         'INTJ': 500,\n",
    "         'INTP': 500,\n",
    "         }).fit_sample(X, y)\n",
    "X_resampled, y_resampled = RandomOverSampler(random_state=0).fit_sample(X_resampled, y_resampled)\n",
    "#format(Counter(y_resampled))\n",
    "X_resampled #ndarray\n",
    "Counter(y_resampled)\n"
   ]
  },
  {
   "source": [
    "# tokenize and stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\n",
      "normalize 완료\n"
     ]
    }
   ],
   "source": [
    "X_data = X_resampled.ravel()\n",
    "y_data = y_resampled\n",
    "\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data: #for sentence in X_data.ravel()\n",
    "    rm_urls = re.sub(r'http[s]?://\\S+', '', sentence)\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '', rm_urls.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "X_data = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gc\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n",
      "['oi', 'went', 'break', 'month', 'ago', 'togeth', 'year', 'plan', 'life', 'around', 'relationship', 'wasnt', 'one', 'break', 'relationship', 'might', 'imagin', 'ourenfj', 'pun', 'mani', 'punswel', 'person', 'dont', 'go', 'much', 'attract', 'gener', 'see', 'chang', 'that', 'good', 'alreadi', 'may', 'ask', 'want', 'mere', 'sexualsorri', 'infp', 'im', 'realli', 'postrock', 'post', 'go', 'leav', 'sneak', 'thread', 'dthat', 'doesnt', 'sound', 'enfj', 'think', 'x', 'id', 'never', 'act', 'cold', 'toward', 'romant', 'interest', 'get', 'warm', 'side', 'think', 'like', 'someon', 'way', 'want', 'straight', 'themnot', 'realli', 'im', 'mostli', 'guitar', 'player', 'bass', 'obsess', 'im', 'person', 'ask', 'bass', 'louder', 'least', 'relev', 'like', 'tri', 'new', 'instrument', 'sometim', 'add', 'aguitar', 'mostli', 'add', 'bass', 'glockenspiel', 'harmonica', 'drum', 'ocarinaahahah', 'im', 'sorri', 'im', 'laugh', 'hard', 'realli', 'plagu', 'never', 'thought', 'possiblemostli', 'sound', 'lyric', 'ill', 'obviusli', 'consid', 'listen', 'mostli', 'post', 'math', 'rock', 'dont', 'care', 'much', 'wordshii', 'welcomei', 'two', 'infj', 'close', 'friend', 'never', 'though', 'naiv', 'actual', 'take', 'serious', 'though', 'notic', 'nonnf', 'friend', 'dont', 'seem', 'way', 'usual', 'dont', 'realli', 'know', 'isthi', 'actual', 'shouldnt', 'cooler', 'pollut', 'big', 'deal', 'peopl', 'avoid', 'better', 'isnt', 'like', 'peopl', 'dont', 'car', 'least', 'countri', 'livewhat', 'ignor', 'side', 'effect', 'slightli', 'mess', 'peopl', 'around', 'let', 'tri', 'get', 'pratic', 'exampl', 'x', 'constatli', 'tri', 'get', 'nerv', 'provok', 'orwelcomehi', 'fellow', 'enfj', 'wonder', 'deal', 'direct', 'conflict', 'within', 'group', 'your', 'feel', 'like', 'someon', 'doesnt', 'like', 'much', 'start', 'bother', 'lot', 'justwelcom', 'isnt', 'way', 'see', 'also', 'part', 'person', 'close', 'two', 'infj', 'one', 'low', 'self', 'esteem', 'amazingli', 'pessimist', 'pretti', 'much', 'everyth', 'otherenfj', 'blue', 'violetsorri', 'infj', 'enfj', 'kinda', 'relat', 'im', 'get', 'point', 'right', 'think', 'low', 'self', 'esteem', 'transvers', 'type', 'quit', 'low', 'self', 'esteem', 'im', 'peopleth', 'friend', 'say', 'welcom', 'hiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii', 'ddddi', 'dont', 'think', 'infj', 'thing', 'social', 'behavior', 'mayb', 'someth', 'good', 'make', 'peopl', 'jealou', 'mayb', 'your', 'pretti', 'smart', 'get', 'along', 'peopl', 'want', 'dwelcom', 'dwelcom', 'dwelcom', 'dddthi', 'truei', 'dont', 'know', 'consid', 'compliment', 'thing', 'make', 'day', 'well', 'done', 'pleas', 'dont', 'ever', 'stop', 'your', 'wish', 'abl', 'love', 'somethingi', 'think', 'notic', 'differ', 'behavior', 'drink', 'littl', 'much', 'get', 'fairli', 'happi', 'less', 'alcohol', 'friend', 'bit', 'time', 'never', 'get', 'realli', 'realli', 'drunk', 'probabl', 'iiv', 'walk', 'away', 'estp', 'theyr', 'realli', 'hard', 'get', 'along', 'also', 'istp', 'kinda', 'feel', 'like', 'energi', 'suck', 'longer', 'togeth', 'understand', 'anddamn', 'friendli', 'fun', 'read', 'nice', 'forum', 'envoirnmentinfj', 'probabl', 'favorit', 'type', 'notic', 'behavior', 'differ', 'group', 'peopl', 'two', 'us', 'privat', 'environ', 'usual', 'take', 'lead', 'talk', 'likei', 'agre', 'cursedsoul', 'actual', 'reciev', 'thing', 'object', 'whatev', 'doesnt', 'make', 'happi', 'realli', 'enjoy', 'peopl', 'stuff', 'mean', 'theyr', 'actual', 'give', 'time', 'andthank', 'welcom', 'ye', 'sorri', 'confus', 'thank', 'di', 'think', 'might', 'romant', 'interst', 'mayb', 'said', 'wouldnt', 'flirt', 'tri', 'get', 'feedback', 'im', 'say', 'lot', 'peopl', 'imthank', 'hi', 'wave', 'ive', 'lurk', 'around', 'forum', 'time', 'think', 'time', 'proper', 'introduct', 'im', 'ri', 'enfj', 'im', 'realli', 'interest', 'pretti', 'much']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in X_data:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        if word not in stop_words:\n",
    "            tempData.append(ps.stem(word))\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "print(stemData[0])\n",
    "\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]\n",
    "del [[mbti]]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n [0.05117135 0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n ...\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.04678176 0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#vectorization (tfidf)\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "print(tfidf.transform(stemData).toarray())\n",
    "X = tfidf.transform(stemData).toarray()\n",
    "y = np.array(y_data)\n"
   ]
  },
  {
   "source": [
    "# XGBoost_Undersampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building Start\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d7cf7a269820>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1234\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    826\u001b[0m                                 missing=self.missing, nthread=self.n_jobs)\n\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m         self._Booster = train(xgb_options, train_dmatrix,\n\u001b[0m\u001b[0;32m    829\u001b[0m                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m                               \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     return _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    209\u001b[0m                            \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python38\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1159\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1160\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1161\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Model building Start\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state=1234)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model building End\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "print(\"Model building End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['XGBoost_undersample.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'XGBoost_undersample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation Start\n0.5159574468085106\n[0.66666667 0.55555556 0.54545455 0.17391304 0.88888889 0.46153846\n 0.63636364 0.77777778 0.4        0.75       0.3        0.3\n 0.61538462 0.5        0.55555556 0.5       ]\n[0.75       0.38461538 0.75       0.4        0.5        0.54545455\n 0.46666667 0.53846154 0.25       0.69230769 0.33333333 0.3\n 0.53333333 0.7        0.55555556 0.54545455]\n[0.70588235 0.45454545 0.63157895 0.24242424 0.64       0.5\n 0.53846154 0.63636364 0.30769231 0.72       0.31578947 0.3\n 0.57142857 0.58333333 0.55555556 0.52173913]\nEvaluation END\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"Evaluation Start\")\n",
    "# labels과 guesses\n",
    "labels = preds\n",
    "guesses = y_test\n",
    "\n",
    "print(accuracy_score(labels, guesses))\n",
    "print(recall_score(labels, guesses, average=None))\n",
    "print(precision_score(labels, guesses, average=None))\n",
    "print(f1_score(labels, guesses, average=None))\n",
    "\n",
    "print(\"Evaluation END\")\n",
    "# https://eunsukimme.github.io/ml/2019/10/21/Accuracy-Recall-Precision-F1-score/"
   ]
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request"
   ]
  },
  {
   "source": [
    "# MBTI 별 posting 모으기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      posts\n",
       "type       \n",
       "ENFJ    190\n",
       "ENFP    675\n",
       "ENTJ    231\n",
       "ENTP    685\n",
       "ESFJ     42\n",
       "ESFP     48\n",
       "ESTJ     39\n",
       "ESTP     89\n",
       "INFJ   1470\n",
       "INFP   1832\n",
       "INTJ   1091\n",
       "INTP   1304\n",
       "ISFJ    166\n",
       "ISFP    271\n",
       "ISTJ    205\n",
       "ISTP    337"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posts</th>\n    </tr>\n    <tr>\n      <th>type</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ENFJ</th>\n      <td>190</td>\n    </tr>\n    <tr>\n      <th>ENFP</th>\n      <td>675</td>\n    </tr>\n    <tr>\n      <th>ENTJ</th>\n      <td>231</td>\n    </tr>\n    <tr>\n      <th>ENTP</th>\n      <td>685</td>\n    </tr>\n    <tr>\n      <th>ESFJ</th>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>ESFP</th>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>ESTJ</th>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>ESTP</th>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>INFJ</th>\n      <td>1470</td>\n    </tr>\n    <tr>\n      <th>INFP</th>\n      <td>1832</td>\n    </tr>\n    <tr>\n      <th>INTJ</th>\n      <td>1091</td>\n    </tr>\n    <tr>\n      <th>INTP</th>\n      <td>1304</td>\n    </tr>\n    <tr>\n      <th>ISFJ</th>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>ISFP</th>\n      <td>271</td>\n    </tr>\n    <tr>\n      <th>ISTJ</th>\n      <td>205</td>\n    </tr>\n    <tr>\n      <th>ISTP</th>\n      <td>337</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "mbti=pd.read_csv('./data/mbti_1.csv')\n",
    "\n",
    "types = mbti.groupby('type').count()\n",
    "\n",
    "output_file = '.\\data\\\\training\\\\mbti.csv' # raw string이 아니라 '\\'를 쓰려면 \\\\라고 해야 함\n",
    "allData = []\n",
    "\n",
    "for type in types.index:\n",
    "    condition = mbti['type'] == type # condition: mbti['type']가 topfive의 원소인 type 같을 bool 조건\n",
    "    ownsentence = mbti[condition]  # ownsentence: condition에 맞는 row만 filtering한 dataframe\n",
    "    allData.append(ownsentence)\n",
    "dataCombine = pd.concat(allData, axis=0, ignore_index=True)\n",
    "dataCombine.to_csv(output_file, index=False)\n",
    "\n",
    "types\n",
    "\n",
    "    #input_file = r'.\\data\\training'\n",
    "    #allFile_list = glob.glob(os.path.join(input_file, 'mbti_*'))\n",
    "    #for file in allFile_list:\n",
    "    #    csv = pd.read_csv(file,sep=';', encoding='iso-8859-1') # for구문으로 csv파일들을 읽어 들인다\n",
    "    #    cleanMbti = csv['posts'].str.replace('[^A-Za-z\\s]+', '')"
   ]
  },
  {
   "source": [
    "# Resampling (Undersampling and Oversampling)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Tokenize and Stemming\n",
    "morph analysis 기법."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\n",
      "normalize 완료\n"
     ]
    }
   ],
   "source": [
    "mbti= dataCombine\n",
    "X_data = mbti['posts']\n",
    "y_data = mbti['type']\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data.tolist():\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '',sentence.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "mbti.posts = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n",
      "[[0.15619188 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.22128246 0.04019078 0.         ... 0.         0.         0.        ]\n",
      " [0.14145265 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.18796687 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.19099817 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.13217717 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'tf = TfidfVectorizer().fit(stemData)\\nprint(tf.transform(stemData).toarray())\\nprint(tf.vocabulary_)\\n#print(vector.vocabulary_)'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stopwords = ['http*']\n",
    "#print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in mbti['posts']:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        tempData.append(ps.stem(word))\n",
    "        #tempData = [word for word in tempData if not word in stopwords] # 불용어 제거\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "#print(stemData[0])\n",
    "\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]\n",
    "\n",
    "#vectorization (tfidf)\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "print(tfidf.transform(stemData).toarray())\n",
    "'''tf = TfidfVectorizer().fit(stemData)\n",
    "print(tf.transform(stemData).toarray())\n",
    "print(tf.vocabulary_)\n",
    "#print(vector.vocabulary_)'''\n"
   ]
  },
  {
   "source": [
    "# Padding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'lenth=[]\\nfor wordlist in stemData:\\n    lenth.append(len(wordlist))\\nmax_len = max(lenth)\\nprint(max_len)\\nX_token = pad_sequences(stemData,maxlen=max_len, value=\" \")\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "'''lenth=[]\n",
    "for wordlist in stemData:\n",
    "    lenth.append(len(wordlist))\n",
    "max_len = max(lenth)\n",
    "print(max_len)\n",
    "X_token = pad_sequences(stemData,maxlen=max_len, value=\" \")\n",
    "'''"
   ]
  },
  {
   "source": [
    "# Model building"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"X_data = np.array(X_token).reshape(-1,1)\\ny_data = np.array(mbti['type']).reshape(-1,1)\\n\\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size= 0.3, random_state=1234)\\nmodel = XGBClassifier()\\nmodel.fit(X_train, y_train)\\npreds = classifier.predict(X_test)\\n\\nplot_importance(model)\\npyplot.show()\""
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "'''X_data = np.array(X_token).reshape(-1,1)\n",
    "y_data = np.array(mbti['type']).reshape(-1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size= 0.3, random_state=1234)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "preds = classifier.predict(X_test)\n",
    "\n",
    "plot_importance(model)\n",
    "pyplot.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
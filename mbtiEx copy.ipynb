{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request"
   ]
  },
  {
   "source": [
    "# MBTI 별 posting 모으기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      posts\n",
       "type       \n",
       "ENFJ    190\n",
       "ENFP    675\n",
       "ENTJ    231\n",
       "ENTP    685\n",
       "ESFJ     42\n",
       "ESFP     48\n",
       "ESTJ     39\n",
       "ESTP     89\n",
       "INFJ   1470\n",
       "INFP   1832\n",
       "INTJ   1091\n",
       "INTP   1304\n",
       "ISFJ    166\n",
       "ISFP    271\n",
       "ISTJ    205\n",
       "ISTP    337"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posts</th>\n    </tr>\n    <tr>\n      <th>type</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ENFJ</th>\n      <td>190</td>\n    </tr>\n    <tr>\n      <th>ENFP</th>\n      <td>675</td>\n    </tr>\n    <tr>\n      <th>ENTJ</th>\n      <td>231</td>\n    </tr>\n    <tr>\n      <th>ENTP</th>\n      <td>685</td>\n    </tr>\n    <tr>\n      <th>ESFJ</th>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>ESFP</th>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>ESTJ</th>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>ESTP</th>\n      <td>89</td>\n    </tr>\n    <tr>\n      <th>INFJ</th>\n      <td>1470</td>\n    </tr>\n    <tr>\n      <th>INFP</th>\n      <td>1832</td>\n    </tr>\n    <tr>\n      <th>INTJ</th>\n      <td>1091</td>\n    </tr>\n    <tr>\n      <th>INTP</th>\n      <td>1304</td>\n    </tr>\n    <tr>\n      <th>ISFJ</th>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>ISFP</th>\n      <td>271</td>\n    </tr>\n    <tr>\n      <th>ISTJ</th>\n      <td>205</td>\n    </tr>\n    <tr>\n      <th>ISTP</th>\n      <td>337</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "mbti=pd.read_csv('./data/mbti_1.csv')\n",
    "\n",
    "types = mbti.groupby('type').count()\n",
    "\n",
    "output_file = '.\\data\\\\training\\\\mbti.csv' # raw string이 아니라 '\\'를 쓰려면 \\\\라고 해야 함\n",
    "allData = []\n",
    "\n",
    "for type in types.index:\n",
    "    condition = mbti['type'] == type # condition: mbti['type']가 topfive의 원소인 type 같을 bool 조건\n",
    "    ownsentence = mbti[condition]  # ownsentence: condition에 맞는 row만 filtering한 dataframe\n",
    "    allData.append(ownsentence)\n",
    "dataCombine = pd.concat(allData, axis=0, ignore_index=True)\n",
    "dataCombine.to_csv(output_file, index=False)\n",
    "\n",
    "types\n",
    "\n",
    "    #input_file = r'.\\data\\training'\n",
    "    #allFile_list = glob.glob(os.path.join(input_file, 'mbti_*'))\n",
    "    #for file in allFile_list:\n",
    "    #    csv = pd.read_csv(file,sep=';', encoding='iso-8859-1') # for구문으로 csv파일들을 읽어 들인다\n",
    "    #    cleanMbti = csv['posts'].str.replace('[^A-Za-z\\s]+', '')"
   ]
  },
  {
   "source": [
    "# Resampling (Undersampling and Oversampling)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Tokenize and Stemming\n",
    "morph analysis 기법."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "normalize 시작\n",
      "normalize 완료\n"
     ]
    }
   ],
   "source": [
    "mbti= dataCombine\n",
    "X_data = mbti['posts']\n",
    "y_data = mbti['type']\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data.tolist():\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '',sentence.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "mbti.posts = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stemming 시작\n",
      "stemming 완료\n",
      "['httpswwwyoutubecomwatchvplaaikvhvz', 'oi', 'went', 'through', 'a', 'break', 'up', 'some', 'month', 'ago', 'we', 'were', 'togeth', 'for', 'year', 'and', 'i', 'had', 'plan', 'my', 'life', 'around', 'that', 'relationship', 'i', 'wasnt', 'the', 'one', 'break', 'the', 'relationship', 'as', 'you', 'might', 'imagin', 'and', 'all', 'ourenfj', 'pun', 'so', 'mani', 'punswel', 'i', 'person', 'dont', 'go', 'that', 'much', 'for', 'attract', 'in', 'gener', 'but', 'i', 'can', 'see', 'you', 'have', 'the', 'will', 'to', 'chang', 'that', 'and', 'that', 'good', 'alreadi', 'may', 'i', 'ask', 'if', 'you', 'want', 'to', 'be', 'with', 'them', 'in', 'a', 'mere', 'sexualsorri', 'not', 'an', 'infp', 'but', 'im', 'realli', 'into', 'postrock', 'so', 'i', 'had', 'to', 'post', 'go', 'to', 'leav', 'thi', 'here', 'and', 'sneak', 'out', 'd', 'httpswwwyoutubecomwatchvushcobpcmwelcomewrong', 'thread', 'dthat', 'doesnt', 'sound', 'veri', 'enfj', 'i', 'think', 'x', 'id', 'never', 'act', 'cold', 'toward', 'a', 'romant', 'interest', 'those', 'just', 'get', 'my', 'warm', 'side', 'i', 'think', 'when', 'i', 'like', 'someon', 'that', 'way', 'all', 'i', 'want', 'to', 'do', 'is', 'to', 'just', 'straight', 'to', 'themnot', 'realli', 'im', 'mostli', 'a', 'guitar', 'player', 'with', 'a', 'bass', 'obsess', 'im', 'the', 'person', 'who', 'ask', 'for', 'the', 'bass', 'to', 'be', 'louder', 'or', 'at', 'least', 'more', 'relev', 'i', 'like', 'tri', 'new', 'instrument', 'and', 'sometim', 'add', 'them', 'to', 'aguitar', 'mostli', 'o', 'but', 'i', 'can', 'add', 'bass', 'glockenspiel', 'harmonica', 'drum', 'and', 'ocarinaahahah', 'im', 'sorri', 'but', 'im', 'laugh', 'so', 'hard', 'it', 'realli', 'is', 'a', 'plagu', 'i', 'never', 'thought', 'that', 'wa', 'possiblemostli', 'becaus', 'of', 'the', 'sound', 'if', 'there', 'are', 'lyric', 'ill', 'obviusli', 'consid', 'them', 'but', 'i', 'listen', 'mostli', 'to', 'post', 'and', 'math', 'rock', 'so', 'i', 'dont', 'care', 'too', 'much', 'about', 'wordshii', 'd', 'and', 'welcomei', 'have', 'two', 'infj', 'close', 'friend', 'and', 'i', 'never', 'though', 'of', 'them', 'as', 'naiv', 'actual', 'i', 'take', 'them', 'veri', 'serious', 'though', 'i', 'notic', 'nonnf', 'friend', 'dont', 'seem', 'them', 'that', 'way', 'usual', 'i', 'dont', 'realli', 'know', 'whi', 'it', 'isthi', 'actual', 'whi', 'shouldnt', 'they', 'be', 'cooler', 'pollut', 'is', 'a', 'big', 'deal', 'if', 'peopl', 'can', 'avoid', 'it', 'is', 'better', 'isnt', 'it', 'it', 'not', 'like', 'peopl', 'dont', 'have', 'car', 'at', 'least', 'in', 'my', 'countri', 'but', 'if', 'you', 'livewhat', 'if', 'ignor', 'them', 'had', 'a', 'side', 'effect', 'of', 'slightli', 'mess', 'with', 'peopl', 'around', 'both', 'of', 'you', 'let', 'me', 'tri', 'to', 'get', 'some', 'pratic', 'exampl', 'if', 'x', 'if', 'constatli', 'tri', 'to', 'get', 'on', 'your', 'nerv', 'provok', 'you', 'orwelcomehi', 'there', 'fellow', 'enfj', 'i', 'wa', 'wonder', 'how', 'do', 'you', 'deal', 'with', 'direct', 'conflict', 'within', 'the', 'group', 'your', 'in', 'when', 'i', 'feel', 'like', 'someon', 'doesnt', 'like', 'me', 'that', 'much', 'and', 'start', 'to', 'bother', 'me', 'a', 'lot', 'i', 'justwelcom', 'isnt', 'the', 'way', 'you', 'see', 'yourself', 'also', 'a', 'part', 'of', 'your', 'person', 'i', 'am', 'veri', 'close', 'to', 'two', 'infj', 'one', 'of', 'them', 'with', 'veri', 'low', 'self', 'esteem', 'amazingli', 'pessimist', 'over', 'pretti', 'much', 'everyth', 'on', 'the', 'otherenfj', 'blue', 'violetsorri', 'not', 'an', 'infj', 'but', 'as', 'an', 'enfj', 'i', 'can', 'kinda', 'relat', 'to', 'thi', 'if', 'im', 'get', 'your', 'point', 'right', 'i', 'think', 'low', 'self', 'esteem', 'is', 'transvers', 'to', 'all', 'type', 'i', 'do', 'have', 'quit', 'a', 'low', 'self', 'esteem', 'and', 'im', 'a', 'peopleth', 'friend', 'say', 'welcom', 'd', 'hiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii', 'ddddi', 'dont', 'think', 'it', 'an', 'infj', 'thing', 'more', 'than', 'just', 'social', 'behavior', 'mayb', 'you', 'have', 'someth', 'good', 'that', 'make', 'other', 'peopl', 'jealou', 'mayb', 'your', 'pretti', 'or', 'smart', 'or', 'get', 'along', 'with', 'peopl', 'they', 'want', 'tohttpswwwyoutubecomwatchvgfbbctshmowelcom', 'dwelcom', 'dwelcom', 'dwelcom', 'dddthi', 'is', 'so', 'truei', 'dont', 'know', 'if', 'thi', 'can', 'be', 'consid', 'compliment', 'but', 'here', 'are', 'some', 'thing', 'that', 'make', 'my', 'day', 'well', 'done', 'pleas', 'dont', 'ever', 'stop', 'do', 'what', 'your', 'do', 'i', 'wish', 'i', 'wa', 'abl', 'to', 'love', 'somethingi', 'think', 'i', 'notic', 'differ', 'behavior', 'when', 'i', 'drink', 'a', 'littl', 'too', 'much', 'i', 'get', 'fairli', 'happi', 'with', 'less', 'alcohol', 'than', 'my', 'friend', 'bit', 'at', 'the', 'same', 'time', 'i', 'never', 'get', 'realli', 'realli', 'drunk', 'probabl', 'becaus', 'iiv', 'walk', 'away', 'from', 'estp', 'and', 'theyr', 'realli', 'hard', 'for', 'me', 'to', 'get', 'along', 'with', 'also', 'istp', 'i', 'kinda', 'feel', 'like', 'my', 'energi', 'is', 'be', 'suck', 'by', 'them', 'the', 'longer', 'were', 'togeth', 'but', 'i', 'can', 'understand', 'them', 'anddamn', 'friendli', 'and', 'veri', 'fun', 'to', 'read', 'it', 'a', 'nice', 'forum', 'envoirnmentinfj', 'are', 'probabl', 'my', 'favorit', 'type', 'i', 'notic', 'some', 'behavior', 'differ', 'if', 'were', 'in', 'a', 'group', 'of', 'peopl', 'or', 'just', 'the', 'two', 'of', 'us', 'in', 'more', 'privat', 'environ', 'they', 'usual', 'take', 'the', 'lead', 'and', 'talk', 'i', 'likei', 'agre', 'with', 'cursedsoul', 'actual', 'reciev', 'thing', 'object', 'or', 'whatev', 'doesnt', 'make', 'me', 'that', 'happi', 'i', 'realli', 'enjoy', 'when', 'peopl', 'do', 'stuff', 'becaus', 'it', 'mean', 'theyr', 'actual', 'give', 'me', 'their', 'time', 'andthank', 'you', 'for', 'welcom', 'me', 'ye', 'it', 'is', 'sorri', 'for', 'confus', 'you', 'd', 'thank', 'you', 'di', 'think', 'she', 'might', 'be', 'romant', 'interst', 'in', 'you', 'mayb', 'she', 'said', 'she', 'wouldnt', 'flirt', 'with', 'you', 'to', 'tri', 'to', 'get', 'some', 'feedback', 'im', 'say', 'that', 'becaus', 'i', 'do', 'that', 'a', 'lot', 'and', 'onli', 'with', 'peopl', 'imthank', 'you', 'hi', 'there', 'wave', 'how', 'you', 'all', 'do', 'ive', 'been', 'lurk', 'around', 'the', 'forum', 'for', 'some', 'time', 'and', 'i', 'think', 'it', 'time', 'for', 'a', 'proper', 'introduct', 'im', 'ri', 'enfj', 'and', 'im', 'realli', 'interest', 'in', 'pretti', 'much']\n"
     ]
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stopwords = ['http*']\n",
    "#print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in mbti['posts']:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        tempData.append(ps.stem(word))\n",
    "        #tempData = [word for word in tempData if not word in stopwords] # 불용어 제거\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n"
   ]
  },
  {
   "source": [
    "# Padding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-ce62f3e9fd13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlenth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwordlist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstemData\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "lenth=[]\n",
    "for wordlist in stemData:\n",
    "    lenth.append(len(wordlist))\n",
    "max_len = max(lenth)\n",
    "print(max_len)\n",
    "X_token = pad_sequences(stemData,maxlen=max_len, value=\" \")\n"
   ]
  },
  {
   "source": [
    "# Model building"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'list'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-8a6c3c05e402>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1234\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplot_importance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    824\u001b[0m         train_dmatrix = DMatrix(X, label=training_labels, weight=sample_weight,\n\u001b[0;32m    825\u001b[0m                                 \u001b[0mbase_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_margin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 826\u001b[1;33m                                 missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m         self._Booster = train(xgb_options, train_dmatrix,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[0mthreads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnthread\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[0mfeature_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m             feature_types=feature_types)\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36mdispatch_data_backend\u001b[1;34m(data, missing, threads, feature_names, feature_types)\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_numpy_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         return _from_numpy_array(data, missing, threads, feature_names,\n\u001b[1;32m--> 495\u001b[1;33m                                  feature_types)\n\u001b[0m\u001b[0;32m    496\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_from_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36m_from_numpy_array\u001b[1;34m(data, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \"\"\"\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mflatten\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_transform_np_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     _check_call(_LIB.XGDMatrixCreateFromMat_omp(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36m_transform_np_array\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;31m# explicitly tell np.array to try and avoid copying)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     flatten = np.array(data.reshape(data.size), copy=False,\n\u001b[1;32m--> 116\u001b[1;33m                        dtype=np.float32)\n\u001b[0m\u001b[0;32m    117\u001b[0m     \u001b[0mflatten\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_maybe_np_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0m_check_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "X_data = np.array(X_token).reshape(-1,1)\n",
    "y_data = np.array(mbti['type']).reshape(-1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size= 0.3, random_state=1234)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "preds = classifier.predict(X_test)\n",
    "\n",
    "plot_importance(model)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
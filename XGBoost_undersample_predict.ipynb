{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# (MBTI 별 Data 모으기: 생략) MBTI file 불러오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=pd.read_csv('.\\data\\\\training\\\\mbti.csv')"
   ]
  },
  {
   "source": [
    "# UnderSampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X_data = mbti['posts']\n",
    "X = X_data.values.reshape(-1, 1)\n",
    "\n",
    "y_data = mbti['type']\n",
    "print(y_data)\n",
    "y = y_data.values.reshape(-1, 1)\n",
    "print(y)\n",
    "\n",
    "X_resampled, y_resampled = RandomUnderSampler(random_state=0).fit_sample(X, y)\n",
    "#format(Counter(y_resampled))\n",
    "X_resampled #ndarray\n",
    "Counter(y_resampled)\n"
   ]
  },
  {
   "source": [
    "# Type value "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_Types = {'I':0, 'E':1, 'N':0, \"S\":1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "b_Types_list =[{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    return[b_Types[i] for i in personality]\n",
    "\n",
    "def translate_back(personality):\n",
    "    s = \"\"\n",
    "    for i, j in enumerate(personality):\n",
    "        s += b_Types_list[i][j]\n",
    "    return s\n",
    "\n",
    "d = y[0]\n",
    "print(d)\n",
    "list_personality_b = np.array([translate_personality(p) for p in d])\n",
    "print(list_personality_b)"
   ]
  },
  {
   "source": [
    "# tokenize and stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_resampled.ravel()\n",
    "y_data = y_resampled\n",
    "\n",
    "\n",
    "print(\"normalize 시작\")\n",
    "normalized_text = []\n",
    "for sentence in X_data: #for sentence in X_data.ravel()\n",
    "    rm_urls = re.sub(r'http[s]?://\\S+', '', sentence)\n",
    "    clean_sentence = re.sub('[^A-Za-z\\s]+', '', rm_urls.lower())\n",
    "    normalized_text.append(clean_sentence)\n",
    "X_data = normalized_text\n",
    "print(\"normalize 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gc\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "\n",
    "# print(\"stopwords 제거 시작\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print(\"stopwords 제거 완료\")\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "stemData=[]\n",
    "for sentence in X_data:\n",
    "    tokenData = nltk.word_tokenize(sentence)\n",
    "    tempData = []\n",
    "    for word in tokenData:\n",
    "        if word not in stop_words:\n",
    "            tempData.append(ps.stem(word))\n",
    "    stemData.append(tempData)\n",
    "print(\"stemming 완료\")\n",
    "print(stemData[0])\n",
    "stemData = np.array(stemDaa)\n",
    "#'list' object has no attribute 'lower' 문제 발생\n",
    "#2차원 리스트인 stemData를 1차원 리스트로\n",
    "#flat_stem = [item for sublist in stemData for item in sublist]\n",
    "del [[mbti]]\n",
    "gc.collect()"
   ]
  },
  {
   "source": [
    "# Vectorization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    max_features=1500,\n",
    "    tokenizer=None,\n",
    "    stop_words=None,\n",
    "    max_df=0.7,\n",
    "    min_df=0.1\n",
    ")\n",
    "\n",
    "#X_ct = count_vectorizer.fit_transform(stemData)\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "tfidf.fit(stemData)\n",
    "print(tfidf.transform(stemData).toarray())\n",
    "X = tfidf.transform(stemData)\n",
    "print(X)\n",
    "y = np.array(y_data)"
   ]
  },
  {
   "source": [
    "# XGBoost_Undersampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3, random_state=1234)\n"
   ]
  },
  {
   "source": [
    "# Model Building & Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model building Start\")\n",
    "print(max(len(l) for l in X_train)\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train, verbose=1)\n",
    "#model.get_booster().get_score(importance_type=\"gain\") #변수의 중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#test_hp = xgb.DMatrix(data=VHX_data, label=hy)\n",
    "#preds = model.predict(X_test)\n",
    "print(\"Model building End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'XGBoost_undersample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "print(\"Evaluation Start\")\n",
    "# labels과 guesses\n",
    "labels = preds\n",
    "guesses = y_test\n",
    "\n",
    "print(accuracy_score(labels, guesses))\n",
    "print(recall_score(labels, guesses, average=None))\n",
    "print(precision_score(labels, guesses, average=None))\n",
    "print(f1_score(labels, guesses, average=None))\n",
    "\n",
    "print(\"Evaluation END\")\n",
    "# https://eunsukimme.github.io/ml/2019/10/21/Accuracy-Recall-Precision-F1-score/"
   ]
  },
  {
   "source": [
    "# Harry Potter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import glob   \n",
    "\n",
    "'''HP data Normalization and Stemming'''\n",
    "# bag_of_words = {} #{등장인물 : 해당인물의 stemData, ~}\n",
    "h_normalized_text = []\n",
    "h_stemData = []\n",
    "\n",
    "ps=PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "input_file = r'.\\data\\\\test\\\\'\n",
    "\n",
    "allHPfile_list = glob.glob(os.path.join(input_file, 'HP_*'))\n",
    "print(allHPfile_list)\n",
    "\n",
    "harryPotter={}\n",
    "for file in allHPfile_list:\n",
    "    df = pd.read_csv(file, sep=',', encoding='iso-8859-1')\n",
    "    harryPotter[df['Character'][0]] = df['Sentence'].ravel()\n",
    "\n",
    "print(\"stemming 시작\")\n",
    "for element in harryPotter.keys():\n",
    "    for sentence in harryPotter[element]:\n",
    "        h_clean_sentence = re.sub('[^A-Za-z\\s]+', '', sentence.lower())\n",
    "        h_normalized_text.append(h_clean_sentence)\n",
    "        for sentence in h_normalized_text:\n",
    "            tokenData = nltk.word_tokenize(sentence)\n",
    "            for word in tokenData:\n",
    "                if word not in stop_words:\n",
    "                    h_stemData.append(ps.stem(word))\n",
    "    harryPotter[element] = h_stemData\n",
    "\n",
    "print(\"stemming 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HP data Vectorization'''\n",
    "# import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf=TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None\n",
    ")\n",
    "\n",
    "stemData = []\n",
    "hy=[]\n",
    "for element in harryPotter.keys():\n",
    "    stemData.append(harryPotter[element])\n",
    "    hy.append(element)\n",
    "HX_data = tfidf.fit_transform(stemData)\n",
    "print(HX_data)\n",
    "HY_data = np.array(hy)\n",
    "\n",
    "'''for element in harryPotter.keys():\n",
    "    tfidf.fit(harryPotter[element])\n",
    "    script = tfidf.transform(harryPotter[element])\n",
    "    X.append(script)\n",
    "    y.append(element)\n",
    "\n",
    "print(X.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1527\n38863\n"
     ]
    }
   ],
   "source": [
    "print(len(HX_data.toarray()[0]))\n",
    "print(len(X.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'DUMBLEDORE': 38863,\n",
       "         'HAGRID': 38863,\n",
       "         'HARRY': 38863,\n",
       "         'HERMIONE': 38863,\n",
       "         'RON': 38863})"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "source": [
    "'''HP data Undersample'''\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "HX_data, HY_data = RandomOverSampler(random_state=0,\n",
    "sampling_strategy={'HARRY':38863,'RON':38863,'HERMIONE':38863,'DUMBLEDORE':38863,'HAGRID':38863}).fit_sample(HX_data, HY_data)\n",
    "#format(Counter(y_resampled))\n",
    "HX_data #ndarray\n",
    "Counter(HY_data)"
   ]
  },
  {
   "source": [
    "# Predict & Get Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib  \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n",
    "\n",
    "xgb_model_loaded = joblib.load('XGBoost_undersample.pkl')\n",
    "# xgb_model_loaded.get_xgb_params()    # Get xgboost specific parameters.\n",
    "harrypotter_predict = xgb_model_loaded.predict(HX_data)\n",
    "# print(harrypotter_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(harrypotter_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "38863"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "cols_when_model_builds = xgb_model_loaded.get_booster().feature_names\n",
    "len(cols_when_model_builds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
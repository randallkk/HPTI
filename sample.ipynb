{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   type                                              posts\n",
       "0  ENFJ  'https://www.youtube.com/watch?v=PLAaiKvHvZs||...\n",
       "1  ENFJ  https://www.youtube.com/watch?v=AwgF14ySLpw  I...\n",
       "2  ENFJ  'That sounds like a beautiful relationship alr...\n",
       "3  ENFJ  'I've always thought of Tony Stark as more of ...\n",
       "4  ENFJ  'ABILITY TO TRANSFORM.  Form of... a bucket of...\n",
       "5  ENFJ  It burns!! Haha|||http://personalitycafe.com/m...\n",
       "6  ENFJ  'http://www.youtube.com/watch?v=3mokC24vTPI|||...\n",
       "7  ENFJ  'I had an ESTJ boss, who was a kinda control f...\n",
       "8  ENFJ  'ENFJ with a concussion: A Case Study. :dry:  ...\n",
       "9  ENFJ  What arguments? There were none. You stated, b..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>posts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ENFJ</td>\n      <td>'https://www.youtube.com/watch?v=PLAaiKvHvZs||...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENFJ</td>\n      <td>https://www.youtube.com/watch?v=AwgF14ySLpw  I...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ENFJ</td>\n      <td>'That sounds like a beautiful relationship alr...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ENFJ</td>\n      <td>'I've always thought of Tony Stark as more of ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENFJ</td>\n      <td>'ABILITY TO TRANSFORM.  Form of... a bucket of...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ENFJ</td>\n      <td>It burns!! Haha|||http://personalitycafe.com/m...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ENFJ</td>\n      <td>'http://www.youtube.com/watch?v=3mokC24vTPI|||...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ENFJ</td>\n      <td>'I had an ESTJ boss, who was a kinda control f...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>ENFJ</td>\n      <td>'ENFJ with a concussion: A Case Study. :dry:  ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ENFJ</td>\n      <td>What arguments? There were none. You stated, b...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 208
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# read data\n",
    "mbti = pd.read_csv('.\\data\\\\training\\\\mbti.csv') \n",
    "mbti.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[p.split('|||') for p in data.head(2).posts.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0    ENFJ\n1    ENFJ\n2    ENFJ\n3    ENFJ\nName: type, dtype: object\nBinarize MBTI list: \n[[1 0 0 0]\n [1 0 0 0]\n [1 0 0 0]\n [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "b_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    # transform mbti to binary vector\n",
    "    \n",
    "    return [b_Pers[l] for l in personality]\n",
    "\n",
    "def translate_back(personality):\n",
    "    # transform binary vector to mbti personality\n",
    "    \n",
    "    s = \"\"\n",
    "    for i, l in enumerate(personality):\n",
    "        s += b_Pers_list[i][l]\n",
    "    return s\n",
    "\n",
    "# Check ...\n",
    "d = mbti.head(4)\n",
    "list_personality_bin = np.array([translate_personality(p) for p in d.type])\n",
    "print(d.type)\n",
    "print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Compute list of subject with Type | list of comments \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "\n",
    "# We want to remove these from the psosts\n",
    "unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "  \n",
    "unique_type_list = [x.lower() for x in unique_type_list]\n",
    "\n",
    "\n",
    "# Lemmatize\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "# Cache the stop words for speed \n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "def pre_process_data(data, remove_stop_words=True, remove_mbti_profiles=True):\n",
    "\n",
    "    list_personality = []\n",
    "    list_posts = []\n",
    "    len_data = len(data)\n",
    "    i = 0\n",
    "    \n",
    "    for row in data.iterrows():\n",
    "        i+=1\n",
    "        if (i % 500 == 0 or i == 1 or i == len_data):\n",
    "            print(\"%s of %s rows\" % (i, len_data))\n",
    "\n",
    "        ##### Remove and clean comments\n",
    "        posts = row[1].posts\n",
    "        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n",
    "        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "        temp = re.sub(' +', ' ', temp).lower()\n",
    "        if remove_stop_words:\n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n",
    "        else:\n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n",
    "            \n",
    "        if remove_mbti_profiles:\n",
    "            for t in unique_type_list:\n",
    "                temp = temp.replace(t,\"\")\n",
    "\n",
    "        type_labelized = translate_personality(row[1].type)\n",
    "        list_personality.append(type_labelized)\n",
    "        list_posts.append(temp)\n",
    "\n",
    "    list_posts = np.array(list_posts)\n",
    "    list_personality = np.array(list_personality)\n",
    "    \n",
    "    return list_posts, list_personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 of 8675 rows\n",
      "500 of 8675 rows\n",
      "1000 of 8675 rows\n",
      "1500 of 8675 rows\n",
      "2000 of 8675 rows\n",
      "2500 of 8675 rows\n",
      "3000 of 8675 rows\n",
      "3500 of 8675 rows\n",
      "4000 of 8675 rows\n",
      "4500 of 8675 rows\n",
      "5000 of 8675 rows\n",
      "5500 of 8675 rows\n",
      "6000 of 8675 rows\n",
      "6500 of 8675 rows\n",
      "7000 of 8675 rows\n",
      "7500 of 8675 rows\n",
      "8000 of 8675 rows\n",
      "8500 of 8675 rows\n",
      "8675 of 8675 rows\n"
     ]
    }
   ],
   "source": [
    "list_posts, list_personality  = pre_process_data(mbti, remove_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num posts and personalities:  (8675,) (8675, 4)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 213
    }
   ],
   "source": [
    "print(\"Num posts and personalities: \",  list_posts.shape, list_personality.shape)\n",
    "list_posts[0]\n",
    "list_personality[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CountVectorizer...\n",
      "Tf-idf...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Posts to a matrix of token counts\n",
    "cntizer = CountVectorizer(  analyzer=\"word\", \n",
    "                            max_features=1000, \n",
    "                            tokenizer=None,    \n",
    "                            preprocessor=None, \n",
    "                            stop_words=None,  \n",
    "                            max_df=0.7,\n",
    "                            min_df=0.1) \n",
    "\n",
    "# Learn the vocabulary dictionary and return term-document matrix\n",
    "print(\"CountVectorizer...\")\n",
    "X_cnt = cntizer.fit_transform(list_posts)\n",
    "\n",
    "# Transform the count matrix to a normalized tf or tf-idf representation\n",
    "tfizer = TfidfTransformer()\n",
    "\n",
    "print(\"Tf-idf...\")\n",
    "# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation\n",
    "X_tfidf =  tfizer.fit_transform(X_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 'ability'), (1, 'able'), (2, 'absolutely'), (3, 'accept'), (4, 'accurate'), (5, 'across'), (6, 'act'), (7, 'action'), (8, 'actual'), (9, 'actually'), (10, 'add'), (11, 'admit'), (12, 'advice'), (13, 'afraid'), (14, 'age'), (15, 'ago'), (16, 'agree'), (17, 'ah'), (18, 'almost'), (19, 'alone'), (20, 'along'), (21, 'already'), (22, 'although'), (23, 'amazing'), (24, 'amount'), (25, 'angry'), (26, 'animal'), (27, 'annoying'), (28, 'another'), (29, 'answer'), (30, 'anxiety'), (31, 'anymore'), (32, 'anyone'), (33, 'anything'), (34, 'anyway'), (35, 'apparently'), (36, 'appreciate'), (37, 'approach'), (38, 'area'), (39, 'argument'), (40, 'around'), (41, 'art'), (42, 'ask'), (43, 'asked'), (44, 'asking'), (45, 'aspect'), (46, 'assume'), (47, 'attention'), (48, 'attracted'), (49, 'avatar'), (50, 'avoid'), (51, 'aware'), (52, 'away'), (53, 'awesome'), (54, 'awkward'), (55, 'baby'), (56, 'back'), (57, 'bad'), (58, 'based'), (59, 'basically'), (60, 'beautiful'), (61, 'become'), (62, 'bed'), (63, 'behavior'), (64, 'behind'), (65, 'belief'), (66, 'believe'), (67, 'best'), (68, 'better'), (69, 'big'), (70, 'bit'), (71, 'black'), (72, 'blue'), (73, 'body'), (74, 'book'), (75, 'bored'), (76, 'boring'), (77, 'born'), (78, 'bother'), (79, 'boy'), (80, 'boyfriend'), (81, 'brain'), (82, 'break'), (83, 'bring'), (84, 'brother'), (85, 'buy'), (86, 'call'), (87, 'called'), (88, 'came'), (89, 'cannot'), (90, 'car'), (91, 'care'), (92, 'career'), (93, 'case'), (94, 'cat'), (95, 'cause'), (96, 'certain'), (97, 'certainly'), (98, 'chance'), (99, 'change'), (100, 'changed'), (101, 'character'), (102, 'check'), (103, 'child'), (104, 'choice'), (105, 'choose'), (106, 'class'), (107, 'clear'), (108, 'clearly'), (109, 'close'), (110, 'cognitive'), (111, 'cold'), (112, 'college'), (113, 'color'), (114, 'come'), (115, 'comfortable'), (116, 'coming'), (117, 'comment'), (118, 'common'), (119, 'company'), (120, 'complete'), (121, 'completely'), (122, 'computer'), (123, 'concept'), (124, 'confused'), (125, 'connection'), (126, 'consider'), (127, 'considered'), (128, 'considering'), (129, 'constantly'), (130, 'contact'), (131, 'control'), (132, 'conversation'), (133, 'cool'), (134, 'correct'), (135, 'count'), (136, 'country'), (137, 'couple'), (138, 'course'), (139, 'crazy'), (140, 'create'), (141, 'creative'), (142, 'cry'), (143, 'curious'), (144, 'current'), (145, 'currently'), (146, 'cut'), (147, 'cute'), (148, 'dad'), (149, 'damn'), (150, 'dark'), (151, 'date'), (152, 'dating'), (153, 'day'), (154, 'dead'), (155, 'deal'), (156, 'dear'), (157, 'death'), (158, 'debate'), (159, 'decide'), (160, 'decided'), (161, 'decision'), (162, 'deep'), (163, 'definitely'), (164, 'degree'), (165, 'depends'), (166, 'depressed'), (167, 'depression'), (168, 'describe'), (169, 'description'), (170, 'desire'), (171, 'detail'), (172, 'die'), (173, 'difference'), (174, 'different'), (175, 'difficult'), (176, 'discussion'), (177, 'dislike'), (178, 'dog'), (179, 'dominant'), (180, 'done'), (181, 'dont'), (182, 'doubt'), (183, 'dream'), (184, 'drink'), (185, 'drive'), (186, 'due'), (187, 'early'), (188, 'easier'), (189, 'easily'), (190, 'easy'), (191, 'eat'), (192, 'edit'), (193, 'effect'), (194, 'effort'), (195, 'either'), (196, 'else'), (197, 'emotion'), (198, 'emotional'), (199, 'emotionally'), (200, 'end'), (201, 'ended'), (202, 'energy'), (203, 'english'), (204, 'enjoy'), (205, 'enneagram'), (206, 'enough'), (207, 'entire'), (208, 'environment'), (209, 'especially'), (210, 'etc'), (211, 'ever'), (212, 'every'), (213, 'everyone'), (214, 'everything'), (215, 'evil'), (216, 'ex'), (217, 'exact'), (218, 'exactly'), (219, 'example'), (220, 'except'), (221, 'exist'), (222, 'expect'), (223, 'experience'), (224, 'explain'), (225, 'express'), (226, 'extremely'), (227, 'extrovert'), (228, 'extroverted'), (229, 'eye'), (230, 'face'), (231, 'fact'), (232, 'fair'), (233, 'fairly'), (234, 'fall'), (235, 'family'), (236, 'fan'), (237, 'far'), (238, 'fast'), (239, 'father'), (240, 'favorite'), (241, 'fe'), (242, 'fear'), (243, 'feeling'), (244, 'fellow'), (245, 'felt'), (246, 'female'), (247, 'fi'), (248, 'fight'), (249, 'figure'), (250, 'finally'), (251, 'finding'), (252, 'fine'), (253, 'fit'), (254, 'focus'), (255, 'follow'), (256, 'food'), (257, 'forget'), (258, 'form'), (259, 'forum'), (260, 'found'), (261, 'four'), (262, 'free'), (263, 'friendship'), (264, 'front'), (265, 'fuck'), (266, 'full'), (267, 'fun'), (268, 'function'), (269, 'funny'), (270, 'future'), (271, 'game'), (272, 'gave'), (273, 'general'), (274, 'generally'), (275, 'getting'), (276, 'girl'), (277, 'give'), (278, 'given'), (279, 'giving'), (280, 'glad'), (281, 'goal'), (282, 'god'), (283, 'gone'), (284, 'gonna'), (285, 'got'), (286, 'gotten'), (287, 'grade'), (288, 'great'), (289, 'group'), (290, 'guess'), (291, 'guy'), (292, 'haha'), (293, 'hair'), (294, 'half'), (295, 'hand'), (296, 'hang'), (297, 'happen'), (298, 'happened'), (299, 'happens'), (300, 'happy'), (301, 'hard'), (302, 'hate'), (303, 'head'), (304, 'healthy'), (305, 'hear'), (306, 'heard'), (307, 'heart'), (308, 'hell'), (309, 'hello'), (310, 'help'), (311, 'helped'), (312, 'helpful'), (313, 'hey'), (314, 'hi'), (315, 'high'), (316, 'highly'), (317, 'history'), (318, 'hit'), (319, 'hmm'), (320, 'hold'), (321, 'home'), (322, 'honest'), (323, 'honestly'), (324, 'hope'), (325, 'hot'), (326, 'hour'), (327, 'house'), (328, 'however'), (329, 'hug'), (330, 'huge'), (331, 'human'), (332, 'humor'), (333, 'hurt'), (334, 'idea'), (335, 'ideal'), (336, 'im'), (337, 'image'), (338, 'imagine'), (339, 'important'), (340, 'individual'), (341, 'information'), (342, 'inside'), (343, 'instead'), (344, 'intelligence'), (345, 'intelligent'), (346, 'interaction'), (347, 'interest'), (348, 'interested'), (349, 'interesting'), (350, 'internet'), (351, 'introvert'), (352, 'introverted'), (353, 'intuition'), (354, 'intuitive'), (355, 'involved'), (356, 'issue'), (357, 'job'), (358, 'joke'), (359, 'keep'), (360, 'kid'), (361, 'kill'), (362, 'kind'), (363, 'kinda'), (364, 'knew'), (365, 'knowing'), (366, 'knowledge'), (367, 'known'), (368, 'lack'), (369, 'language'), (370, 'large'), (371, 'last'), (372, 'late'), (373, 'lately'), (374, 'later'), (375, 'laugh'), (376, 'laughing'), (377, 'lazy'), (378, 'le'), (379, 'lead'), (380, 'learn'), (381, 'learned'), (382, 'learning'), (383, 'least'), (384, 'leave'), (385, 'left'), (386, 'let'), (387, 'level'), (388, 'lie'), (389, 'light'), (390, 'liked'), (391, 'likely'), (392, 'line'), (393, 'link'), (394, 'list'), (395, 'listen'), (396, 'listening'), (397, 'literally'), (398, 'little'), (399, 'live'), (400, 'living'), (401, 'logic'), (402, 'logical'), (403, 'lol'), (404, 'long'), (405, 'longer'), (406, 'look'), (407, 'looked'), (408, 'looking'), (409, 'lose'), (410, 'lost'), (411, 'loud'), (412, 'loved'), (413, 'low'), (414, 'made'), (415, 'main'), (416, 'major'), (417, 'making'), (418, 'male'), (419, 'man'), (420, 'many'), (421, 'math'), (422, 'matter'), (423, 'may'), (424, 'maybe'), (425, 'mbti'), (426, 'mean'), (427, 'meaning'), (428, 'meant'), (429, 'meet'), (430, 'member'), (431, 'memory'), (432, 'men'), (433, 'mental'), (434, 'mention'), (435, 'mentioned'), (436, 'message'), (437, 'met'), (438, 'middle'), (439, 'might'), (440, 'mind'), (441, 'mine'), (442, 'minute'), (443, 'miss'), (444, 'mistake'), (445, 'mom'), (446, 'moment'), (447, 'money'), (448, 'month'), (449, 'mood'), (450, 'morning'), (451, 'mostly'), (452, 'mother'), (453, 'move'), (454, 'movie'), (455, 'music'), (456, 'must'), (457, 'name'), (458, 'natural'), (459, 'naturally'), (460, 'nature'), (461, 'ne'), (462, 'necessarily'), (463, 'needed'), (464, 'negative'), (465, 'new'), (466, 'next'), (467, 'ni'), (468, 'nice'), (469, 'night'), (470, 'non'), (471, 'none'), (472, 'normal'), (473, 'note'), (474, 'nothing'), (475, 'notice'), (476, 'noticed'), (477, 'nt'), (478, 'number'), (479, 'obvious'), (480, 'obviously'), (481, 'often'), (482, 'oh'), (483, 'ok'), (484, 'okay'), (485, 'old'), (486, 'older'), (487, 'online'), (488, 'op'), (489, 'open'), (490, 'opinion'), (491, 'opposite'), (492, 'option'), (493, 'order'), (494, 'original'), (495, 'others'), (496, 'otherwise'), (497, 'outside'), (498, 'page'), (499, 'pain'), (500, 'parent'), (501, 'part'), (502, 'particular'), (503, 'particularly'), (504, 'partner'), (505, 'party'), (506, 'past'), (507, 'pay'), (508, 'perc'), (509, 'perfect'), (510, 'perhaps'), (511, 'period'), (512, 'personal'), (513, 'personality'), (514, 'personally'), (515, 'perspective'), (516, 'phone'), (517, 'physical'), (518, 'pick'), (519, 'picture'), (520, 'piece'), (521, 'place'), (522, 'plan'), (523, 'play'), (524, 'playing'), (525, 'please'), (526, 'plus'), (527, 'point'), (528, 'positive'), (529, 'possibility'), (530, 'possible'), (531, 'possibly'), (532, 'post'), (533, 'posted'), (534, 'posting'), (535, 'power'), (536, 'prefer'), (537, 'present'), (538, 'pretty'), (539, 'probably'), (540, 'problem'), (541, 'process'), (542, 'project'), (543, 'proud'), (544, 'public'), (545, 'purpose'), (546, 'put'), (547, 'putting'), (548, 'quality'), (549, 'question'), (550, 'quickly'), (551, 'quiet'), (552, 'quite'), (553, 'quote'), (554, 'random'), (555, 'rare'), (556, 'rarely'), (557, 'rather'), (558, 'read'), (559, 'reading'), (560, 'real'), (561, 'reality'), (562, 'realize'), (563, 'realized'), (564, 'reason'), (565, 'recently'), (566, 'red'), (567, 'relate'), (568, 'related'), (569, 'relationship'), (570, 'religion'), (571, 'remember'), (572, 'reply'), (573, 'research'), (574, 'respect'), (575, 'response'), (576, 'rest'), (577, 'result'), (578, 'romantic'), (579, 'room'), (580, 'rule'), (581, 'run'), (582, 'sad'), (583, 'said'), (584, 'saw'), (585, 'saying'), (586, 'school'), (587, 'science'), (588, 'se'), (589, 'second'), (590, 'seeing'), (591, 'seem'), (592, 'seemed'), (593, 'seems'), (594, 'seen'), (595, 'self'), (596, 'sense'), (597, 'sensitive'), (598, 'series'), (599, 'serious'), (600, 'seriously'), (601, 'set'), (602, 'several'), (603, 'sex'), (604, 'share'), (605, 'shit'), (606, 'short'), (607, 'show'), (608, 'shy'), (609, 'si'), (610, 'side'), (611, 'sign'), (612, 'similar'), (613, 'simple'), (614, 'simply'), (615, 'since'), (616, 'single'), (617, 'sister'), (618, 'sit'), (619, 'site'), (620, 'situation'), (621, 'skill'), (622, 'sleep'), (623, 'small'), (624, 'smart'), (625, 'smile'), (626, 'social'), (627, 'society'), (628, 'somehow'), (629, 'sometimes'), (630, 'somewhat'), (631, 'somewhere'), (632, 'song'), (633, 'soon'), (634, 'sorry'), (635, 'sort'), (636, 'soul'), (637, 'sound'), (638, 'space'), (639, 'speak'), (640, 'speaking'), (641, 'special'), (642, 'specific'), (643, 'spend'), (644, 'spent'), (645, 'stand'), (646, 'start'), (647, 'started'), (648, 'starting'), (649, 'state'), (650, 'statement'), (651, 'stay'), (652, 'step'), (653, 'stereotype'), (654, 'stick'), (655, 'still'), (656, 'stop'), (657, 'story'), (658, 'straight'), (659, 'strange'), (660, 'stress'), (661, 'strong'), (662, 'stuck'), (663, 'student'), (664, 'study'), (665, 'stuff'), (666, 'stupid'), (667, 'style'), (668, 'subject'), (669, 'suck'), (670, 'super'), (671, 'support'), (672, 'suppose'), (673, 'supposed'), (674, 'sure'), (675, 'surprised'), (676, 'sweet'), (677, 'system'), (678, 'take'), (679, 'taken'), (680, 'taking'), (681, 'talk'), (682, 'talking'), (683, 'te'), (684, 'teacher'), (685, 'tell'), (686, 'telling'), (687, 'tend'), (688, 'tendency'), (689, 'term'), (690, 'terrible'), (691, 'test'), (692, 'text'), (693, 'th'), (694, 'thank'), (695, 'thanks'), (696, 'theory'), (697, 'thinking'), (698, 'thread'), (699, 'three'), (700, 'ti'), (701, 'tired'), (702, 'title'), (703, 'today'), (704, 'together'), (705, 'told'), (706, 'tongue'), (707, 'took'), (708, 'top'), (709, 'topic'), (710, 'totally'), (711, 'touch'), (712, 'towards'), (713, 'trait'), (714, 'tried'), (715, 'trouble'), (716, 'true'), (717, 'truly'), (718, 'trust'), (719, 'truth'), (720, 'try'), (721, 'trying'), (722, 'turn'), (723, 'tv'), (724, 'two'), (725, 'typing'), (726, 'understand'), (727, 'understanding'), (728, 'unfortunately'), (729, 'unhealthy'), (730, 'unless'), (731, 'unsure'), (732, 'upon'), (733, 'use'), (734, 'used'), (735, 'useful'), (736, 'user'), (737, 'using'), (738, 'usually'), (739, 'value'), (740, 'video'), (741, 'view'), (742, 'voice'), (743, 'wait'), (744, 'walk'), (745, 'wanted'), (746, 'wanting'), (747, 'watch'), (748, 'watched'), (749, 'watching'), (750, 'water'), (751, 'wear'), (752, 'week'), (753, 'weird'), (754, 'welcome'), (755, 'went'), (756, 'whatever'), (757, 'whenever'), (758, 'whether'), (759, 'white'), (760, 'whole'), (761, 'wink'), (762, 'wish'), (763, 'within'), (764, 'without'), (765, 'woman'), (766, 'wonder'), (767, 'wondering'), (768, 'word'), (769, 'work'), (770, 'worked'), (771, 'working'), (772, 'world'), (773, 'worry'), (774, 'worse'), (775, 'worst'), (776, 'worth'), (777, 'wow'), (778, 'write'), (779, 'writing'), (780, 'written'), (781, 'wrong'), (782, 'wrote'), (783, 'xd'), (784, 'yeah'), (785, 'year'), (786, 'yes'), (787, 'yesterday'), (788, 'yet'), (789, 'young'), (790, 'younger')]\nX: Posts in tf-idf representation \n* 1st row:\n  (0, 786)\t0.032600381071687654\n  (0, 785)\t0.03136288977608111\n  (0, 781)\t0.043762930465140924\n  (0, 768)\t0.04052197808982334\n  (0, 767)\t0.06408327485074945\n  (0, 763)\t0.06938275896602293\n  (0, 762)\t0.0499129874331799\n  (0, 756)\t0.05358920382012592\n  (0, 755)\t0.05211901322898174\n  (0, 754)\t0.4943094859365744\n  (0, 738)\t0.07629626111609479\n  (0, 726)\t0.037922757597737194\n  (0, 724)\t0.10847364410320848\n  (0, 721)\t0.07558124204128044\n  (0, 720)\t0.06738364198840834\n  (0, 716)\t0.04231551039124347\n  (0, 712)\t0.06069277285402125\n  (0, 704)\t0.1100186040407932\n  (0, 698)\t0.03488710578720808\n  (0, 694)\t0.1306010643541958\n  (0, 681)\t0.03867930535232904\n  (0, 678)\t0.06225405317477049\n  (0, 665)\t0.0451609269525616\n  (0, 658)\t0.07043802849883424\n  (0, 656)\t0.05029536375962649\n  :\t:\n  (0, 126)\t0.05635159658671166\n  (0, 111)\t0.06602307380084001\n  (0, 109)\t0.09333019115022119\n  (0, 99)\t0.045577617621162726\n  (0, 91)\t0.04200866341891142\n  (0, 90)\t0.07084398430576652\n  (0, 82)\t0.06329544404776077\n  (0, 78)\t0.0694029885385002\n  (0, 72)\t0.07130123420471954\n  (0, 70)\t0.038949600471202835\n  (0, 69)\t0.046754973923671374\n  (0, 68)\t0.03554807687675638\n  (0, 63)\t0.20712471268288293\n  (0, 52)\t0.04484814745273172\n  (0, 50)\t0.06748246981876639\n  (0, 42)\t0.09016814616606796\n  (0, 40)\t0.10221555514174664\n  (0, 21)\t0.04641234521883926\n  (0, 20)\t0.10183441139415275\n  (0, 16)\t0.039331303474815775\n  (0, 15)\t0.04679252792300483\n  (0, 10)\t0.12174626216777565\n  (0, 9)\t0.1260654524025153\n  (0, 6)\t0.05587668050805698\n  (0, 1)\t0.047964693107378956\n"
     ]
    }
   ],
   "source": [
    "feature_names = list(enumerate(cntizer.get_feature_names()))\n",
    "print(feature_names)\n",
    "print(\"X: Posts in tf-idf representation \\n* 1st row:\\n%s\" % X_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IE: Introversion (I) / Extroversion (E)\nNS: Intuition (N) – Sensing (S)\nFT: Feeling (F) - Thinking (T)\nJP: Judging (J) – Perceiving (P)\n"
     ]
    }
   ],
   "source": [
    "type_indicators = [ \"IE: Introversion (I) / Extroversion (E)\", \"NS: Intuition (N) – Sensing (S)\", \n",
    "                   \"FT: Feeling (F) - Thinking (T)\", \"JP: Judging (J) – Perceiving (P)\"  ]\n",
    "\n",
    "for l in range(len(type_indicators)):\n",
    "    print(type_indicators[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MBTI 1st row: ENFJ\nY: Binarized MBTI 1st row: [1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"MBTI 1st row: %s\" % translate_back(list_personality[0,:]))\n",
    "print(\"Y: Binarized MBTI 1st row: %s\" % list_personality[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IE: Introversion (I) / Extroversion (E) ...\n",
      "* IE: Introversion (I) / Extroversion (E) Accuracy: 84.37%\n",
      "[0.78047741 0.93313253]\n",
      "[0.94284243 0.7504845 ]\n",
      "[0.85401119 0.83190118]\n",
      "Counter({0: 6676, 1: 1999})\n",
      "Counter({1: 6676, 0: 6676})\n",
      "NS: Intuition (N) – Sensing (S) ...\n",
      "* NS: Intuition (N) – Sensing (S) Accuracy: 91.44%\n",
      "[0.87883749 0.95694716]\n",
      "[0.9606264  0.86856128]\n",
      "[0.91791364 0.91061453]\n",
      "Counter({0: 7478, 1: 1197})\n",
      "Counter({0: 7478, 1: 7478})\n",
      "FT: Feeling (F) - Thinking (T) ...\n",
      "* FT: Feeling (F) - Thinking (T) Accuracy: 74.55%\n",
      "[0.73825967 0.75310446]\n",
      "[0.75977257 0.73120567]\n",
      "[0.74886165 0.74199352]\n",
      "Counter({0: 4694, 1: 3981})\n",
      "Counter({0: 4694, 1: 4694})\n",
      "JP: Judging (J) – Perceiving (P) ...\n",
      "* JP: Judging (J) – Perceiving (P) Accuracy: 70.21%\n",
      "[0.74085138 0.67331118]\n",
      "[0.62705436 0.77799104]\n",
      "[0.67921945 0.72187593]\n",
      "Counter({1: 5241, 0: 3434})\n",
      "Counter({0: 5241, 1: 5241})\n",
      "[0.69618529 0.69414169 0.68779823 0.68166326 0.69120654]\n",
      "0.6901990010940046\n"
     ]
    }
   ],
   "source": [
    "# First XGBoost model for MBTI dataset\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Posts in tf-idf representation\n",
    "X = X_tfidf\n",
    "\n",
    "# Let's train type indicator individually\n",
    "for l in range(len(type_indicators)):\n",
    "    print(\"%s ...\" % (type_indicators[l]))\n",
    "    \n",
    "    Y = list_personality[:,l]\n",
    "    \n",
    "    X_resampled, y_resampled = SMOTE(random_state=0).fit_sample(X, Y)\n",
    "    \n",
    "    # model building\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=1234)\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    # evaluation, validation score\n",
    "    labels = predictions\n",
    "    guesses = y_test\n",
    "\n",
    "    accuracy = accuracy_score(labels, guesses)\n",
    "    print(\"* %s Accuracy: %.2f%%\" % (type_indicators[l], accuracy * 100.0))\n",
    "    print(recall_score(labels, guesses, average=None))\n",
    "    print(precision_score(labels, guesses, average=None))\n",
    "    print(f1_score(labels, guesses, average=None))\n",
    "    print(Counter(Y))\n",
    "    print(Counter(y_resampled))\n",
    "\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "score = cross_val_score(model, X_train, y_train, cv=skf )\n",
    "print(score)\n",
    "print(score.mean())"
   ]
  },
  {
   "source": [
    "# Harry Potter mbti"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Tell us, have you been setting anything mad and hairy loose in the castle lately?Whoever it was, they must have been looking for something.Hermione!But you heard McGonagall.We're not allowed to leave the tower except for class.That's Dad's boss.Cornelius Fudge, Minister of Magic.Hagrid's right.With Dumbledore gone, there'll be an attack a day.What?They're heading to the Dark Forest.Why spiders? Why couldn't it be \"follow the butterflies\"?Harry, I don't like this.Harry, I don't like this at all.Can we go back now?Harry.Harry!Can we panic now?Know any spells?Where's Hermione when you need her?Glad we're out of there.Thanks for that.The flying gear's jammed!I'm trying!Follow the spiders!Follow the spiders.If Hagrid ever gets out of Azkaban, I'll kill him.I mean, what was the point of sending us in there?What have we found out?What's that?But if it kills by looking people in the eye, why is it no one's dead?And Mrs. Norris?I'm pretty sure she didn't havea camera or a mirror, Harry.How's a basilisk been getting around?A dirty, great snake.Someone would have seen it.Pipes? It's using the plumbing.Moaning Myrtle.Her skeleton will lie in the Chamber forever.Ginny.What about my sister?You're the Defense Against the Dark Arts teacher. You can't go now.Is there anything you can do?Say something. Harry say something in Parseltongue!Better you than us.Go on.What's this?Bloody hell.Whatever shed this must be 60 feet long. Or more.Heart of a lion, this one.Harry! HarryI'm fine.Ron Weasley.Lockhart's Memory Charm backfired.He hasn't got a clue who he is.No.What do I do now?Okay.Thanks, sir.Welcome back, Hermione.I'm warning you, Hermione.Keep that beast away from Scabbers, or I'll turn it into a tea cozy.A cat? Is that what they told you?Looks like a pig with hair.Harry.Brilliant. Loads of old stuff like mummies, tombs, even Scabbers enjoyed himself.Along with the dung beetle.I haven't shown anyone.Brilliant.I still think it was brilliant.Who do you think that is?Do you know everything?How is it she knows everything?Oh.Let me get this straight.Sirius Black escaped from Azkaban to come after you?Sure.Except no one's ever broken out of Azkaban before and he's a murderous, raving lunatic.What's going on?There's something moving out there.I think someone's coming aboard.Bloody hell! What's happening?Well, you sort of went rigid.We thought maybe you were having a fit or something.No.I felt weird, though.Like I'd never be cheerful again.Shove off, Malfoy.Green. That's a monkey.What is that?Do not give him one again.Hey, Neville, try an elephant.I will.Oh, don't try one of them.Look at him. His face.Where did you come from?Sure.Yeah.Harry's got sort of a wonky cross.That's trials and suffering.And that there could be the sun and that's happiness.So you're gonna suffer, but you're gonna be happy about it.You don't think that Grim thing's got anything to do with Sirius Black?Ancient Runes? Exactly how many classes are you taking?Hang on. That's not possible.Ancient Runes is in the same time as Divination.You have to be in two classes at once.You're supposed to stroke it.Hagrid, exactly what is that?Listen to the idiot.He's really laying it on thick, isn't he?Who?When did she get here?Riddikulus!Honeyduke's Sweetshop is brilliant, but nothing beats Zonko's Joke Shop.We never got to go to the Shrieking Shack.You heard it's the most...Probably Neville forgot the password again.Oh, you're there.Serves her right.She was a terrible singer.Werewolves?When did she come in?Did you see her come in?He's got a point, you know.He looks a bit peaky, doesn't he?Well, you fell off your broom.There's something else you should know too.When you fell, your broom sort of blew into the Whomping Willow, and...Well...Twice.Actually, I'm fine here.Shut your mouth, Malfoy.What's up, Malfoy? Lost your skis?Bloody hell, Harry.That was not funny.Those weasels! Never told me about any Marauder's Map.Sure. Along with his Invisibility Cloak.That's not true!Thick heads.Merry Christmas.Gorgeous.Unless you've been ripped to pieces!I haven't lost anything!Your cat killed him!Harry, you've seen the way that bloodthirsty beast of hers is always lurking about.And Scabbers is gone.Your cat killed him!Did.They're not sacking you!Spiders! There's... There's spiders.Spiders. They want me to tap-dance.I don't wanna tap-dance!Right, yeah. Tell them. I'll tell them...She's gone mental, Hermione has.I mean, not that she wasn't always mental but now it's in the open for everyone to see.I'm not going back.See you.It just got worse.Hermione, no!He's not worth it.Not good, brilliant.Scabbers! You're alive!Right. Next time I see Crookshanks, I'll let him know.Let's go!He bit me. Scabbers.Scabbers, come back.Scabbers, you bit me!Harry, Hermione, run!It's the Grim!Ah! Harry!Harry!Help!It's a trap. He's the dog.He's an Animagus.Harry! What did you just do?Me?! He's mental!Scabbers has been in my family for...So what?What are you trying to do to him?Scabbers!Leave him alone! Get off him!What are you doing?A bit? A bit?You almost tore my leg off!You better go.I'm fine. Go.So painful.They might chop it.It's too late. It's ruined.It'll have to be chopped off.Hermione! Bad idea. Bad idea.Nice doggy. Nice doggy!It's Scabbers who did it.He's my rat, sir.He's not really a rat. He was a rat.He was my brother Percy's rat.But then they gave him an owl...What the bloody hell was that all about?Hermione, no! He's not worth it.Not good, brilliant.Scabbers, you're alive!Let's go.How did you get there?I was talking to you there.And now you're there.Stand back, I said! Or I'll take it upstairs if you don't settle.Quiet.Let the man through.I didn't mean to open it, Harry.It was badly wrapped.They made me do it.No one knows.\n1 of 1 rows\n"
     ]
    }
   ],
   "source": [
    "#sample my_posts is ndarray\n",
    "hp = pd.read_csv('.\\data\\\\test\\\\HP_RON.csv')\n",
    "hp_list = hp['Sentence'].tolist()\n",
    "hp_string = ''.join([str(elem) for elem in hp_list])\n",
    "print(hp_string)\n",
    "\n",
    "# The type is just a dummy so that the data prep fucntion can be reused\n",
    "mydata = pd.DataFrame(data={'type': ['ENFJ'], 'posts': [hp_string]})\n",
    "\n",
    "hp_string, dummy = pre_process_data(mydata, remove_stop_words=True)\n",
    "\n",
    "my_X_cnt = cntizer.transform(hp_string)\n",
    "my_X_tfidf =  tfizer.transform(my_X_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IE: Introversion (I) / Extroversion (E) ...\n",
      "NS: Intuition (N) – Sensing (S) ...\n",
      "FT: Feeling (F) - Thinking (T) ...\n",
      "JP: Judging (J) – Perceiving (P) ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "param['n_estimators'] = 200\n",
    "param['max_depth'] = 2\n",
    "param['nthread'] = 8\n",
    "param['learning_rate'] = 0.2\n",
    "\n",
    "result = []\n",
    "# Let's train type indicator individually\n",
    "for l in range(len(type_indicators)):\n",
    "    print(\"%s ...\" % (type_indicators[l]))\n",
    "    \n",
    "    Y = list_personality[:,l]\n",
    "\n",
    "    # modeling\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1234)\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # make predictions for my  data\n",
    "    y_pred = model.predict(my_X_tfidf)\n",
    "    result.append(y_pred[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "oo's MBTI is...:  INFP\n"
     ]
    }
   ],
   "source": [
    "print(\"oo's MBTI is...: \", translate_back(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}